ID;TITLE;YEAR;KEY;PUBLISHED INTO;PUBLICATION TYPE;BIBTEX;TYPE OF WORK;ABSTRACT;CATEGORY;LLM APPROACH TYPE;BENCHMARK;LLMs USED;EVALUATION METRIC;TOOL
P01;A Survey of Testing Techniques Based on Large Language Models;2024;Fei2024;C: ICCMT;Conference;"@Inproceedings{Fei2024 ,
  author =       ""Qi, Fei and Hou, Yingnan and Lin, Ning and Bao, Shanshan and Xu, Nuo"",
  title =        ""A Survey of Testing Techniques Based on Large Language Models"",
  booktitle =    ""Proceedings of the 2024 International Conference on Computer and Multimedia Technology"",
  series =       ""ICCMT '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Sanming, China"",
  pages =        ""280–-284"",
  doi =          ""10.1145/3675249.3675298"",
  url =          ""https://doi.org/10.1145/3675249.3675298""
}";Survey;With the development of software testing technology, Large Language Model (LLM) driven testing method have gradually become an emerging trend in the field of software testing. This paper presents a comprehensive review of LLM-based testing techniques. The results of 19 studies using LLM to optimize testing techniques are analyzed from the perspective of software testing. This paper discusses in detail how to use LLM to optimize test techniques for generating automated test code and generating diverse input in software test tasks. It also summarizes the challenges and opportunities faced by this field. The above conclusions can identify the shortcomings of LLM-based software testing technology and the direction of future research.;Reflections;None;No Bmk-Ds;;No Eval.;
P02;Software Testing with Large Language Models: Survey, Landscape, and Vision;2024;WangTSE2024;J: TSE;Journal;"@article{WangTSE2024,
  title = ""Software testing with large language models: Survey, landscape, and vision"",
  author = ""Wang, Junjie and Huang, Yuchao and Chen, Chunyang and Liu, Zhe and Wang, Song and Wang, Qing"",
  journal = ""IEEE Transactions on Software Engineering"",
  volume = ""50"",
  number = ""4"",
  pages = ""911–-936"",
  year = ""2024"",
  publisher = ""IEEE Press"",
  doi = ""10.1109/TSE.2024.3368208"",
  url = ""https://doi.org/10.1109/TSE.2024.3368208""
}";Survey;"Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language
processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide
range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability
of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing
techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides
a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software
testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for
which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes
the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs.
It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research
in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in
software testing.";Reflections;None;No Bmk-Ds;;No Eval.;
P03;An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation;2023;Schafer2024;J: TSE;Journal;"@article{Schafer2024,
  title = ""An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation"",
  author = ""Schäfer, Max and Nadi, Sarah and Eghbali, Aryaz and Tip, Frank"",
  journal = ""IEEE Transactions on Software Engineering"",
  volume = ""50"",
  number = ""1"",
  pages = ""85--105"",
  year = ""2024"",
  publisher = ""IEEE Press"",
  doi = ""10.1109/TSE.2023.3334955"",
  url = ""https://doi.org/10.1109/TSE.2023.3334955""
}";Research Contribution;"Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task,
motivating the need for automation. Large Language Models (LLMs) have recently been applied to various aspects of software
development, including their suggested use for automated generation of unit tests, but while requiring additional training or few-shot
learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for
automated unit test generation without requiring additional training or manual effort. Concretely, we consider an approach where the
LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples
extracted from documentation. Furthermore, if a generated test fails, our approach attempts to generate a new test that fixes the
problem by re-prompting the model with the failing test and error message. We implement our approach in TESTPILOT, an adaptive
LLM-based test generation tool for JavaScript that automatically generates unit tests for the methods in a given project’s API.
We evaluate TESTPILOT using OpenAI’s gpt3.5-turbo LLM on 25 npm packages with a total of 1,684 API functions. The generated
tests achieve a median statement coverage of 70.2% and branch coverage of 52.8%. In contrast, the state-of-the feedback-directed
JavaScript test generation technique, Nessie, achieves only 51.3% statement coverage and 25.6% branch coverage. Furthermore,
experiments with excluding parts of the information included in the prompts show that all components contribute towards the
generation of effective test suites. We also find that 92.8% of TESTPILOT’s generated tests have ≤ 50% similarity with existing tests (as
measured by normalized edit distance), with none of them being exact copies. Finally, we run TESTPILOT with two additional LLMs,
OpenAI’s older code-cushman-002 LLM and StarCoder, an LLM for which the training process is publicly documented. Overall, we
observed similar results with the former (68.2% median statement coverage), and somewhat worse results with the latter (54.0%
median statement coverage), suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM,
but does not fundamentally depend on the specific model.";Unit Test Generation;LLM-Pure-Prompting;Custom;GPT-Family;Test Coverage, % Pass, Other;TestPilot
P04;Are We Testing or Being Tested? Exploring the Practical Applications of Large Language Models in Software Testing;2024;Santos2024;C: ICST;Conference;"@Inproceedings{Santos2024 ,
  author =       ""Santos, Robson and Santos, Italo and Magalhaes, Cleyton and de Souza Santos, Ronnie"",
  title =        ""Are We Testing or Being Tested? Exploring the Practical Applications of Large Language Models in Software Testing"",
  booktitle =    ""2024 IEEE Conference on Software Testing, Verification and Validation (ICST)"",
  series =       ""ICST '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Toronto, Canada"",
  pages =        ""353--360"",
  doi =          ""10.1109/ICST60714.2024.00039"",
  url =          ""https://doi.org/10.1109/ICST60714.2024.00039""
}";Survey;"A Large Language Model (LLM) represents a
cutting-edge artificial intelligence model that generates coherent
content, including grammatically precise sentences, human-like
paragraphs, and syntactically accurate code snippets. LLMs can
play a pivotal role in software development, including software
testing. LLMs go beyond traditional roles such as requirement
analysis and documentation and can support test case generation,
making them valuable tools that significantly enhance testing
practices within the field. Hence, we explore the practical
application of LLMs in software testing within an industrial
setting, focusing on their current use by professional testers. In
this context, rather than relying on existing data, we conducted
a cross-sectional survey and collected data within real working
contexts—specifically, engaging with practitioners in industrial
settings. We applied quantitative and qualitative techniques to
analyze and synthesize our collected data. Our findings demonstrate
that LLMs effectively enhance testing documents and
significantly assist testing professionals in programming tasks
like debugging and test case automation. LLMs can support
individuals engaged in manual testing who need to code. However,
it is crucial to emphasize that, at this early stage, software testing
professionals should use LLMs with caution while well-defined
methods and guidelines are being built for the secure adoption
of these tools.";Reflections;None;No Bmk-Ds;;Other;-
P05;Automated Unit Test Improvement using Large Language Models at Meta;2024;Alshahwan2024;C: FSE;Conference;"@Inproceedings{Alshahwan2024 ,
  author =       ""Alshahwan, Nadia and Chheda, Jubin and Finogenova, Anastasia and Gokkaya, Beliz and Harman, Mark and Harper, Inna and Marginean, Alexandru and Sengupta, Shubho and Wang, Eddy"",
  title =        ""Automated Unit Test Improvement using Large Language Models at Meta"",
  booktitle =    ""Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering"",
  series =       ""FSE 2024"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Porto de Galinhas, Brazil"",
  pages =        ""185–-196"",
  doi =          ""https://doi.org/10.1145/3663529.3663839"",
  url =          ""10.1145/3663529.3663839""
}";Research Contribution;"This paper describes Meta’s TestGen-LLM tool, which uses LLMs
to automatically improve existing human-written tests. TestGen-
LLM verifies that its generated test classes successfully clear a set
of filters that assure measurable improvement over the original
test suite, thereby eliminating problems due to LLM hallucination.
We describe the deployment of TestGen-LLM at Meta test-a-thons
for the Instagram and Facebook platforms. In an evaluation on
Reels and Stories products for Instagram, 75% of TestGen-LLM’s
test cases built correctly, 57% passed reliably, and 25% increased
coverage. During Meta’s Instagram and Facebook test-a-thons, it
improved 11.5% of all classes to which it was applied, with 73% of
its recommendations being accepted for production deployment
by Meta software engineers. We believe this is the first report on
industrial scale deployment of LLM-generated code backed by such
assurances of code improvement.";Test Augmentation or Improvement;LLM-Pure-Prompting;No Bmk-Ds;N/S;Test Coverage, % Build, % Pass;TestGen-LLM
P07;Automatic Generation of Test Cases based on Bug Reports: a Feasibility Study with Large Language Models;2024;Plein2024;C: ICSE;Conference;"@Inproceedings{Plein2024 ,
  author =       ""Plein, Laura and Ou\'{e}draogo, Wendk\^{u}uni C. and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F."",
  title =        ""Automatic Generation of Test Cases based on Bug Reports: a Feasibility Study with Large Language Models"",
  booktitle =    ""Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings"",
  series =       ""ICSE-Companion '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Lisbon, Portugal"",
  pages =        ""360-–361"",
  doi =          ""https://doi.org/10.1145/3639478.3643119"",
  url =          ""10.1145/3639478.3643119""
}";Survey;Software testing is a core discipline in software engineering where  a large array of research results has been produced, notably in the area of automatic test generation. Because existing approaches produce test cases that either can be qualified as simple (e.g., unit tests) or that require precise (and executable) specifications, most testing procedures still rely on test cases written by humans to form development project test suites. Such test suites, however, are incomplete: they only cover parts of the project or they are produced after the bug is fixed and therefore can only serve as regression tests. Yet, several research challenges, such as automatic program repair, and practitioner processes, such as continuous integration, build on the assumption that available test suites are sufficient. There is thus a need to break existing barriers in automatic test case generation. While prior work largely focused on random unit testing inputs, we propose to consider generating test cases that realistically represent complex user execution scenarios, which reveal buggy behaviour. Such scenarios are informally described in bug reports, which should therefore be considered as natural inputs for specifying bug-triggering test cases. In this work, we investigate the feasibility of performing this generation by leveraging large language models (LLMs) and using bug reports as inputs. Our experiments consider various settings, including the use of ChatGPT, as an online service for accessing an LLM, as well as CodeGPT, an existing code-related pre-trained LLM that was fine-tuned for our task. Our study is carried out on the Defects4J dataset. Overall, we experimentally show that bug reports associated to up to 50% of Defects4J bugs can prompt ChatGPT to generate an executable test case. We show that even new bug reports (i.e., previously-unseen data to mitigate data leakage threat to validity), can indeed be used as input for generating the executable test cases. Finally, we report experimental results which confirm that LLM-generated test cases are immediately useful in software engineering tasks such as fault localization as well as patch validation in automated program repair.;Unit Test Generation;LLM-Pure-Prompting;Defects4J;GPT-Family;Other;-
P08;Beyond Turing- Testing LLMs for Intelligence;2024;Savage2024;J: Com. ACM;Other;"@article{Savage2024,
  title = ""Beyond Turing: Testing LLMs for Intelligence"",
  author = ""Neil Savage"",
  journal = ""Communications of the ACM"",
  volume = ""67"",
  number = ""9"",
  pages = ""10--12"",
  year = ""2024"",
  publisher = ""Association for Computing Machinery"",
  doi = ""10.1145/3673427"",
  url = ""https://doi.org/10.1145/3673427""
}";Research Contribution;;Reflections;None;;;No Eval.;
P09;Can Large Language Models Write Good Property-Based Tests?;2024;Vikram2024;arXiv;arXiv;"@misc{Vikram2024,
  title = ""Can Large Language Models Write Good Property-Based Tests?"",
  author = ""Vasudev Vikram and Caroline Lemieux and Joshua Sunshine and Rohan Padhye"",
  archivePrefix = ""arXiv Preprint"",
  year = ""2024"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2307.04346"",
  url = ""https://doi.org/10.48550/arXiv.2307.04346""
}";Research Contribution;"Property-based testing (PBT), while an established technique in the software testing research community, is still relatively underused in real-world software. Pain points in writing property-based tests include implementing diverse random input generators and thinking of meaningful properties to test. Developers, however, are more amenable to writing documentation; plenty of library API documentation is available and can be used as natural language specifications for PBTs. As large language models (LLMs) have recently shown promise in a variety of coding tasks, we investigate using modern LLMs to automatically synthesize PBTs using two prompting techniques. A key challenge is to rigorously evaluate the LLM-synthesized PBTs. We propose a methodology to do so considering several properties of the generated tests: (1) validity, (2) soundness, and (3) property coverage, a novel metric that measures the ability of the PBT to detect property violations through generation of property mutants. In our evaluation on 40 Python library API methods across three models (GPT-4, Gemini-1.5-Pro, Claude-3-Opus), we find that with the best model and prompting approach, a valid and sound PBT can be synthesized in 2.4 samples on average. We additionally find that our metric for determining soundness of a PBT is aligned with human judgment of property assertions, achieving a precision of 100% and recall of 97%. Finally, we evaluate the property coverage of LLMs across all API methods and find that the best model (GPT-4) is able to automatically synthesize correct PBTs for 21% of properties extractable from API documentation.";Unit Test Generation;LLM-Pure-Prompting;Custom;Claude 3, Gemini, GPT-Family;Test Coverage, Mutation Rate, % Build, Other;-
P10;exLong: Generating Exceptional Behavior Tests with Large Language Models;2025;ZhangICSE2025;C: ICSE;Conference;"@Inproceedings{ZhangICSE2025 ,
  author =       ""Jiyang Zhang and Yu Liu and Pengyu Nie and Junyi Jessy Li and Milos Gligoric"",
  title =        ""exLong: Generating Exceptional Behavior Tests with Large Language Models"",
  booktitle =    ""Proceedings of the 47th International Conference on Software Engineering"",
  series =       ""ICSE '25"",
  editor =       """",
  volume =       """",
  year =         ""2025"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Ottawa,Ontario,Canada"",
  pages =        ""-"",
  doi =          ""10.48550/arXiv.2405.14619"",
  url =          ""https://doi.org/10.48550/arXiv.2405.14619""
}";Research Contribution;"Many popular programming languages, including C#, Java, and Python, support exceptions. Exceptions are thrown during program execution if an unwanted event happens, e.g., a method is invoked with an illegal argument value. Software developers write exceptional behavior tests (EBTs) to check that their code detects unwanted events and throws appropriate exceptions. Prior research studies have shown the importance of EBTs, but those studies also highlighted that developers put most of their efforts on ""happy paths"", e.g., paths without unwanted events. To help developers fill the gap, we present the first framework, dubbed exLong, that automatically generates EBTs. exLong is a large language model instruction fine-tuned from CodeLlama and embeds reasoning about traces that lead to throw statements, conditional expressions that guard throw statements, and non-exceptional behavior tests that execute similar traces. We compare exLong with the state-of-the-art models for test generation (CAT-LM) and one of the strongest foundation models (GPT-4o), as well as with analysis-based tools for test generation (Randoop and EvoSuite). Our results show that exLong outperforms existing models and tools. Furthermore, we contributed several pull requests to open-source projects and 23 EBTs generated by exLong were already accepted.";Unit Test Generation;LLM-Pure-FineTune;CodeSearchNet;CodeLlama, GPT-Family;XMatch, BLEU, CodeBLEU, EditSim, %Run, % Build, Test Coverage;exLong
P11;ChatUniTest- A Framework for LLM-Based Test Generation;2024;ChenFSE2024;C: FSE;Conference;"@Inproceedings{ChenFSE2024 ,
  author =       ""Chen, Yinghao and Hu, Zehao and Zhi, Chen and Han, Junxiao and Deng, Shuiguang and Yin, Jianwei"",
  title =        ""ChatUniTest: A Framework for LLM-Based Test Generation"",
  booktitle =    ""Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering"",
  series =       ""FSE '24'"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Porto de Galinhas, Brazil, Brazil"",
  pages =        ""572--576"",
  doi =          ""10.48550/arXiv.2305.04764"",
  url =          ""https://doi.org/10.48550/arXiv.2305.04764""
}";Research Contribution;Unit testing is an essential yet frequently arduous task. Various automated unit test generation tools have been introduced to mitigate this challenge. Notably, methods based on large language models (LLMs) have garnered considerable attention and exhibited promising results in recent years. Nevertheless, LLM-based tools encounter limitations in generating accurate unit tests. This paper presents ChatUniTest, an LLM-based automated unit test generation framework. ChatUniTest incorporates an adaptive focal context mechanism to encompass valuable context in prompts and adheres to a generation-validation-repair mechanism to rectify errors in generated unit tests. Subsequently, we have developed ChatUniTest Core, a common library that implements core workflow, complemented by the ChatUniTest Toolchain, a suite of seamlessly integrated tools enhancing the capabilities of ChatUniTest. Our effectiveness evaluation reveals that ChatUniTest outperforms TestSpark and EvoSuite in half of the evaluated projects, achieving the highest overall line coverage. Furthermore, insights from our user study affirm that ChatUniTest delivers substantial value to various stakeholders in the software testing domain. ChatUniTest is available at https: //github.com/ZJU-ACES-ISE/ChatUniTest, and the demo video is available at https://www.youtube.com/watch?v=GmfxQUqm2ZQ.;Unit Test Generation;LLM-Pure-Prompting;Apache-Commons;CodeLlama, GPT-Family;Test Coverage;ChatUnitTest
P12;"CODAMOSA: Escaping Coverage Plateaus in Test
Generation with Pre-trained Large Language Models";2023;Lemieux2024;C: ICSE;Conference;"@Inproceedings{Lemieux2024 ,
  author =       ""Lemieux, Caroline and Inala, Jeevana Priya and Lahiri, Shuvendu K and Sen, Siddhartha"",
  title =        ""Codamosa: Escaping coverage plateaus in test generation with pre-trained large language models"",
  booktitle =    ""Proceedings of the 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)"",
  series =       ""ICSE '23"",
  editor =       """",
  volume =       """",
  year =         ""2023"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Melbourne, Australia"",
  pages =        ""919--931"",
  doi =          ""https://doi.org/10.1109/ICSE48619.2023.00085"",
  url =          ""10.1109/ICSE48619.2023.00085""
}";Research Contribution;"Search-based software testing (SBST) generates
high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST’s performance
relies on there being a reasonable probability of generating
test cases that exercise the core logic of the program under
test. Given such test cases, SBST can then explore the space
around them to exercise various parts of the program. This paper
explores whether Large Language Models (LLMs) of code, such
as OpenAI’s Codex, can be used to help SBST’s exploration.
Our proposed algorithm, CODAMOSA, conducts SBST until its
coverage improvements stall, then asks Codex to provide example
test cases for under-covered functions. These examples help
SBST redirect its search to more useful areas of the search
space. On an evaluation over 486 benchmarks, CODAMOSA
achieves statistically significantly higher coverage on many more
benchmarks (173 and 279) than it reduces coverage on (10 and
4), compared to SBST and LLM-only baselines.";Test Augmentation or Improvement;Hybrid-Prompting;Pynguin, BugsInPy;OpenAI Codex;Test Coverage;
P13;Code-Aware Prompting- A Study of Coverage-Guided Test Generation in Regression Setting using LLM;2024;Ryan2024;J: Emp. Soft. Eng.;Journal;"@article{Ryan2024,
  title = ""Code-Aware Prompting: A Study of Coverage-Guided Test Generation in Regression Setting using LLM"",
  author = ""Gabriel Ryan and Siddhartha Jain and Mingyue Shang and Shiqi Wang and Xiaofei Ma and Murali Krishna Ramanathan and Baishakhi Ray"",
  journal = ""Proceedings of the ACM on Software Engineering"",
  volume = ""1"",
  number = ""4"",
  issue = ""FSE"",
  pages = ""951--971"",
  year = ""2024"",
  publisher = ""Association for Computing Machinery"",
  doi = ""10.1145/3643769"",
  url = ""https://doi.org/10.1145/3643769""
}
";Research Contribution;Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated testsuites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt’s approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a specific prompt aligned with the execution paths of the method under test, and exposing relevant type and dependency focal context to the model. Our approach enables pretrained LLMs to generate more complete test cases without any additional training. We implement SymPrompt using the TreeSitter parsing framework and evaluate on a benchmark challenging methods from open source Python projects. SymPrompt enhances correct test generations by a factor of 5 and bolsters relative coverage by 26% for CodeGen2. Notably, whenapplied to GPT-4, SymPrompt improves coverage by over 2× compared to baseline prompting strategies;Unit Test Generation;LLM-Pure-Prompting;BugsInPy, Pynguin;CodeGen 2, GPT-Family;% Build, % Pass, Test Coverage;SymPrompt
P14;Design choices made by LLM-based test generators prevent them from finding bugs;2024;Mathews2024;arXiv;arXiv;"@misc{Mathews2024,
  title = ""Design choices made by LLM-based test generators prevent them from finding bugs"",
  author = ""Noble Saji Mathews and Meiyappan Nagappan"",
  archivePrefix = ""arXiv Preprint"",
  year = ""2024"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2412.14137"",
  url = ""https://doi.org/10.48550/arXiv.2412.14137""
}";Research Contribution;There is an increasing amount of research and commercial tools for automated test case generation using Large Language Models (LLMs). This paper critically examines whether recent LLM-based test generation tools, such as Codium CoverAgent and CoverUp, can effectively find bugs or unintentionally validate faulty code. Considering bugs are only exposed by failing test cases, we explore the question: can these tools truly achieve the intended objectives of software testing when their test oracles are designed to pass? Using real humanwritten buggy code as input, we evaluate these tools, showing how LLM-generated tests can fail to detect bugs and, more alarmingly, how their design can worsen the situation by validating bugs in the generated test suite and rejecting bug-revealing tests. These f indings raise important questions about the validity of the design behind LLM-based test generation tools and their impact on software quality and test suite reliability.;Reflections;LLM-Pure-Prompting;Refactory;GPT-Family;Other;
P15;"Generating Test Scenarios from NL Requirements
using Retrieval-Augmented LLMs: An Industrial Study";2024;Chetan2024;C: RE;Conference;"@Inproceedings{Chetan2024 ,
  author =       ""Chetan Arora and Tomas Herda and Verena Homm"",
  title =        ""Training Large Language Models for System-Level Test Program Generation Targeting Non-functional Properties"",
  booktitle =    ""Proceedings of the IEEE 32nd International Requirements Engineering Conference (RE)"",
  series =       """",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Institute of Electrical and Electronics Engineers Inc."",
  address =      ""Reykjavik, Iceland"",
  pages =        ""240-251"",
  doi =          ""10.1109/re59067.2024.00031"",
  url =          ""https://doi.org/10.1109/re59067.2024.00031"",
  number =       """",
  month =        """",
  organization = """",
  note =         """",
}";Research Contribution;"Test scenarios are specific instances of test
cases that describe a sequence of actions to validate a
particular software functionality. By outlining the conditions
under which the software operates and the expected
outcomes, test scenarios ensure that the software
functionality is tested in an integrated manner. Test
scenarios are crucial for systematically testing an application
under various conditions, including edge cases,
to identify potential issues and guarantee overall performance
and reliability. Manually specifying test scenarios
is tedious and requires a deep understanding of software
functionality and the underlying domain. It further
demands substantial effort and investment from already
time- and budget-constrained requirements engineers
and testing teams. This paper presents an automated
approach (RAGTAG) for test scenario generation using
Retrieval-Augmented Generation (RAG) with Large
Language Models (LLMs). RAG allows the integration
of specific domain knowledge with LLMs’ generation
capabilities. We evaluate RAGTAG on two industrial
projects from Austrian Post with bilingual requirements
in German and English. Our results from an interview
survey conducted with four experts on five dimensions
– relevance, coverage, correctness, coherence and feasibility,
affirm the potential of RAGTAG in automating
test scenario generation. Specifically, our results indicate
that, despite the difficult task of analyzing bilingual
requirements, RAGTAG is able to produce scenarios that
are well-aligned with the underlying requirements and
provide coverage of different aspects of the intended
functionality. The generated scenarios are easily understandable
to experts and feasible for testing in the
project environment. The overall correctness is deemed
satisfactory; however, gaps in capturing exact action
sequences and domain nuances remain, underscoring
the need for domain expertise when applying LLMs.";High-Level Test Gen;LLM-Pure-Prompting;Custom;GPT-Family;ROUGE, BLEU, METEOR;Approach not a tool
P16;Large-scale, Independent and Comprehensive study of the power of LLMs for test case generation;2024;OuedraogoarXiv2024;arXiv;arXiv;"@misc{OuedraogoarXiv2024,
  title = ""Large-scale, Independent and Comprehensive study of the power of LLMs for test case generation"",
  author = ""Wendkûuni C. Ouédraogo and Kader Kaboré and Haoye Tian and Yewei Song and Anil Koyuncu and Jacques Klein and David Lo and Tegawendé F. Bissyandé"",
  archivePrefix = ""arXiv Preprint"",
  year = ""2025"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2407.00225"",
  url = ""https://doi.org/10.48550/arXiv.2407.00225""
}";Research Contribution;Unit testing, crucial for ensuring the reliability of code modules, such as classes and methods, is often overlooked by developers due to time constraints. Automated test generation techniques have emerged to address this, but they frequently lack readability and require significant developer intervention. Large Language Models (LLMs), such as GPT and Mistral, have shown promise in software engineering tasks, including test generation, but their overall effectiveness remains unclear. This study presents an extensive investigation of LLMs, evaluating the effectiveness of four models and five prompt engineering techniques for unit test generation. We analyze 216300 tests generated by the selected advanced instruct-tuned LLMs for 690 Java classes collected from diverse datasets. Our evaluation considers correctness, understandability, coverage, and test smell detection in the generated tests, comparing them to a widely used automated testing tool, EvoSuite. While LLMs demonstrate potential, improvements in test quality—particularly in reducing common test smells—are necessary. This study highlights the strengths and limitations of LLM-generated tests compared to traditional methods, paving the way for further research on LLMs in test automation.;Reflections;LLM-Pure-Prompting;Defects4J, Custom, SF11;Mistral, GPT-Family;MSR, CSR, % Build, % Pass, Test Coverage, Static Analysis Metrics, % Test Smells;-
P17;Leveraging Large Language Models to Improve REST API Testing;2024;Myeongsoo2024;C: ICSE;Conference;"@Inproceedings{Myeongsoo2024 ,
  author =       ""Kim, Myeongsoo and Stennett, Tyler and Shah, Dhruv and Sinha, Saurabh and Orso, Alessandro"",
  title =        ""Leveraging Large Language Models to Improve REST API Testing"",
  booktitle =    ""Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results"",
  series =       ""ICSE-NIER'24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Lisbon, Portugal"",
  pages =        ""37–-41"",
  doi =          ""https://doi.org/10.1145/3639476.3639769"",
  url =          ""10.1145/3639476.3639769""
}";Research Contribution;Thewidespread adoption of REST APIs, coupled with their growing complexity and size, has led to the need for automated REST API testing tools. Current tools focus on the structured data in REST API specifications but often neglect valuable insights available in unstructured natural-language descriptions in the specifications, which leads to suboptimal test coverage. Recently, to address this gap, researchers have developed techniques that extract rules from these human-readable descriptions and query knowledge bases to derive meaningful input values. However, these techniques are limited in the types of rules they can extract and prone to produce inaccurate results. This paper presents RESTGPT, an innovative approach that leverages the power and intrinsic context-awareness of Large Language Models (LLMs) to improve REST API testing. RESTGPT takes as input an API specification, extracts machineinterpretable rules, and generates example parameter values from natural-language descriptions in the specification. It then augments the original specification with these rules and values. Our evaluations indicate that RESTGPT outperforms existing techniques in both rule extraction and value generation. Given these promising results, we outline future research directions for advancing REST API testing through LLMs.;Test Augmentation or Improvement;LLM-Pure-Prompting;NLP2REST;GPT-Family;Other;
P18;LLM4Fin: Fully Automating LLM-Powered Test Case Generation for FinTech Software Acceptance Testing;2024;XueISSTA2024;C: SIGSOFT;Conference;"@Inproceedings{XueISSTA2024 ,
  author =       ""Zhiyi Xue and Liangguo Li and Senyue Tian and Xiaohong Chen and Pingping Li and Liangyu Chen and Tingting Jiang and Min Zhang"",
  title =        ""LLM4Fin: Fully Automating LLM-Powered Test Case Generation for FinTech Software Acceptance Testing"",
  booktitle =    ""ISSTA 2024 - Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis"",
  series =       """",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery, Inc"",
  address =      ""Vienna, Austria"",
  pages =        ""1643-1655"",
  doi =          ""10.1145/3650212.3680388"",
  url =          ""https://dl.acm.org/doi/10.1145/3650212.3680388"",
  number =       """",
  month =        ""9"",
  organization = """",
  note =         """",
}";Research Contribution;FinTech software, crucial for both safety and timely market deployment, presents a compelling case for automated acceptance testing against regulatory business rules. However, the inherent challenges of comprehending unstructured natural language descriptions of these rules and crafting comprehensive test cases demand human intelligence. The emergence of Large Language Models (LLMs) holds promise for automated test case generation, leveraging their natural language processing capabilities. Yet, their dependence on human intervention for effective prompting hampers efficiency. In response, we introduce a groundbreaking, fully automated approach for generating high-coverage test cases from natural language business rules. Our methodology seamlessly integrates the versatility of LLMs with the predictability of algorithmic methods. We fine-tune pre-trained LLMs for improved information extraction accuracy and algorithmically generate comprehensive testable scenarios for the extracted business rules.Our prototype, LLM4Fin, is designed for testing real-world stock-trading software. Experimental results demonstrate LLM4Fin's superiority over both state-of-the-art LLM, such as ChatGPT, and skilled testing engineers. We achieve remarkable performance, with up to 98.18% and an average of 20%-110% improvement on business scenario coverage, and up to 93.72% on code coverage, while reducing the time cost from 20 minutes to a mere 7 seconds. These results provide robust evidence of the framework's practical applicability and efficiency, marking a significant advancement in FinTech software testing.;High-Level Test Gen;LLM-Pure-FineTune;Custom;ChatGLM, FinBert, Mengzi, Llama-Family, GPT-Family;Test Coverage, BS Coverage;LLMFin
P19;Navigating Confidentiality in Test Automation: A Case Study in LLM Driven Test Data Generation;2024;Karmarkar2024;C: SANER;Conference;"@Inproceedings{Karmarkar2024 ,
  author =       ""Karmarkar, Hrishikesh and Agrawal, Supriya and Chauhan, Avriti and Shete, Pranav"",
  title =        ""Navigating Confidentiality in Test Automation: A Case Study in LLM Driven Test Data Generation"",
  booktitle =    ""2024 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)"",
  series =       """",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Rovaniemi , Finland"",
  pages =        ""337--348"",
  doi =          ""10.1109/SANER60148.2024.00041"",
  url =          ""https://doi.org/10.1109/SANER60148.2024.00041""
}";Research Contribution;"In outsourced industrial projects for testing of web applications, often neither the application
to be tested, nor its source code are provided to the
testing team, due to confidentiality reasons, making
systematic testing of these applications very challenging. However, textual descriptions of such systems are
often available. So, one can consider leveraging a Large
Language Model (LLM) to parse these descriptions
and synthesize test generators (programs that produce
test data). In our experience, LLM synthesized test
generators suffer from two problems:- (1) unsound:
the generators might produce invalid data and (2)
incomplete: the generators typically fail to generate all
expected valid inputs. To mitigate these problems, we
introduce TestRefineGen a method for autonomously
generating test data from textual descriptions. TestRefineGen begins by invoking an LLM to parse a given
corpus of documents and produce multiple test generators. It then uses a novel ranking approach to identify
generators that can produce invalid test data, and then
automatically repairs them using a counterexampleguided refinement process. Lastly, TestRefineGen performs a generalization procedure that offsets synthesis
or refinements that leads to incompleteness, to obtain
generators that produce more comprehensive valid inputs.
We evaluated the effectiveness of TestRefineGen on
a manually curated set of 256 textual descriptions of
test data. TestRefineGen synthesized generators that
produce valid test data for 66.01 % of the descriptions.
Using a combination of post-processing sanitisation and
refinement it was able to successfully repair synthesized
generators, which improved the success rate to 76.95
%. Further, our statistical analysis on a small subset
of synthesized generators shows that TestRefineGen is
able to generate test data that is well distributed across
the input space. Thus, TestRefineGen can be an effective technique for autonomous test data generation for
web testing in projects with confidentiality concerns.
Index Terms—automated test input generation, testing, refinement, Large Language Models";High-Level Test Gen;LLM-Pure-Prompting;Custom;GPT-Family;;TestReﬁneGen
P20;Evaluating and Improving ChatGPT for Unit Test Generation;2024;Yuan2024;J: ACM Soft. Eng.;Conference;"@article{Yuan2024,
  title = ""Evaluating and Improving ChatGPT for Unit Test Generation"",
  author = ""Yuan, Zhiqiang and Liu, Mingwei and Ding, Shiji and Wang, Kaixin and Chen, Yixuan and Peng, Xin and Lou, Yiling"",
  journal = ""Proceedings of the ACM on Software Engineering"",
  volume = ""1"",
  number = ""76"",
  issue = ""FSE"",
  pages = """",
  year = ""2024"",
  publisher = ""Association for Computing Machinery"",
  doi = ""10.1145/3660783"",
  url = ""https://doi.org/10.1145/3660783""
}";Research Contribution;"Unit testing plays an essential role in detecting bugs in functionally-discrete program units (e.g., methods). Manually writing high-quality unit tests is time-consuming and laborious. Although the traditional techniques are able to generate tests with reasonable coverage, they are shown to exhibit low readability and still cannot be directly adopted by developers in practice. Recent work has shown the large potential of large language models (LLMs) in unit test generation. By being pre-trained on a massive developer-written code corpus, the models are capable of generating more human-like and meaningful test code. In this work, we perform the rst empirical study to evaluate the capability of ChatGPT (i.e., one of the most representative LLMs with outstanding performance in code generation and comprehension) in unit test generation. In particular, we conduct both a quantitative analysis and a user study to systematically investigate the quality of its generated tests in terms of correctness, su ciency, readability, and usability. We nd that the tests generated by ChatGPT still su er from correctness issues, including diverse compilation errors and execution failures (mostly caused by incorrect assertions); but the passing tests generated by ChatGPT almost resemble manually-written tests by achieving comparable coverage, readability, and even sometimes developers’ preference. Our ndings indicate that generating unit tests with ChatGPT could be very promising if the correctness of its generated tests could be further improved. Inspired by our ndings above, we further propose ChatTester, a novel ChatGPT-based unit test generation approach, which leverages ChatGPT itself to improve the quality of its generated tests. ChatTester incorporates an initial test generator and an iterative test re ner. Our evaluation demonstrates the e ectiveness of ChatTester by generating 34.3% more compilable tests and 18.7% more tests with correct assertions than the default ChatGPT. In addition to ChatGPT, we further investigate the generalization capabilities of ChatTester by applying it to two recent open-source LLMs (i.e., CodeLlama-Instruct and CodeFuse) and our results show that ChatTester can also improve the quality of tests generated by these LLMs.";Unit Test Generation;LLM-Pure-Prompting;CodeSearchNet;CodeLlama, CodeFuse, GPT-Family;% Build, % Pass, Test Coverage, Other;-
P21;On the Use of Large Language Models in Mutation Testing;2025;WangarXiv2024;arXiv;arXiv;"@misc{WangarXiv2024,
  title = ""An Exploratory Study on Using Large Language Models for Mutation Testing"",
  author = ""Bo Wang and Mingda Chen and Youfang Lin and Mike Papadakis and Jie M. Zhang"",
  archivePrefix = ""arXiv Preprint"",
  year = ""2024"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2406.09843"",
  url = ""https://doi.org/10.48550/arXiv.2406.09843""
}";Research Contribution;Mutation testing is a foundation approach in the software testing field, based on automatically seeded small syntactic changes, known as mutations. The question of how to generate high-utility mutations, to be used for testing purposes, forms a key challenge in mutation testing literature. Large Language Models (LLMs) have shown great potential in code-related tasks but their utility in mutation testing remains unexplored. To this end, we systematically investigate the performance of LLMs in generating effective mutations w.r.t. to their usability, fault detection potential, and relationship with real bugs. In particular, we perform a large-scale empirical study involving six LLMs, including both state-of-the-art open- and closed-source models, and 851 real bugs on two Java benchmarks (i.e., 605 bugs from 12 projects of Defects4J 2.0 and 246 bugs of ConDefects). Wefindthat compared to existing approaches, LLMs generate more diverse mutations that are behaviorally closer to real bugs, which leads to approximately 19% higher fault detection than current approaches (i.e., 93% vs. 74%). Nevertheless, the mutants generated by LLMs have worse compilability rate, useless mutation rate, and equivalent mutation rate than those generated by rule-based approaches. This paper also examines alternative prompt engineering strategies and identifies the root causes of uncompilable mutations, providing insights for researchers to further enhance the performance of LLMs in mutation testing.;Test Augmentation or Improvement;LLM-Pure-Prompting;Defects4J, ConDefects;CodeLlama, DeepSeekCoder, StarChat, GPT-Family;Bugs Rate, % Build, Other;
P22;Tasks People Prompt- A Taxonomy of LLM Downstream Tasks in Software Verification and Falsification Approaches;2024;Braberman2024;arXiv;arXiv;"@misc{Braberman2024,
  title = ""Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software Verification and Falsification Approaches"",
  author = ""Víctor A Braberman and Flavia Bonomo-Braberman and Yiannis Charalambous and Juan G Colonna and Lucas C Cordeiro and Rosiane de Freitas"",
  archivePrefix = ""arXiv Preprint"",
  year = ""2024"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2404.09384"",
  url = ""https://doi.org/10.48550/arXiv.2404.09384""
}";Research Contribution;Prompting has become one of the main approaches to leverage emergent capabilities of Large Language Models [Brown et al. NeurIPS 2020, Wei et al. TMLR 2022, Wei et al. NeurIPS 2022]. Recently, researchers and practitioners have been “playing” with prompts (e.g., In-Context Learning) to see how to make the most of pre-trained Language Models. By homogeneously dissecting more than a hundred articles, we investigate how software testing and verification research communities have leveraged LLMs capabilities. First, we validate that downstream tasks are adequate to convey a nontrivial modular blueprint of prompt-based proposals in scope. Moreover, we name and classify the concrete downstream tasks we recover in both validation research papers and solution proposals. In order to perform classification, mapping, and analysis, we also develop a novel downstream-task taxonomy. The main taxonomy requirement is to highlight commonalities while exhibiting variation points of task types that enable pinpointing emerging patterns in a varied spectrum of Software Engineering problems that encompasses testing, fuzzing, fault localization, vulnerability detection, static analysis, and program verification approaches. Avenues for future research are also discussed based on conceptual clusters induced by the taxonomy;Reflections;None;;;;
P23;Test Oracle Automation in the era of LLMs;2024;Molina2024;J: TOSEM;Journal;"@article{Molina2024,
  title = ""Test Oracle Automation in the Era of LLMs"",
  author = ""Molina, Facundo and Gorla, Alessandra and d’Amorim, Marcelo"",
  journal = ""ACM Transactions on Software Engineering and Methodology"",
  month = ""jan"",
  year = ""2025"",
  address = ""New York, NY, USA"",
  publisher = ""Association for Computing Machinery"",
  doi = ""10.1145/3715107"",
  url = ""https://doi.org/10.1145/3715107""
}";Research Contribution;The effectiveness of a test suite in detecting faults highly depends on the correctness and completeness of its test oracles. Large Language Models (LLMs) have already demonstrated remarkable proficiency  in tackling diverse software testing tasks, such as automated test generation and program repair. This paper aims to enable discussions on the potential of using LLMs for test oracle automation, along with the challenges that may emerge during the generation of various types of oracles. Additionally, our aim is to initiate discussions on the primary threats that SE researchers must consider when employing LLMs for oracle automation, encompassing concerns regarding oracle deficiencies and data leakages.;Reflections, Oracle Generation;None;;;;
P24;TESTEVAL- Benchmarking Large Language Models for Test Case Generation;2025;WangTESTEVAL2024;C: Other;Conference;"@Inproceedings{WangTESTEVAL2024 ,
  author =       ""Wang, Wenhan  and
      Yang, Chenyuan  and
      Wang, Zhijie  and
      Huang, Yuheng  and
      Chu, Zhaoyang  and
      Song, Da  and
      Zhang, Lingming  and
      Chen, An Ran  and
      Ma, Lei"",
  title =        ""TestEval: Benchmarking Large Language Models for Test Case Generation"",
  booktitle =    ""Findings of the Association for Computational Linguistics: NAACL 2025"",
  series =       ""NAACL '25"",
  editor =       """",
  volume =       """",
  year =         ""2025"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Albuquerque, New Mexico"",
  pages =        ""3547--3562"",
  doi =          ""10.18653/v1/2025.findings-naacl.197"",
  url =          ""https://doi.org/10.18653/v1/2025.findings-naacl.197""
}";Research Contribution;"Testing plays a crucial role in the software development cycle, enabling the detection of bugs, vulnerabilities, and other undesirable behaviors. To perform software testing, testers need to write code snippets that execute the program under test. Recently, researchers have recognized the potential of large language models (LLMs) in software testing. However, there remains a lack of fair comparisons between different LLMs in terms of test case generation capabilities.
In this paper, we propose TESTEVAL, a novel benchmark for test case generation with LLMs. We collect 210 Python programs from an online programming platform, LeetCode, and design three different tasks: overall coverage, targeted line/branch coverage, and targeted path coverage. We further evaluate sixteen popular LLMs, including both commercial and open-source ones, on TESTEVAL. We find that generating test cases to cover specific program lines/branches/paths is still challenging for current LLMs, indicating a lack of ability to comprehend program logic and execution paths. We have open-sourced our dataset and benchmark pipelines at this https URL.";Unit Test Generation;LLM-Pure-Prompting;TESTEVAL;Gemini, Gemma, DeepSeekCoder, CodeQwen, Mistral, Starcoder-2, GPT-Family, Llama-Family;% Build, % Pass, Test Coverage;
P25;Towards Understanding the Effectiveness of Large Language Models on Directed Test Input Generation;2024;Jiang2024;C: ASE;Conference;"@Inproceedings{Jiang2024 ,
  author =       ""Zongze Jiang and Ming Wen and Jialun Cao and Xuanhua Shi and Hai Jin"",
  title =        ""{Towards Understanding the Effectiveness of Large Language Models on Directed Test Input Generation"",
  booktitle =    ""Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering"",
  series =       ""ASE '25'"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Seoul , South Korea"",
  pages =        ""1408--1420"",
  doi =          ""10.1145/3691620.3695513"",
  url =          ""https://doi.org/10.1145/3691620.3695513""
}";Research Contribution;"Automatic testing has garnered significant attention and success
over the past few decades. Techniques such as unit testing and
coverage-guided fuzzing have revealed numerous critical software
bugs and vulnerabilities. However, a long-standing, formidable
challenge for existing techniques is how to achieve higher testing
coverage. Constraint-based techniques, such as symbolic execution
and concolic testing, have been well-explored and integrated into
the existing approaches. With the popularity of Large Language
Models (LLMs), recent research efforts to design tailored prompts
to generate inputs that can reach more uncovered target branches.
However, the effectiveness of using LLMs for generating such directed inputs and the comparison with the proven constraint-based
solutions has not been systematically explored.
To bridge this gap, we conduct the first systematic study on the
mainstream LLMs and constraint-based tools for directed input
generation with a comparative perspective. We find that LLMs such
as ChatGPT are comparable to or even better than the constraintbased tools, succeeding in 43.40%-58.57% samples in our dataset.
Meanwhile, there are also limitations for LLMs in specific scenarios
such as sequential calculation, where constraint-based tools are
in a position of strength. Based on these findings, we propose a simple yet effective method to combine these two types of tools and
implement a prototype based on ChatGPT and constraint-based
tools. Our evaluation shows that our approach can outperform the
baselines by 1.4x to 2.3x relatively. We believe our study can provide
novel insights into directed input generation using LLMs, and our
findings are essential for future testing research.
";Unit Test Generation;Hybrid-Prompting;logic_bombs, PathEval;CodeLlama, Starcoder-2, CodeQwen, GPT-Family;Other;-
P26;Towards Autonomous Testing Agents via Conversational Large Language Models;2023;Feldt2023;C: ASE;Conference;"@Inproceedings{Feldt2023 ,
  author =       ""Robert Feldt and Sungmin Kang and Juyeon Yoon and Shin Yoo"",
  title =        ""Towards Autonomous Testing Agents via Conversational Large Language Models"",
  booktitle =    ""Proceedings of the 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)"",
  series =       ""ASE '23'"",
  editor =       """",
  volume =       """",
  year =         ""2023"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Kirchberg, Luxembourg"",
  pages =        ""1688--1693"",
  doi =          ""10.1109/ASE56229.2023.00148"",
  url =          ""https://doi.org/10.1109/ASE56229.2023.00148""
}";Research Contribution;Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized “hallucination” of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations. Index Terms—software testing, machine learning, large language model, artificial intelligence, test automation;Test Agents;LLM-Pure-Prompting;No Bmk-Ds;GPT-Family;No Eval.;SOCRATEST
P28;Unit Test Generation using Generative AI;2024;Bhatia2024;C: Other;Conference;"@Inproceedings{Bhatia2024 ,
  author =       ""Bhatia, Shreya and Gandhi, Tarushi and Kumar, Dhruv and Jalote, Pankaj"",
  title =        ""Unit Test Generation using Generative AI : A Comparative Performance Analysis of Autogeneration Tools"",
  booktitle =    ""Proceedings of the 1st International Workshop on Large Language Models for Code"",
  series =       ""LLM4Code '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Lisbon, Portugal"",
  pages =        ""54–61"",
  doi =          ""10.1145/3643795.3648396"",
  url =          ""https://doi.org/10.1145/3643795.3648396""
}";Survey;Generating unit tests is a crucial task in software development, demanding substantial time and effort from programmers. The advent of Large Language Models (LLMs) introduces a novel avenue for unit test script generation. This research aims to experimentally investigate the effectiveness of LLMs, specifically exemplified by ChatGPT, for generating unit test scripts for Python programs, and how the generated test cases compare with those generated by an existing unit test generator (Pynguin). For experiments, we consider three types of code units: 1) Procedural scripts, 2) Function-based modular code, and 3) Class-based code. The generated test cases are evaluated based on criteria such as coverage, correctness, and readability. Our results show that ChatGPT’s performance is comparable with Pynguin in terms of coverage, though for some cases its performance is superior to Pynguin. We also find that about a third of assertions generated by ChatGPT for some categories were incorrect. Our results also show that there is minimal overlap in missed statements between ChatGPT and Pynguin, thus, suggesting that a combination of both tools may enhance unit test generation performance. Finally, in our experiments, prompt engineering improved ChatGPT’s performance, achieving a much higher coverage.*These authors contributed equally.;Unit Test Generation;LLM-Pure-Prompting;Pynguin;GPT-Family;Test Coverage, % Pass;-
P29;You Name It, I Run It- An LLM Agent to Execute Tests of Arbitrary Projects;2024;Bouzenia2024;J: Proc. ACM Softw. Eng;Journal;"@article{Bouzenia2024,
  title = ""You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects"",
  author = ""Bouzenia, Islem and Pradel, Michael"",
  journal = ""Proc. ACM Softw. Eng."",
  month = ""jan"",
  year = ""2025"",
  volume = ""2"",
  number = ""ISSTA047"",
  pages = ""23"",
  address = ""New York, NY, USA"",
  publisher = ""Association for Computing Machinery"",
  doi = ""10.1145/3728922"",
  url = ""https://doi.org/10.1145/3728922""
}";Research Contribution;The ability to execute the test suite of a project is essential in many scenarios, e.g., to assess code quality and code coverage, to validate code changes made by developers or automated tools, and to ensure compatibility with dependencies. Despite its importance, executing the test suite of a project can be challenging in practice because different projects use different programming languages, software ecosystems, build systems, testing frameworks, and other tools. These challenges make it difficult to create a reliable, universal test execution method that works across different projects. This paper presents ExecutionAgent, an automated technique that prepares scripts for building an arbitrary project from source code and running its test cases. Inspired by the way a human developer would address this task, our approach is a large language model (LLM)-based agent that autonomously executes commands and interacts with the host system. The agent uses meta-prompting to gather guidelines on the latest technologies related to the given project, and it iteratively refines its process based on feedback from the previous steps. Our evaluation applies ExecutionAgent to 50 open-source projects that use 14 different programming languages and many different build and testing tools. The approach successfully executes the test suites of 33/50 projects, while matching the test results of ground truth test suite executions with a deviation of only 7.5%. These results improve over the best previously available technique by 6.6x. The costs imposed by the approach are reasonable, with an execution time of 74 minutes and LLM costs of USD 0.16, on average per project. We envision ExecutionAgent to serve as a valuable tool for developers, automated programming tools, and researchers that need to execute tests across a wide variety of projects.;Test Agents;LLM-Pure-Prompting;Custom;GPT-Family;% Build, % Pass;
P30;Software System Testing Assisted by Large Language Models: An Exploratory Study;2025;Augusto2025;C: ICTSS;Conference;"@Inproceedings{Augusto2025 ,
  author =       ""Cristian Augusto and Jesús Morán and Antonia Bertolino and Claudio de la Riva and Javier Tuya"",
  title =        ""Software System Testing Assisted by Large Language Models: An Exploratory Study"",
  booktitle =    ""Testing Software and Systems"",
  series =       ""LLM4Code '24"",
  editor =       ""Gema and Barnard Pepitaand Bautista John Robertand Farahi Aryaand Dash Santanuand Han DongGyunand Fortz Sophieand Rodriguez-Fernandez Victor Menéndez Héctor D.and Bello-Orgaz"",
  volume =       """",
  year =         ""2025"",
  publisher =    ""Springer Nature Switzerland"",
  address =      ""London, United Kingdom"",
  pages =        ""239--255"",
  doi =          ""10.1007/978-3-031-80889-0_17"",
  url =          ""https://doi.org/10.1007/978-3-031-80889-0_17""
}";Research Contribution;"Large language models (LLMs) based on transformer architecture have revolutionized natural language processing (NLP), demonstrating excellent capabilities in understanding and generating human-like text. In Software Engineering,
LLMs have been applied in code generation, documentation, and report writing tasks, to support the developer and reduce the amount of manual work. In Software Testing, one of the cornerstones of Software Engineering, LLMs have been
explored for generating test code, test inputs, automating the oracle process or generating test scenarios. However, their application to high-level testing stages such as system testing, in which a deep knowledge of the business and the technological stack is needed, remains largely unexplored. This paper presents an exploratory study about how LLMs can support system test development. Given that LLM performance depends on input data quality, the study focuses on how to query general purpose LLMs to first obtain test scenarios and then derive test cases from them. The study evaluates two popular LLMs (GPT-4o and GPT-4o-mini), using as a benchmark a European project demonstrator. The study compares two different prompt strategies and employs well-established prompt patterns, showing promising results as well as room for improvement in the application of LLMs to support system testing.";High-Level Test Gen;LLM-Pure-Prompting;Custom;GPT-Family;BS Coverage, Other;-
P31;Navigating the Complexity of Generative AI Adoption in Software Engineering;2024;Russo2024;J: TOSEM;Journal;"@article{Russo2024,
  title = ""Navigating the Complexity of Generative AI Adoption in Software Engineering"",
  author = ""Russo, Daniel"",
  journal = ""ACM Transactions on Software Engineering and Methodology"",
  month = ""jan"",
  year = ""2025"",
  volume = ""33"",
  number = ""5"",
  pages = ""1--50"",
  address = ""New York, NY, USA"",
  publisher = ""Association for Computing Machinery"",
  doi = ""10.1145/3652154"",
  url = ""https://doi.org/10.1145/3652154""
}";Survey;This article explores the adoption ofGenerativeArtificial Intelligence (AI) tools within the domainofsoftware engineering, focusing on the influencing factors at the individual, technological, and social levels. We applied a convergent mixed-methods approach to offer a comprehensive understanding of AI adoption dynamics. We initially conducted a questionnaire survey with 100 software engineers, drawing upon the Technology AcceptanceModel,theDiffusionofInnovationTheory,andtheSocialCognitiveTheoryasguidingtheoretical frameworks. Employing the Gioia methodology, we derived a theoretical model of AI adoption in software engineering: the Human-AI Collaboration and Adaptation Framework. This model was then validated using Partial Least Squares–Structural Equation Modeling based on data from 183 software engineers. Findings indicate that at this early stage of AI integration, the compatibility of AI tools within existing development workflows predominantly drives their adoption, challenging conventional technology acceptance theories. The impact of perceived usefulness, social factors, and personal innovativeness seems less pronounced than expected. The study provides crucial insights for future AI tool design and offers a framework for developing effective organizational implementation strategies.;Reflections;None;;;;
P32;Rethinking the Influence of Source Code on Test Case Generation;2024;Huang2024;C: Other;Conference;"@Inproceedings{Huang2024 ,
  author =       ""Dong Huang and Jie M. Zhang and Mingzhe Du and Mark Harman and Heming Cui"",
  title =        ""Rethinking the Influence of Source Code on Test Case Generation"",
  booktitle =    ""Proceedings of the 31st International Conference on Computational Linguistics"",
  series =       """",
  editor =       """",
  volume =       """",
  year =         ""2025"",
  publisher =    ""Association for Computational Linguistics"",
  address =      ""Abu Dhabi, UAE"",
  pages =        ""3043-–3056"",
  url =          ""https://aclanthology.org/2025.coling-main.131/""
}";Research Contribution;Large language models (LLMs) have been widely applied to assist test generation with the source code under test provided as the context. This paper aims to answer the question: If the source code under test is incorrect, will LLMs be misguided when generating tests? The effectiveness of test cases is measured by their accuracy, coverage, and bug detection effectiveness. Our evaluation results with five open- and six closed-source LLMs on four datasets demonstrate that incorrect code can significantly mislead LLMs in generating correct, high-coverage, and bug-revealing tests. For instance, in the HumanEval dataset, LLMs achieve 80.45% test accuracy when provided with task descriptions and correct code, but only 57.12% when given task descriptions and incorrect code. For the APPS dataset, prompts with correct code yield tests that detect 39.85% of the bugs, while prompts with incorrect code detect only 19.61%. These findings have important implications for the deployment of LLM-based testing — using it on mature code may help protect against future regression, but on early-stage immature code, it may simply bake in errors. Our findings also underscore the need for further research to improve LLMs’ resilience against incorrect code in generating reliable and bug-revealing tests;Unit Test Generation;LLM-Pure-Prompting;HumanEval, BugsInPy, Custom, MBPP, APPS;CodeLlama, DeepSeekCoder, Starcoder-2, CodeStral, Claude 3, GPT-Family, Llama-Family;Test Coverage, Bugs Rate, % Pass, Other;No tool
P33;Testing the limits: Unusual text inputs generation for mobile app crash detection with large language model;2024;Liu2024;C: ICSE;Conference;"@Inproceedings{Liu2024 ,
  author =       ""Liu, Zhe and Chen, Chunyang and Wang, Junjie and Chen, Mengzhuo and Wu, Boyu and Tian, Zhilin and Huang, Yuekai and Hu, Jun and Wang, Qing"",
  title =        ""Testing the Limits: Unusual Text Inputs Generation for Mobile App Crash Detection with Large Language Model"",
  booktitle =    ""Proceedings of the IEEE/ACM 46th International Conference on Software Engineering"",
  series =       ""ICSE '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Lisbon, Portugal"",
  pages =        ""1--12"",
  doi =          ""10.1145/3597503.3639118"",
  url =          ""https://doi.org/10.1145/3597503.3639118""
}";Research Contribution;Mobile applications have become a ubiquitous part of our daily life, providing users with access to various services and utilities. Text input, as an important interaction channel between users and applications, plays an important role in core functionality such as search queries, authentication, messaging, etc. However, certain special text (e.g., -18 for Font Size) can cause the app to crash, and generating diversified unusual inputs for fully testing the app is highly demanded. Nevertheless, this is also challenging due to the combination of explosion dilemma, high context sensitivity, and complex constraint relations. This paper proposes InputBlaster which leverages the LLM to automatically generate unusual text inputs for mobile app crash detection. It formulates the unusual inputs generation problem as a task of producing a set of test generators, each of which can yield a batch of unusual text inputs under the same mutation rule. In detail, InputBlaster leverages LLM to produce the test generators together with the mutation rules serving as the reasoning chain, and utilizes the in-context learning schema to demonstrate the LLM with examples for boosting the performance. InputBlaster is evaluated on 36 text input widgets with cash bugs involving 31 popular Android apps, and results show that it achieves 78% Bugs Rate, with 136% higher than the best baseline. Besides, we integrate it with the automated GUI testing tool and detect 37 unseen crashes in real-world apps.;High-Level Test Gen;LLM-Pure-Prompting;Custom;GPT-Family;Bugs Rate;
P34;LLMs and Prompting for Unit Test Generation: A Large-Scale Evaluation;2024;OuedraogoASE2024;C: ASE;Conference;"@Inproceedings{OuedraogoASE2024 ,
  author =       ""Ouedraogo, Wendkuuni C. and Kabore, Kader and Tian, Haoye and Song, Yewei and Koyuncu, Anil and Klein, Jacques and Lo, David and Bissyande, Tegawende F."",
  title =        ""LLMs and Prompting for Unit Test Generation: A Large-Scale Evaluation"",
  booktitle =    ""Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering"",
  series =       ""ASE '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Sacramento, CA, USA"",
  pages =        ""2464–-2465"",
  doi =          ""10.1145/3691620.3695330"",
  url =          ""https://doi.org/10.1145/3691620.3695330""
}";Research Contribution;Unit testing, essential for identifying bugs, is often neglected due to time constraints. Automated test generation tools exist but typically lack readability and require developer intervention. Large Language Models (LLMs) like GPT and Mistral show potential in test generation, but their effectiveness remains unclear. This study evaluates four LLMs and five prompt engineering techniques, analyzing 216300 tests for 690 Java classes from diverse datasets. We assess correctness, readability, coverage, and bug detection, comparing LLM-generated tests to EvoSuite. While LLMs show promise, improvements in correctness are needed. The study highlights both the strengths and limitations of LLMs, offering insights for future research.;Unit Test Generation;LLM-Pure-Prompting;SF11, Defects4J, Custom;Mistral, GPT-Family;% Test Correct, % Build, Test Coverage, Bugs Rate;
P35;AugmenTest: Enhancing Tests with LLM-Driven Oracles;2025;Khandaker2025;C: ICST;Conference;"@article{Khandaker2025,
  title = ""AugmenTest: Enhancing Tests with LLM-Driven Oracles"",
  author = ""Khandaker, Shaker Mahmud and Kifetew, Fitsum and Prandi, Davide and Susi, Angelo"",
  journal = ""arXiv Preprint "",
  volume = """",
  number = """",
  pages = """",
  year = ""2025"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2501.17461"",
  url = ""https://doi.org/10.48550/arXiv.2501.17461""
}";Research Contribution;Automated test generation is crucial for ensuring the reliability and robustness of software applications while at the same time reducing the effort needed. While significant progress has been made in test generation research, generating valid test oracles still remains an open problem. To address this challenge, we present AugmenTest, an approach leveraging Large Language Models (LLMs) to infer correct test oracles based on available documentation of the software under test. Unlike most existing methods that rely on code, AugmenTest utilizes the semantic capabilities of LLMs to infer the intended behavior of a method from documentation and developer comments, without looking at the code. AugmenTest includes four variants: Simple Prompt, Extended Prompt, RAG with a generic prompt (without the context of class or method under test), and RAG with Simple Prompt, each offering different levels of contextual information to the LLMs. To evaluate our work, we selected 142 Java classes and generated multiple mutants for each. We then generated tests from these mutants, focusing only on tests that passed on the mutant but failed on the original class, to ensure that the tests effectively captured bugs. This resulted in 203 unique tests with distinct bugs, which were then used to evaluate AugmenTest. Results show that in the most conservative scenario, AugmenTest’s Extended Prompt consistently outperformed the Simple Prompt, achieving a success rate of 30% for generating correct assertions. In comparison, the state-of-the-art TOGA approach achieved 8.2%. Contrary to our expectations, the RAG-based approaches did not lead to improvements, with performance of 18.2% success rate for the most conservative scenario. Our study demonstrates the potential of LLMs in improving the reliability of automated test generation tools, while also highlighting areas for future enhancement.;Oracle Generation;LLM-Pure-Prompting;;GPT-Family, Llama-Family;Other;AugmenTest 
P36;LLMs for Intelligent Software Testing: A Comparative Study;2024;Boukhlif2024;C: Other;Conference;"@Inproceedings{Boukhlif2024 ,
  author =       ""Boukhlif, Mohamed and Kharmoum, Nassim and Hanine, Mohamed"",
  title =        ""LLMs for Intelligent Software Testing: A Comparative Study"",
  booktitle =    ""Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security"",
  series =       ""NISS '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Meknes, AA, Morocco"",
  pages =        ""2464–-2465"",
  doi =          ""10.1145/3659677.3659749"",
  url =          ""https://doi.org/10.1145/3659677.3659749""
}";Survey;The need for effective and timely testing processes has become critical in the constantly changing field of software development. Large Language Models (LLMs) have demonstrated promise in automating test case creation, defect detection, and other software testing tasks through the use of the capabilities of machine/deep learning and natural language processing. This work explores the field of intelligent software testing, with a focus on the use of LLMs in this context. The purpose of this comparative study is to assess the corpus of research in the field in terms of used LLMs, how to interact with them, the use of fine-tuning, and prompt engineering, and explore the different technologies and testing types automated using LLMs. The findings of this study not only contribute to the growing body of knowledge on intelligent software testing but also guide fellow researchers and industry engineers in selecting the most suitable LLM for their specific testing needs.;Reflections;None;;;;
P37;Can LLM Generate Regression Tests for Software Commits_;2025;Liu2025;arXiv;arXiv;"@misc{Liu2025,
  title = ""Can LLM Generate Regression Tests for Software Commits?"",
  author = ""Liu, Jing and Lee, Seongmin and Losiouk, Eleonora and Böhme, Marcel"",
  archivePrefix = ""arXiv Preprint"",
  year = ""2025"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2501.11086"",
  url = ""https://doi.org/10.48550/arXiv.2501.11086""
}";Research Contribution;"Large Language Models (LLMs) have shown tremendous promise in automated software engineering. In this paper, we investigate the opportunities of LLMs for automatic regression test generation for programs that take highly structured, human-readable inputs, such as XML parsers or JavaScript interpreters. Concretely, we explore the following regression test generation scenarios for such programs that have so far been difficult to test automatically in the absence of corresponding input grammars:
\bullet Bug finding. Given a code change (e.g., a commit or pull request), our LLM-based approach generates a test case with the objective of revealing any bugs that might be introduced if that change is applied.
\bullet Patch testing. Given a patch, our LLM-based approach generates a test case that fails before but passes after the patch. This test can be added to the regression test suite to catch similar bugs in the future.
We implement Cleverest, a feedback-directed, zero-shot LLM-based regression test generation technique, and evaluate its effectiveness on 22 commits to three subject programs: Mujs, Libxml2, and Poppler. For programs using more human-readable file formats, like XML or JavaScript, we found Cleverest performed very well. It generated easy-to-understand bug-revealing or bug-reproduction test cases for the majority of commits in just under three minutes -- even when only the code diff or commit message (unless it was too vague) was given. For programs with more compact file formats, like PDF, as expected, it struggled to generate effective test cases. However, the LLM-supplied test cases are not very far from becoming effective (e.g., when used as a seed by a greybox fuzzer or as a starting point by the developer).";Unit Test Generation;LLM-Pure-Prompting;Custom;GPT-Family;Bugs Rate, Execution Time, Other;
P38;CoverUp- Coverage-Guided LLM-Based Test Generation;2025;Altmayer2025;J: Proc. ACM Softw. Eng;Journal;"@article{Altmayer2025,
  title = ""CoverUp: Effective High Coverage Test Generation for Python"",
  author = ""Altmayer Pizzorno, Juan and Berger, Emery D."",
  journal = ""Proc. ACM Softw. Eng."",
  month = ""jul"",
  year = ""2025"",
  volume = ""2"",
  number = ""FSE"",
  pages = ""23"",
  address = ""New York, NY, USA"",
  publisher = ""Association for Computing Machinery"",
  doi = ""10.1145/3729398"",
  url = ""https://doi.org/10.1145/3729398""
}";Research Contribution;Testing is an essential part of software development. Test generation tools attempt to automate the otherwise labor-intensive task of test creation, but generating high-coverage tests remains challenging. This paper proposes CoverUp, a novel approach to driving the generation of high-coverage Python regression tests. CoverUp combines coverage analysis, code context, and feedback in prompts that iteratively guide the LLM to generate tests that improve line and branch coverage. We evaluate our prototype CoverUp implementation across a benchmark of challenging code derived from open-source Python projects and show that CoverUp substantially improves on the state of the art. Compared to CodaMosa, a hybrid search/LLM-based test generator, CoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%). Compared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves an overall line+branch coverage of 90% (vs. 77%). We also demonstrate that CoverUp's performance stems not only from the LLM used but from the combined effectiveness of its components.;Test Augmentation or Improvement;Hybrid-Prompting;CM, PY, HumanEval;GPT-Family;Test Coverage;CoverUp
P39;Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing;2024;Dakhel2024;J: IST;Journal;"@article{Dakhel2024,
  title = ""Effective test generation using pre-trained large language models and mutation testing"",
  author = ""Dakhel, Arghavan Moradi and Nikanjam, Amin and Majdinasab, Vahid and Khomh, Foutse and Desmarais, Michel C"",
  journal = ""Information and Software Technology"",
  month = """",
  year = ""2025"",
  volume = ""171"",
  number = """",
  pages = ""107468"",
  address = """",
  publisher = ""Elsevier"",
  doi = ""10.1016/j.infsof.2024.107468"",
  url = ""https://doi.org/10.1016/j.infsof.2024.107468""
}
";Research Contribution;One of the critical phases in software development is software testing. Testing helps with identifying potential bugs and reducing maintenance costs. The goal of automated test generation tools is to ease the development of tests by suggesting efficient bug-revealing tests. Recently, researchers have leveraged Large Language Models (LLMs) of code to generate unit tests. While the code coverage of generated tests was usually assessed, the literature has acknowledged that the coverage is weakly correlated with the efficiency of tests in bug detection. To improve over this limitation, in this paper, we introduce MuTAP for improving the effectiveness of test cases generated by LLMs in terms of revealing bugs by leveraging mutation testing. Our goal is achieved by augmenting prompts with surviving mutants, as those mutants highlight the limitations of test cases in detecting bugs. MuTAP is capable of generating effective test cases in the absence of natural language descriptions of the Program Under Test (PUTs). We employ different LLMs within MuTAP and evaluate their performance on different benchmarks. Our results show that our proposed method is able to detect up to 28% more faulty human-written code snippets. Among these, 17% remained undetected by both the current state-of-the-art fully automated test generation tool (i.e., Pynguin) and zero-shot/few-shot learning approaches on LLMs. Furthermore, MuTAP achieves a Mutation Score (MS) of 93.57% on synthetic buggy code, outperforming all other approaches in our evaluation. Our findings suggest that although LLMs can serve as a useful tool to generate test cases, they require specific post-processing steps to enhance the effectiveness of the generated test cases which may suffer from syntactic or functional errors and may be ineffective in detecting certain types of bugs and testing corner cases PUTs.;Test Augmentation or Improvement;LLM-Pure-Prompting;HumanEval, Refactory;OpenAI Codex, Llama-Family;Mutation Rate;
P40;Enhancing LLM’s Ability to Generate More Repository-Aware Unit Tests Through Precise Contextual Information Injection;2025;Yin2025;arXiv;arXiv;"@misc{Yin2025,
  title = ""Enhancing LLM's Ability to Generate More Repository-Aware Unit Tests Through Precise Contextual Information Injection"",
  author = ""Yin, Xin and Ni, Chao and Li, Xinrui and Chen, Liushan and Ma, Guojun and Yang, Xiaohu"",
  archivePrefix = ""arXiv Preprint"",
  year = ""2025"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2501.07425"",
  url = ""https://doi.org/10.48550/arXiv.2501.07425""
}";Research Contribution;Though many learning-based approaches have been proposed for unit test generation and achieved remarkable performance, they still have limitations in relying on task-specific datasets. Recently, Large Language Models (LLMs) guided by prompt engineering have gained attention for their ability to handle a broad range of tasks, including unit test generation. Despite their success, LLMs may exhibit hallucinations when generating unit tests for focal methods or functions due to their lack of awareness regarding the project's global context. These hallucinations may manifest as calls to non-existent methods, as well as incorrect parameters or return values, such as mismatched parameter types or numbers. While many studies have explored the role of context, they often extract fixed patterns of context for different models and focal methods, which may not be suitable for all generation processes (e.g., excessive irrelevant context could lead to redundancy, preventing the model from focusing on essential information). To overcome this limitation, we propose RATester, which enhances the LLM's ability to generate more repository-aware unit tests through global contextual information injection. To equip LLMs with global knowledge similar to that of human testers, we integrate the language server gopls, which provides essential features (e.g., definition lookup) to assist the LLM. When RATester encounters an unfamiliar identifier (e.g., an unfamiliar struct name), it first leverages gopls to fetch relevant definitions and documentation comments, and then uses this global knowledge to guide the LLM. By utilizing gopls, RATester enriches the LLM's knowledge of the project's global context, thereby reducing hallucinations during unit test generation.;Unit Test Generation;LLM-Pure-Prompting;Custom;DeepSeekCoder, CodeLlama, Magicoder;% Build, Test Coverage;
P41;Evaluating large language models for software testing;2025;Li2025;J: Comp. Std. and Int.;Journal;"@article{Li2025,
  title = ""Evaluating large language models for software testing"",
  author = ""Yihao Li and Pan Liu and Haiyang Wang and Jie Chu and W. Eric Wong"",
  journal = ""Computer Standards & Interfaces"",
  month = """",
  year = ""2025"",
  volume = ""93"",
  number = """",
  pages = ""103942"",
  address = """",
  publisher = ""Elsevier"",
  doi = ""10.1016/j.csi.2024.103942"",
  url = ""https://doi.org/10.1016/j.csi.2024.103942""
}";Research Contribution;Large language models (LLMs) have demonstrated significant prowess in code analysis and natural language processing, making them highly valuable for software testing. This paper conducts a comprehensive evaluation of LLMs applied to software testing, with a particular emphasis on test case generation, error tracing, and bug localization across twelve open-source projects. The advantages and limitations, as well as recommendations associated with utilizing LLMs for these tasks, are delineated. Furthermore, we delve into the phenomenon of hallucination in LLMs, examining its impact on software testing processes and presenting solutions to mitigate its effects. The findings of this work contribute to a deeper understanding of integrating LLMs into software testing, providing insights that pave the way for enhanced effectiveness in the field.;Reflections;LLM-Pure-Prompting;Custom;GPT-Family, Other;% Build, Bugs Rate, Other;
P42;A Large-scale Empirical Study on Fine-tuning Large Language Models for Unit Testing;2024;Shang2024;C: ISSTA;Conference;"@article{Shang2024,
  title = ""A Large-scale Empirical Study on Fine-tuning Large Language Models for Unit Testing"",
  author = ""Shang, Ye and Zhang, Quanjun and Fang, Chunrong and Gu, Siqi and Zhou, Jianyi and Chen, Zhenyu"",
  journal = ""arXiv Preprint (Submitted to ISSTA25) "",
  volume = """",
  number = """",
  pages = """",
  year = ""2024"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2412.16620"",
  url = ""https://doi.org/10.48550/arXiv.2412.16620""
}
";Research Contribution;"Unit testing plays a pivotal role in software development, improving software quality and reliability. However, generating effective test cases manually is time-consuming, prompting interest in unit testing research. Recently, Large Language Models (LLMs) have shown potential in various unit testing tasks, including test generation, assertion generation, and test evolution, but existing studies are limited in scope and lack a systematic evaluation of the effectiveness of LLMs.
To bridge this gap, we present a large-scale empirical study on fine-tuning LLMs for unit testing. Our study involves three unit testing tasks, five benchmarks, eight evaluation metrics, and 37 popular LLMs across various architectures and sizes, consuming over 3,000 NVIDIA A100 GPU hours. We focus on three key research questions: (1) the performance of LLMs compared to state-of-the-art methods, (2) the impact of different factors on LLM performance, and (3) the effectiveness of fine-tuning versus prompt engineering. Our findings reveal that LLMs outperform existing state-of-the-art approaches on all three unit testing tasks across nearly all metrics, highlighting the potential of fine-tuning LLMs in unit testing tasks. Furthermore, large-scale, decoder-only models achieve the best results across tasks, while encoder-decoder models perform better under the same parameter scale. Additionally, the comparison of the performance between fine-tuning and prompt engineering approaches reveals the considerable potential capability of the prompt engineering approach in unit testing tasks. We then discuss the concerned issues on the test generation task, including data leakage issues, bug detection capabilities, and metrics comparisons. Finally, we further pinpoint carious practical guidelines for LLM-based approaches to unit testing tasks in the near future.";Reflections;LLM-Pure-Prompting;Methods2Test, Other, Defects4J;ATLAS, CodeBert, GraphCodeBert, UniXcoder, CodeT5, Other, CodeGPT, Starcoder-2, CodeLlama, DeepSeekCoder;% Test Correct, % Pass, % Test Fail, % Build, % Test Syntax Error, BLEU, CodeBLEU, Bugs Rate, ML-metrics, Other, EM (Exact Match);
P43;A System for Automated Unit Test Generation Using Large Language Models and Assessment of Generated Test Suites;2024;Lops2024;C: ICST;Conference;"@Inproceedings{Lops2024 ,
  author =       ""Lops, Andrea and Narducci, Fedelucio and Ragone, Azzurra and Trizio, Michelantonio and Bartolini, Claudio"",
  title =        ""A System for Automated Unit Test Generation Using Large Language Models and Assessment of Generated Test Suites"",
  booktitle =    ""2025 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)"",
  series =       ""ICST '25"",
  editor =       """",
  volume =       """",
  year =         ""2025"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Naples, Italy"",
  pages =        ""29--36"",
  doi =          ""10.1109/ICSTW64639.2025.10962454"",
  url =          ""https://doi.org/10.1109/ICSTW64639.2025.10962454""
}";Research Contribution;Unit tests represent the most basic level of testing within the software testing lifecycle and are crucial to ensuring software correctness. Designing and creating unit tests is a costly and labor-intensive process that is ripe for automation. Recently, Large Language Models (LLMs) have been applied to various aspects of software development, including unit test generation. Although several empirical studies evaluating LLMs' capabilities in test code generation exist, they primarily focus on simple scenarios, such as the straightforward generation of unit tests for individual methods. These evaluations often involve independent and small-scale test units, providing a limited view of LLMs' performance in real-world software development scenarios. Moreover, previous studies do not approach the problem at a suitable scale for real-life applications. Generated unit tests are often evaluated via manual integration into the original projects, a process that limits the number of tests executed and reduces overall efficiency. To address these gaps, we have developed an approach for generating and evaluating more real-life complexity test suites. Our approach focuses on class-level test code generation and automates the entire process from test generation to test assessment. In this work, we present AgoneTest: an automated system for generating test suites for Java projects and a comprehensive and principled methodology for evaluating the generated test suites. Starting from a state-of-the-art dataset (i.e., Methods2Test), we built a new dataset for comparing human-written tests with those generated by LLMs. Our key contributions include a scalable automated software system, a new dataset, and a detailed methodology for evaluating test quality.;Unit Test Generation;LLM-Pure-Prompting;Custom;GPT-Family;% Test Smells, Test Coverage;AgoneTEST
P44;Test Wars: A Comparative Study of SBST, Symbolic Execution, and LLM-Based Approaches to Unit Test Generation;2025;Abdullin2025;C: ICST;Conference;"@Inproceedings{Abdullin2025 ,
  author =       ""Azat Abdullin and Pouria Derakhshanfar and Annibale Panichella"",
  title =        ""Test Wars: A Comparative Study of SBST, Symbolic Execution, and LLM-Based Approaches to Unit Test Generation"",
  booktitle =    ""Proceedings of the 18th IEEE International Conference on Software Testing, Verification and Validation (ICST) 2025"",
  series =       ""ICST '25"",
  editor =       """",
  volume =       """",
  year =         ""2025"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Naples, Italy"",
  pages =        ""--"",
  doi = ""10.48550/arXiv.2501.10200"",
  url = ""https://doi.org/10.48550/arXiv.2501.10200""
}";Survey;Unit tests represent the most basic level of testing within the software testing lifecycle and are crucial to ensuring software correctness. Designing and creating unit tests is a costly and labor-intensive process that is ripe for automation. Recently, Large Language Models (LLMs) have been applied to various aspects of software development, including unit test generation. Although several empirical studies evaluating LLMs' capabilities in test code generation exist, they primarily focus on simple scenarios, such as the straightforward generation of unit tests for individual methods. These evaluations often involve independent and small-scale test units, providing a limited view of LLMs' performance in real-world software development scenarios. Moreover, previous studies do not approach the problem at a suitable scale for real-life applications. Generated unit tests are often evaluated via manual integration into the original projects, a process that limits the number of tests executed and reduces overall efficiency. To address these gaps, we have developed an approach for generating and evaluating more real-life complexity test suites. Our approach focuses on class-level test code generation and automates the entire process from test generation to test assessment. In this work, we present AgoneTest: an automated system for generating test suites for Java projects and a comprehensive and principled methodology for evaluating the generated test suites. Starting from a state-of-the-art dataset (i.e., Methods2Test), we built a new dataset for comparing human-written tests with those generated by LLMs. Our key contributions include a scalable automated software system, a new dataset, and a detailed methodology for evaluating test quality.;Reflections;Hybrid-Prompting;GitBug;CodeLlama, GPT-Family, Llama-Family;Test Coverage, Static Analysis Metrics, Other, Mutation Rate;-
P45;Using large language models to generate junit tests: An empirical study;2024;Siddiq2024;C:EASE;Conference;"@Inproceedings{Siddiq2024 ,
  author =       ""Siddiq, Mohammed Latif and Da Silva Santos, Joanna Cecilia and Tanvir, Ridwanul Hasan and Ulfat, Noshin and Al Rifat, Fahmid and Carvalho Lopes, Vinícius"",
  title =        ""Using large language models to generate junit tests: An empirical study"",
  booktitle =    ""Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering"",
  series =       ""EASE '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Salerno, Italy"",
  pages =        ""313--322"",
  doi =          ""10.1145/3661167.3661216"",
  url =          ""https://doi.org/10.1145/3661167.3661216""
}";Survey;Acode generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although codegenerationmodels(e.g., GitHubCopilot)areincreasinglybeing adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning for a strongly typed language like Java. To fill this gap, we investigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can generate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the effect of context generation on the unit test generation process. We evaluated the models based on compilation rates, test correctness, test coverage, and test smells. We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark.Thegeneratedtestsalsosufferedfromtestsmells, such as Duplicated Asserts and Empty Tests.;Reflections;LLM-Pure-Prompting;Other;Other, GPT-Family;% Test Smells, Test Coverage, % Build;-
P46;Generating Software Tests for Mobile Applications Using Fine-Tuned Large Language Models;2024;Hoffman2024;C: AST;Conference;"@Inproceedings{Hoffman2024 ,
  author =       ""Hoffmann, Jacob and Frister, Demian"",
  title =        ""Generating Software Tests for Mobile Applications Using Fine-Tuned Large Language Models"",
  booktitle =    ""Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)"",
  series =       ""AST '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Lisbon, Portugal"",
  pages =        ""76--77"",
  doi =          ""10.1145/3644032.3644454"",
  url =          ""https://doi.org/10.1145/3644032.3644454""
}";Research Contribution;Check, too much long to be included here;Unit Test Generation;LLM-Pure-FineTune;MBPP, HumanEval;Mistral, CodeLlama, Llama-Family;% Pass;-
P47;FlakyFix: Using Large Language Models for Predicting Flaky Test Fix Categories and Test Code Repair;2024;Fatima2024;J: TSE;Journal;"@article{Fatima2024,
  title = ""FlakyFix: Using Large Language Models for Predicting Flaky Test Fix Categories and Test Code Repair"",
  author = ""Fatima, Sakina and Hemmati, Hadi and C. Briand, Lionel"",
  journal = ""IEEE Transactions on Software Engineering"",
  month = """",
  year = ""2025"",
  volume = ""50"",
  number = ""12"",
  pages = ""3146-3171"",
  address = ""Los Alamitos, CA, USA"",
  publisher = ""IEEE Computer Society"",
  doi = ""10.1109/TSE.2024.3472476"",
  url = ""https://doi.org/10.1109/TSE.2024.3472476""
}";Research Contribution;Flaky tests are problematic because they nondeterministically pass or fail for the same software version under test, causing confusion and wasting development effort. While machine learning models have been used to predict flakiness and its root causes, there is much less work on providing support to f ix the problem. To address this gap, in this paper, we focus on predicting the type of fix that is required to remove flakiness and then repair the test code on that basis. We do this for a subset of flaky tests where the root cause of flakiness is in the test itself and not in the production code. One key idea is to guide the repair process with additional knowledge about the test’s flakiness in the form of its predicted fix category. Thus, we first propose a framework that automatically generates labeled datasets for 13 fix categories and trains models to predict the fix category of a flaky test by analyzing the test code only. Our experimental results using code models and few-shot learning show that we can correctly predict most of the fix categories. To show the usefulness of such fix category labels for automatically repairing flakiness, we augment the prompts of GPT 3.5 Turbo, a Large Language Model (LLM), with such extra knowledge to request repair suggestions. The results show that our suggested fix category labels, complemented with incontext learning, significantly enhance the capability of GPT 3.5 Turbo in generating fixes for flaky tests. Based on the execution and analysis of a sample of GPT-repaired flaky tests, we estimate that a large percentage of such repairs, (roughly between 51% and 83%) can be expected to pass. For the failing repaired tests, on average, 16% of the test code needs to be further changed for them to pass.;Test Augmentation or Improvement;LLM-Pure-Prompting;Other;GPT-Family;% Pass, BLEU, CodeBLEU;FlakyFix
P48;First Experiments on Automated Execution of Gherkin Test Specifications with Collaborating LLM Agents;;Bergsmann2024;C: A-TEST;Conference;"@Inproceedings{Bergsmann2024 ,
  author =       ""Severin Bergsmann and Alexander Schmidt and Stefan Fischer and Rudolf Ramler"",
  title =        ""First Experiments on Automated Execution of Gherkin Test Specifications with Collaborating LLM Agents"",
  booktitle =    ""Proceedings of the 15th ACM International Workshop on Automating Test Case Design, Selection and Evaluation, Co-located with: SSTA 2024"",
  series =       """",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery, Inc"",
  address =      ""Vienna, Austria"",
  pages =        ""12-15"",
  doi =          ""10.1145/3678719.3685692"",
  url =          ""https://dl.acm.org/doi/10.1145/3678719.3685692"",
  number =       """",
  month =        ""9"",
  organization = """",
  note =         """",
}";Research Contribution;"Gherkin is a domain-specific language for describing test scenar-
ios in natural language, which are the basis for automated accep-
tance testing. The emergence ofLarge Language Models (LLMs) has
opened up new possibilities for processing such test specifications
and for generating executable test code. This paper investigates the
feasibility ofemploying LLMs to execute Gherkin test specifications
utilizing the AutoGen multi-agent framework. Our findings show
that our LLM agent system is able to automatically run the given
test scenarios by autonomously exploring the system under test,
generating executable test code on the fly, and evaluating execution
results. We observed high success rates for executing simple as well
as more complex test scenarios, but we also identified difficulties
regarding failure scenarios and fault detection.";High-Level Test Gen;LLM-Pure-Prompting;No Bmk-Ds;GPT-Family;% Pass, Execution Time, Other;-
P49;Generating Accurate Assert Statements for Unit Test Cases using Pretrained Transformers;2022;Tufano2022;C: AST;Conference;"@Inproceedings{Tufano2022 ,
  author =       ""Tufano, Michele and Drain, Dawn and Svyatkovskiy, Alexey and Sundaresan, Neel"",
  title =        ""Generating accurate assert statements for unit test cases using pretrained transformers"",
  booktitle =    ""Proceedings of the 3rd ACM/IEEE International Conference on Automation of Software Test"",
  series =       ""AST '22"",
  editor =       """",
  volume =       """",
  year =         ""2022"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Pittsburgh, Pennsylvania"",
  pages =        ""54--64"",
  doi =          ""10.1145/3524481.3527220"",
  url =          ""https://doi.org/10.1145/3524481.3527220""
}";Research Contribution;Unit testing represents the foundational basis of the software testing pyramid, beneath integration and end-to-end testing. Automated software testing researchers have proposed a variety of techniques to assist developers in this time-consuming task. In this paper we present an approach to support developers in writing unit test cases by generating accurate and useful assert statements. Our approach is based on a state-of-the-art transformer model initially pretrained on an English textual corpus. This semantically rich model is then trained in a semisupervised fashion on a large corpus of source code. Finally, we f inetune this model on the task of generating assert statements for unit tests. The resulting model is able to generate accurate assert statements for a given method under test. In our empirical evaluation, the model was able to predict the exact assert statements written by developers in 62% of the cases in the first attempt. The results show 80% relative improvement for top-1 accuracy over the previous RNN-based approach in the literature. We also show the substantial impact of the pretraining process on the performances of our model, as well as comparing it with assert auto-completion task. Finally, we demonstrate how our approach can be used to augment EvoSuite test cases, with additional asserts leading to improved test coverage.;Oracle Generation;LLM-Pure-FineTune;Defects4J, CodeSearchNet;BART;Test Coverage, Other;-
P50;On Learning Meaningful Assert Statements for Unit Test Cases;2020;Watson2020;C: ICSE;Conference;"@Inproceedings{Watson2020 ,
  author =       ""Cody Watson and Michele Tufano and Kevin Moran and Gabriele Bavota and Denys Poshyvanyk"",
  title =        ""On learning meaningful assert statements for unit test cases"",
  booktitle =    ""Proceedings - International Conference on Software Engineering"",
  series =       ""ICSE '20"",
  editor =       """",
  volume =       """",
  year =         ""2020"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Seoul, South Korea"",
  pages =        ""1398--1409"",
  doi =          ""10.1145/3377811.3380429"",
  url =          ""https://doi.org/10.1145/3377811.3380429""
}";Research Contribution;"Software testing is an essential part of the software lifecycle and
requires a substantial amount of time and effort. It has been estimated
that software developers spend close to 50% of their time on
testing the code they write. For these reasons, a long standing goal
within the research community is to (partially) automate software
testing. While several techniques and tools have been proposed
to automatically generate test methods, recent work has criticized
the quality and usefulness of the assert statements they generate.
Therefore, we employ a Neural Machine Translation (NMT) based
approach called Atlas (AuTomatic Learning of Assert Statements)
to automatically generate meaningful assert statements for test
methods. Given a test method and a focal method (i.e., the main
method under test), Atlas can predict a meaningful assert statement
to assess the correctness of the focal method. We applied
Atlas to thousands of test methods from GitHub projects and it
was able to predict the exact assert statement manually written
by developers in 31% of the cases when only considering the top-
1 predicted assert. When considering the top-5 predicted assert
statements, Atlas is able to predict exact matches in 50% of the
cases. These promising results hint to the potential usefulness of
our approach as (i) a complement to automatic test case generation
techniques, and (ii) a code completion support for developers, who
can benefit from the recommended assert statements while writing
test code.";Oracle Generation;None;Custom;ATLAS;BLEU;ATLAS
P51;AsserT5: Test Assertion Generation Using a Fine-Tuned Code Language Model;2025;Primbs2025;C: AST;Conference;"@Inproceedings{Primbs2025 ,
  author =       ""Severin Primbs and Benedikt Fein and Gordon Fraser"",
  title =        ""AsserT5: Test Assertion Generation Using a Fine-Tuned Code Language Model"",
  booktitle =    ""Proceedings 6th ACM/IEEE International Conference on Automation of Software Test (AST 2025)"",
  series =       ""AST '25"",
  editor =       """",
  volume =       """",
  year =         ""2025"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Ontario, Canada"",
  pages =        """",
  doi =          ""10.48550/arXiv.2502.02708"",
  url =          ""https://doi.org/10.48550/arXiv.2502.02708""
}";Research Contribution;Writing good software tests can be challenging, therefore approaches that support developers are desirable. While generating complete tests automatically is such an approach commonly proposed in research, developers may already have specific test scenarios in mind and thus just require help in selecting the most suitable test assertions for these scenarios. This can be done using deep learning models to predict assertions for given test code. Prior research on assertion generation trained these models specifically for the task, raising the question how much the use of larger models pre-trained on code that have emerged since then can improve their performance. In particular, while abstracting identifiers has been shown to improve specifically trained models, it remains unclear whether this also generalises to models pre-trained on non-abstracted code. Finally, even though prior work demonstrated high accuracy it remains unclear how this translates into the effectiveness of the assertions at their intended application -- finding faults. To shed light on these open questions, in this paper we propose AsserT5, a new model based on the pre-trained CodeT5 model, and use this to empirically study assertion generation. We find that the abstraction and the inclusion of the focal method are useful also for a fine-tuned pre-trained model, resulting in test assertions that match the ground truth assertions precisely in up to 59.5\% of cases, more than twice as precise as prior models. However, evaluation on real bugs from the Defects4J dataset shows that out of 138 bugs detectable with assertions in real-world projects, AsserT5 was only able to suggest fault-finding assertions for 33, indicating the need for further improvements.;Oracle Generation;LLM-Pure-Prompting;Defects4J, Methods2Test;ATLAS, TOGA, BART, AssertT5, CodeT5, GPT-Family;% Build,ML-metrics, BLEU, Bugs Rate;
P54;Simulink Mutation Testing using CodeBERT;2025;ZhangAST2025;C: AST;Conference;"@Inproceedings{ZhangAST2025 ,
  author =       ""Zhang, Jingfan and Ghobari, Delaram and Sabetzadeh, Mehrdad and Nejati, Shiva"",
  title =        ""Simulink Mutation Testing using CodeBERT"",
  booktitle =    ""Proceedings 6th ACM/IEEE International Conference on Automation of Software Test (AST 2025)"",
  series =       ""AST '25"",
  editor =       """",
  volume =       """",
  year =         ""2025"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Ontario, Canada"",
  pages =        """",
  doi =          ""-"",
  url =          ""-""
}";Research Contribution;We present BERTiMuS, an approach that uses CodeBERTto generate mutants for Simulink models. BERTiMuS converts Simulink models into textual representations, masks tokens from the derived text, and uses CodeBERT to predict the masked tokens. Simulink mutants are obtained by replacing the masked tokens with predictions from CodeBERT. We evaluate BERTiMuS using Simulink models from an industrial benchmark, and compare it with FIM– a state-of-the-art mutation tool for Simulink. We show that, relying exclusively on CodeBERT, BERTiMuS can generate the block-based Simulink mutation patterns documented in the literature. Further, our results indicate that: (a) BERTiMuS is complementary to FIM, and (b) when one considers a requirements-aware notion of mutation testing, BERTiMuS outperforms FIM.;Test Augmentation or Improvement;Hybrid-FineTune;Custom;CodeBert;Mutation Rate;-
P55;VALTEST: Automated Validation of Language Model Generated Test Cases;2024;Taherkhani2024;arXiv;arXiv;"@misc{Taherkhani2024,
  title = ""VALTEST: Automated Validation of Language Model Generated Test Cases"",
  author = ""Hamed Taherkhani and Hadi Hemmati"",
  archivePrefix = ""arXiv Preprint"",
  year = ""2024"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2411.08254"",
  url = ""https://doi.org/10.48550/arXiv.2411.08254""
}";Research Contribution;Large Language Models (LLMs) have demonstrated significant potential in automating software testing, specifically in generating unit test cases. However, the validation of LLM-generated test cases remains a challenge, particularly when thegroundtruthisunavailable.ThispaperintroducesVALTEST,anovelframework designed to automatically validate test cases generated by LLMs by leveraging token probabilities. We evaluate VALTEST using nine test suites generated from three datasets—HumanEval, MBPP, and LeetCode—across three LLMs—GPT-4o, GPT-3.5-turbo, and LLama3.1 8b. By extracting statistical features from token probabilities, we train a machine learning model to predict test case validity. VALTEST increases the validity rate of test cases by 6.2% to 24%, depending on the dataset and LLM. Our results suggest that token probabilities are reliable indicators for distinguishing between valid and invalid test cases, which provides a robust solution for improving the correctness of LLM-generated test cases in software testing. In addition, we found that replacing the identified invalid test cases by VALTEST, using a Chain-of-Thought prompting results in a more effective test suite while keeping the high validity rates;Unit Test Generation;Hybrid-Prompting;HumanEval, MBPP, LeetCode;Llama-Family, GPT-Family;Test Coverage, Mutation Rate;-
P56;Codet: Code Generation with Generated Tests;2022;Chen2023;arXiv;arXiv;"@misc{Chen2023,
  author = ""Bei Chen and   Fengji Zhang and Anh Nguyen and Daoguang Zan and  Zeqi Lin and Jian{-}Guang Lou and Weizhu Chen"",
  title = ""CodeT: Code Generation with Generated Tests"",
  archivePrefix = ""arXiv Preprint"",
  year = ""2023"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2207.10397"",
  url = ""https://doi.org/10.48550/arXiv.2207.10397""
}";Research Contribution;The task of generating code solutions for a given programming problem can benef it from the use of pre-trained language models such as Codex, which can produce multiple diverse samples. However, a major challenge for this task is to select the most appropriate solution from the multiple samples generated by the pretrained language models. A natural way to evaluate the quality and correctness of a code solution is to run it against a set of test cases, but the manual creation of such test cases is often costly and time-consuming. In this paper, we propose a novel method, CODET, that leverages the same pre-trained language models to automatically generate test cases for the code samples, thus reducing the human effort and increasing the coverage of the test scenarios. CODET then executes the code samples using the generated test cases and performs a dual execution agreement, which considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples. We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS, and CodeContests, using five different pre-trained language models with varying sizes and capabilities. Our results show that CODET can significantly improve the performance of code solution selection over previous methods, achieving remarkable and consistent gains across different models and benchmarks. For instance, CODET improves the pass@1 metric on HumanEval to 658%, which represents an absolute improvement of 188% over the code-davinci-002 model, and an absolute improvement of more than 20% over the previous state-of-the-art results;Unit Test Generation;LLM-Pure-Prompting;HumanEval, MBPP, APPS;OpenAI Codex, Codegen, Other;Test Coverage;CodeT
P57;The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for Test Case Generation;2025;Gao2025;arXiv;arXiv;"@misc{Gao2025,
  author = ""Shuzheng Gao and Chaozheng Wang and Cuiyun Gao and Xiaoqian Jiao and Chun Yong Chong and Shan Gao and Michael R Lyu"",
  title = ""The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for Test Case Generation"",
  archivePrefix = ""arXiv Preprint"",
  year = ""2025"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2501.01329"",
  url = ""https://doi.org/10.48550/arXiv.2501.01329""
}";Research Contribution;Test cases are essential for validating the reliability and quality of software applications. Recent studies have demonstrated the capability of Large Language Models (LLMs) to generate useful test cases for given source code. However, the existing work primarily relies on human-written plain prompts, which often leads to suboptimal results since the performance of LLMs can be highly influenced by the prompts. Moreover, these approaches use the same prompt for all LLMs, overlooking the fact that different LLMs might be best suited to different prompts. Given the wide variety of possible prompt formulations, automatically discovering the optimal prompt for each LLM presents a significant challenge. Although there are methods on automated prompt optimization in the natural language processing field, they are hard to produce effective prompts for the test case generation task. First, the methods iteratively optimize prompts by simply combining and mutating existing ones without proper guidance, resulting in prompts that lack diversity and tend to repeat the same errors in the generated test cases. Second, the prompts are generally lack of domain contextual knowledge, limiting LLMs' performance in the task.;Unit Test Generation;LLM-Pure-Prompting;Defects4J;Qwen 2, Llama-Family, GPT-Family;Test Coverage;-
P58;LLM for Test Script Generation and Migration: Challenges, Capabilities, and Opportunities;2023;Yu2023;C: Other;Conference;"@Inproceedings{Yu2023 ,
  author =       ""Shengcheng Yu and Chunrong Fang and Yuchen Ling and Chentian Wu and Zhenyu Chen"",
  title =        ""LLM for Test Script Generation and Migration: Challenges, Capabilities, and Opportunities"",
  booktitle =    ""Proceedings of  IEEE 23rd International Conference on Software Quality, Reliability and Security, QRS"",
  series =       """",
  editor =       """",
  volume =       """",
  year =         ""2023"",
  publisher =    ""Institute of Electrical and Electronics Engineers Inc."",
  address =      ""Chiang Mai, Thailand"",
  pages =        ""206-217"",
  doi =          ""10.1109/QRS60937.2023.00029"",
  url =          ""https://doi.org/10.1109/QRS60937.2023.00029"",
  number =       """",
  month =        """",
  organization = """",
  note =         """",
}";Research Contribution;"This paper investigates the application of large language
models (LLM) in the domain of mobile application test
script generation. Test script generation is a vital component
of software testing, enabling efficient and reliable automation
of repetitive test tasks. However, existing generation
approaches often encounter limitations, such as difficulties in
accurately capturing and reproducing test scripts across diverse
devices, platforms, and applications. These challenges arise
due to differences in screen sizes, input modalities, platform
behaviors, API inconsistencies, and application architectures.
Overcoming these limitations is crucial for achieving robust
and comprehensive test automation.
By leveraging the capabilities of LLMs, we aim to address
these challenges and explore its potential as a versatile tool
for test automation. We investigate how well LLMs can adapt
to diverse devices and systems while accurately capturing and
generating test scripts. Additionally, we evaluate its crossplatform
generation capabilities by assessing its ability to
handle operating system variations and platform-specific behaviors.
Furthermore, we explore the application of LLMs
in cross-app migration, where it generates test scripts across
different applications and software environments based on
existing scripts.
Throughout the investigation, we analyze its adaptability to
various user interfaces, app architectures, and interaction patterns,
ensuring accurate script generation and compatibility.
The findings of this research contribute to the understanding of
LLMs’ capabilities in test automation. Ultimately, this research
aims to enhance software testing practices, empowering app
developers to achieve higher levels of software quality and
development efficiency.";High-Level Test Gen;LLM-Pure-Prompting;Custom;GPT-Family;;
P59;A Retrospective on Whole Test Suite Generation: On the Role of SBST in the Age of LLMs;2025;Fraser2025;J: TSE;Journal;"@article{Fraser2025,
  title = ""A Retrospective on Whole Test Suite Generation: On the Role of SBST in the Age of LLMs"",
  author = ""Fraser, Gordon and Arcuri, Andrea"",
  journal = ""IEEE Transactions on Software Engineering"",
  month = """",
  year = ""2025"",
  volume = """",
  number = """",
  pages = ""1-5"",
  address = """",
  publisher = ""Institute of Electrical and Electronics Engineers Inc."",
  doi = ""10.1109/TSE.2025.3539458"",
  url = ""https://doi.org/10.1109/TSE.2025.3539458""
}";Survey;"This paper presents a retrospective of the article
“Whole Test Suite Generation”, published in the IEEE Transactions on Software Engineering, in 2012. We summarize its
main contributions, and discuss how this work impacted the
research field of Search-Based Software Testing (SBST) in the
last 12 years. The novel techniques presented in the paper were
implemented in the tool EvoSuite, which has been so far the
state-of-the-art in unit test generation for Java programs using
SBST. SBST has shown practical and impactful applications,
creating the foundations to open the doors to tackle several other
software testing problems besides unit testing, like for example
system testing of Web APIs with EvoMaster. We conclude our
retrospective with our reflections on what lies ahead, especially
considering the important role that SBST still plays even in the
age of Large Language Models (LLMs).";Reflections;None;;;;
P60;Enhancing LLM-based Test Generation for Hard-to-Cover Branches via Program Analysis;2025;ChenarXiv2024;J: TOSEM;Journal;"@article{ChenArxiv2024,
  title = ""Advancing Code Coverage: Incorporating Program Analysis with Large Language Models"",
  author = ""Yang, Chen and Chen, Junjie and Lin, Bin and Wang, Ziqi and Zhou, Jianyi"",
  journal = ""ACM Trans. Softw. Eng. Methodol."",
  volume = """",
  number = """",
  pages = """",
  year = ""2025"",
  publisher = ""Association for Computing Machinery"",
  doi = ""10.1145/3748505"",
  url = ""https://doi.org/10.1145/3748505"",
  note = ""Just Accepted""
}";Research Contribution;Automatic test generation plays a critical role in software quality assurance. While the recent advances in SearchBased Software Testing (SBST) and Large Language Models (LLMs) have shown promise in generating useful tests, these techniques still struggle to cover certain branches. Reaching these hard-to-cover branches usually requires constructing complex objects and resolving intricate inter-procedural dependencies in branch conditions, which poses significant challenges for existing techniques. In this work, we propose TELPA, a novel technique aimed at addressing these challenges. Its key insight lies in extracting real usage scenarios of the target method under test to learn how to construct complex objects and extracting methods entailing inter-procedural dependencies with hard-tocover branches to learn the semantics of branch constraints. To enhance efficiency and effectiveness, TELPA identifies a set of ineffective tests as counter-examples for LLMs and employs a feedback-based process to iteratively refine these counterexamples. Then, TELPA integrates program analysis results and counter-examples into the prompt, guiding LLMs to gain deeper understandings of the semantics of the target method and generate diverse tests that can reach the hard-to-cover branches. Our experimental results on 27 open-source Python projects demonstrate that TELPA significantly outperforms the state-ofthe-art SBST and LLM-based techniques, achieving an average improvement of 31.39% and 22.22% in terms of branch coverage;Unit Test Generation;Hybrid-Prompting;Pynguin, BugsInPy;CodeLlama;Test Coverage;
P61;Large Language Model assisted Hybrid Fuzzing;2024;Meng2024;arXiv;arXiv;"@misc{Meng2024,
  title = ""Large Language Model assisted Hybrid Fuzzing"",
  author = ""Ruijie Meng and Gregory J. Duck and Abhik Roychoudhury"",
  archivePrefix = ""arXiv Preprint"",
  year = ""2024"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2412.15931"",
  url = ""https://doi.org/10.48550/arXiv.2412.15931""
}";Research Contribution;"Greybox fuzzing is one of the most popular methods for detecting software vulnerabilities, which conducts a biased random search within the program input space. To enhance its effectiveness in achieving deep coverage of program behaviors, greybox fuzzing is often combined with concolic execution, which performs a path-sensitive search over the domain of program inputs. In hybrid fuzzing, conventional greybox fuzzing is followed by concolic execution in an iterative loop, where reachability roadblocks encountered by greybox fuzzing are tackled by concolic execution. However, such hybrid fuzzing still suffers from difficulties conventionally faced by symbolic execution, such as the need for environment modeling and system call support. In this work, we show how to achieve the effect of concolic execution without having to compute and solve symbolic path constraints. When coverage-based greybox fuzzing reaches a roadblock in terms of reaching certain branches, we conduct a slicing on the execution trace and suggest modifications of the input to reach the relevant branches. A Large Language Model (LLM) is used as a solver to generate the modified input for reaching the desired branches. Compared with both the vanilla greybox fuzzer AFL and hybrid fuzzers Intriguer and Qsym, our LLM-based hybrid fuzzer HyLLfuzz (pronounced ""hill fuzz"") demonstrates superior coverage. Furthermore, the LLM-based concolic execution in HyLLfuzz takes a time that is 4-19 times faster than the concolic execution running in existing hybrid fuzzing tools. This experience shows that LLMs can be effectively inserted into the iterative loop of hybrid fuzzers, to efficiently expose more program behaviors.";Test Augmentation or Improvement;Hybrid-Prompting;Other;GPT-Family;Test Coverage, Execution Time;HyLLfuzz
P62;Do LLMs generate test oracles that capture the actual or the expected program behaviour?;2024;Konstantinou2024;arXiv;arXiv;"@misc{Konstantinou2024,
  title = ""Do LLMs generate test oracles that capture the actual or the expected program behaviour?"",
  author = ""Michael Konstantinou and Renzo Degiovanni and Mike Papadakis"",
  archivePrefix = ""arXiv Preprint"",
  year = ""2024"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2410.21136"",
  url = ""https://doi.org/10.48550/arXiv.2410.21136""
}";Survey;—Software testing is an essential part of the software development cycle to improve the code quality. Typically, a unit test consists of a test prefix and a test oracle which captures the developer’s intended behaviour. A known limitation of traditional test generation techniques (e.g. Randoop and Evosuite) is that they produce test oracles that capture the actual program behaviour rather than the expected one. Recent approaches leverage Large Language Models (LLMs), trained on an enormous amount of data, to generate developer-like code and test cases. We investigate whether the LLM-generated test oracles capture the actual or expected software behaviour. We thus, conduct a controlled experiment to answer this question, by studying LLMs performance on two tasks, namely, test oracle classification and generation. The study includes developerwritten and automatically generated test cases and oracles for 24 open-source Java repositories, and different well tested prompts. Our findings show that LLM-based test generation approaches are also prone on generating oracles that capture the actual program behaviour rather than the expected one. Moreover, LLMs are better at generating test oracles rather than classifying the correct ones, and can generate better test oracles when the code contains meaningful test or variable names. Finally, LLMgenerated test oracles have higher fault detection potential than the Evosuite ones;Oracle Generation;LLM-Pure-Prompting;GitBug, Other;GPT-Family;Other, Mutation Rate;-
P63;ChatAssert: LLM-Based Test Oracle Generation With External Tools Assistance;2025;Hayet2025;J: TSE;Journal;"@article{Hayet2025,
  title = ""ChatAssert: LLM-based Test Oracle Generation with External Tools Assistance"",
  author = ""Ishrak Hayet and Adam Scott and Marcelo Damorim"",
  journal = ""IEEE Transactions on Software Engineering"",
  volume = ""51"",
  number = ""1"",
  pages = ""305--319"",
  year = ""2025"",
  publisher = ""Institute of Electrical and Electronics Engineers Inc."",
  doi = ""10.1109/TSE.2024.3519159"",
  url = ""https://doi.org/10.1109/TSE.2024.3519159""
}
";Research Contribution;"—Test oracle generation is an important and challenging problem. Neural-based solutions have been recently proposed
for oracle generation but they are still inaccurate. For example,
the accuracy of the state-of-the-art technique TECO is only
27.5% on its dataset including 3,540 test cases. We propose
CHATASSERT, a prompt engineering framework designed for
oracle generation that uses dynamic and static information to
iteratively refine prompts for querying large language models
(LLMs). CHATASSERT uses code summaries and examples to
assist an LLM in generating candidate test oracles, uses a
lightweight static analysis to assist the LLM in repairing generated oracles that fail to compile, and uses dynamic information
obtained from test runs to help the LLM in repairing oracles
that compile but do not pass. Experimental results using an
independent publicly-available dataset show that CHATASSERT
improves the state-of-the-art technique, TECO, on key evaluation
metrics. For example, it improves Acc@1 by 15%. Overall, results
provide initial yet strong evidence that using external tools in the
formulation of prompts is an important aid in LLM-based oracle
generation.";Oracle Generation;LLM-Pure-Prompting;Other;Mistral, CodeStral, Magicoder, ATLAS, TOGA, TECO, GPT-Family;Mutation Rate, BLEU, CodeBLEU, EditSim, ROUGE, Acc@x;ChatAssert
P64;TOGLL: Correct and Strong Test Oracle Generation with LLMs;2024;Hossain2025;C: ICSE;Conference;"@Inproceedings{Hossain2025,
  author =       ""Hossain, Soneya Binta and Dwyer, Matthew"",
  title =        ""TOGLL: Correct and Strong Test Oracle Generation with LLMs"",
  booktitle =    ""Proceedings of IEEE/ACM 47th International Conference on Software Engineering (ICSE)"",
  series =       ""ICSE '25"",
  editor =       """",
  volume =       """",
  year =         ""2025"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Los Alamitos, CA, USA"",
  pages =        ""635-635"",
  doi =          ""10.1109/ICSE55347.2025.00098"",
  url =          ""https://doi.org/10.1109/ICSE55347.2025.00098""
}
";Research Contribution;"Test oracles play a crucial role in software testing, enabling effective bug detection. Despite initial promise, neural-based methods for automated test oracle generation often result in a large number of false positives and weaker test oracles. While LLMs have demonstrated impressive effectiveness in various software engineering tasks, including code generation, test case creation, and bug fixing, there remains a notable absence of large-scale studies exploring their effectiveness in test oracle generation. The question of whether LLMs can address the challenges in effective oracle generation is both compelling and requires thorough investigation.
In this research, we present the first comprehensive study to investigate the capabilities of LLMs in generating correct, diverse, and strong test oracles capable of effectively identifying a large number of unique bugs. To this end, we fine-tuned seven code LLMs using six distinct prompts on the SF110 dataset. Utilizing the most effective fine-tuned LLM and prompt pair, we introduce TOGLL, a novel LLM-based method for test oracle generation. To investigate the generalizability of TOGLL, we conduct studies on 25 large-scale Java projects. Besides assessing the correctness, we also assess the diversity and strength of the generated oracles. We compare the results against EvoSuite and the state-of-the-art neural method, TOGA. Our findings reveal that TOGLL can produce 3.8 times more correct assertion oracles and 4.9 times more exception oracles. Moreover, our findings demonstrate that TOGLL is capable of generating significantly diverse test oracles. It can detect 1,023 unique bugs that EvoSuite cannot, which is ten times more than what the previous SOTA neural-based method, TOGA, can detect";Oracle Generation;LLM-Pure-FineTune;OracleEval25, Defects4J;Other;% Pass, % Build, Bugs Rate, Mutation Rate;
P65;Fill in the Blank: Context-aware Automated Text Input Generation for Mobile GUI Testing;2023;Liu2023;C: ICSE;Conference;"@Inproceedings{Liu2023 ,
  author =       ""Zhe Liu and Chunyang Chen and Junjie Wang and Xing Che and Yuekai Huang and Jun Hu and Qing Wang"",
  title =        ""Fill in the Blank: Context-aware Automated Text Input Generation for Mobile GUI Testing"",
  booktitle =    ""Proceedings of IEEE/ACM 46th International Conference on Software Engineering (ICSE)"",
  series =       ""ICSE '23"",
  editor =       """",
  volume =       """",
  year =         ""2023"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Melbourne, Australia"",
  pages =        ""1355-1367"",
  doi =          ""10.1109/ICSE48619.2023.00119"",
  url =          ""https://doi.org/10.1109/ICSE48619.2023.00119""
}";Research Contribution;Automated GUI testing is widely used to help ensure the quality of mobile apps. However, many GUIs require appropriate text inputs to proceed to the next page, which remains a prominent obstacle for testing coverage. Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), it is challenging to automate the text input generation. Inspired by the fact that the pre-trained Large Language Model (LLM) has made outstanding progress in text generation, we propose an approach named QTypist based on LLM for intelligently generating semantic input text according to the GUI context. To boost the performance of LLM in the mobile testing scenario, we develop a prompt-based data construction and tuning method which automatically extracts the prompts and answers for model tuning. We evaluate QTypist on 106 apps from Google Play, and the result shows that the passing rate of QTypist is 87%, which is 93% higher than the best baseline. We also integrate QTypist with the automated GUI testing tools and it can cover 42% more app activities, 52% more pages, and subsequently help reveal 122% more bugs compared with the raw tool.;High-Level Test Gen;LLM-Pure-FineTune;Custom;GPT-Family;% Pass, Other;-
P66;On the Evaluation of Large Language Models in Unit Test Generation                                                                                                                                                                                                                                          ;2024;Lin2024;C: ASE;Conference;"@Inproceedings{Lin2024 ,
  author =       ""Yang, Lin and Yang, Chen and Gao, Shutao and Wang, Weijing and Wang, Bo and Zhu, Qihao and Chu, Xiao and Zhou, Jianyi and Liang, Guangtai and Wang, Qianxiang and Chen, Junjie "",
  title =        ""On the Evaluation of Large Language Models in Unit Test Generation"",
  booktitle =    ""Proceedings of 39th IEEE/ACM International Conference on Automated Software Engineering (ASE)"",
  series =       ""ASE '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Sacramento, California, United States"",
  pages =        ""1607-1619"",
  doi =          ""10.1145/3691620.3695529"",
  url =          ""https://doi.org/10.1145/3691620.3695529""
}";Research Contribution;Unit testing is an essential activity in software development for verifying the correctness of software components. However, manually writing unit tests is challenging and time-consuming. The emergence of Large Language Models (LLMs) offers a new direction for automating unit test generation. Existing research primarily focuses on closed-source LLMs (e.g., ChatGPT and CodeX) with fixed prompting strategies, leaving the capabilities of advanced open-source LLMs with various prompting settings unexplored. Particularly, open-source LLMs offer advantages in data privacy protection and have demonstrated superior performance in some tasks. Moreover, effective prompting is crucial for maximizing LLMs' capabilities. In this paper, we conduct the first empirical study to fill this gap, based on 17 Java projects, five widely-used open-source LLMs with different structures and parameter sizes, and comprehensive evaluation metrics. Our findings highlight the significant influence of various prompt factors, show the performance of open-source LLMs compared to the commercial GPT-4 and the traditional Evosuite, and identify limitations in LLM-based unit test generation. We then derive a series of implications from our study to guide future research and practical use of LLM-based unit test generation.;Unit Test Generation;LLM-Pure-Prompting;Defects4J;CodeLlama, DeepSeekCoder;;
P67;Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning;2023;Noor2023;C: ICSE;Conference;"@Inproceedings{Noor2023 ,
  author =       ""Nashid, Noor and Sintaha, Mifta and Mesbah, Ali"",
  title =        ""Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning"",
  booktitle =    ""Proceedings of IEEE/ACM 45th International Conference on Software Engineering (ICSE)"",
  series =       ""ICSE '23"",
  editor =       """",
  volume =       """",
  year =         ""2023"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Melbourne, Australia"",
  pages =        ""2450-2462"",
  doi =          ""10.1109/ICSE48619.2023.00205"",
  url =          ""https://doi.org/10.1109/ICSE48619.2023.00205""
}";Research Contribution;"—Large language models trained on massive code
corpora can generalize to new tasks without the need for taskspecific fine-tuning. In few-shot learning, these models take as
input a prompt, composed of natural language instructions, a
few instances of task demonstration, and a query and generate
an output. However, the creation of an effective prompt for coderelated tasks in few-shot learning has received little attention.
We present a technique for prompt creation that automatically
retrieves code demonstrations similar to the developer task, based
on embedding or frequency analysis. We apply our approach,
CEDAR, to two different programming languages, statically and
dynamically typed, and two different tasks, namely, test assertion
generation and program repair. For each task, we compare
CEDAR with state-of-the-art task-specific and fine-tuned models.
The empirical results show that, with only a few relevant code
demonstrations, our prompt creation technique is effective in
both tasks with an accuracy of 76% and 52% for exact matches in
test assertion generation and program repair tasks, respectively.
For assertion generation, CEDAR outperforms existing taskspecific and fine-tuned models by 333% and 11%, respectively.
For program repair, CEDAR yields 189% better accuracy than
task-specific models and is competitive with recent fine-tuned
models. These findings have practical implications for practitioners, as CEDAR could potentially be applied to multilingual
and multitask settings without task or language-specific training
with minimal examples and effort";Oracle Generation;LLM-Pure-Prompting;Other;Other;Execution Time, Other;CEDAR
P68;Feature-Driven End-To-End Test Generation;2025;Alian2025;C: ICSE;Conference;"@Inproceedings{Alian2025 ,
  author =       ""Alian, Parsa and Nashid, Noor and Shahbandeh, Mobina and Shabani, Taha and Mesbah, Ali"",
  title =        ""Feature-Driven End-To-End Test Generation"",
  booktitle =    ""Proceedings of IEEE/ACM 47th International Conference on Software Engineering (ICSE) "",
  series =       ""ICSE '25"",
  editor =       """",
  volume =       """",
  year =         ""2025"",
  publisher =    ""IEEE Computer Society"",
  address =      ""IEEE/ACM 47th International Conference on Software Engineering (ICSE) "",
  pages =        ""678--678"",
  doi =          ""10.1109/ICSE55347.2025.00141"",
  url =          ""https://doi.org/10.1109/ICSE55347.2025.00141""
}
";Research Contribution;End-to-end (E2E) testing is essential for ensuring web application quality. However, manual test creation is time-consuming and current test generation techniques produce random tests. In this paper, we present AUTOE2E, a novel approach that leverages Large Language Models (LLMs) to automate the generation of semantically meaningful feature-driven E2E test cases for web applications. AUTOE2E intelligently infers potential features within a web application and translates them into executable test scenarios. Furthermore, we address a critical gap in the research community by introducing E2EBENCH, a new benchmark for automatically assessing the feature coverage of E2E test suites. Our evaluation on E2EBENCH demonstrates that AUTOE2E achieves an average feature coverage of 79%, outperforming the best baseline by 558%, highlighting its effectiveness in generating high-quality, comprehensive test cases.;High-Level Test Gen;LLM-Pure-Prompting;E2EBench;Claude 3;BS Coverage, ML-metrics;-
P70;Automated Soap Opera Testing Directed by LLMs and Scenario Knowledge: Feasibility, Challenges, and Road Ahead;2024;SuYanqi2024;J: Proc. ACM Softw. Eng;Journal;"@article{SuYanqi2024,
  title = ""Automated Soap Opera Testing Directed by LLMs and Scenario Knowledge: Feasibility, Challenges, and Road Ahead"",
  author = ""Su, Yanqi and Xing, Zhenchang and Wang, Chong and Chen, Chunyang and Xu, Sherry (Xiwei) and Lu, Qinghua and Zhu, Liming"",
  journal = ""Proc. ACM Softw. Eng."",
  volume = ""2"",
  number = ""FSE"",
  pages = ""22"",
  year = ""2025"",
  publisher = ""Association for Computing Machinery"",
  doi = ""10.1145/3715752"",
  url = ""https://doi.org/10.1145/3715752""
}";Research Contribution;Exploratory testing (ET) harnesses tester's knowledge, creativity, and experience to create varying tests that uncover unexpected bugs from the end-user's perspective. Although ET has proven effective in system-level testing of interactive systems, the need for manual execution has hindered large-scale adoption. In this work, we explore the feasibility, challenges and road ahead of automated scenario-based ET (a.k.a soap opera testing). We conduct a formative study, identifying key insights for effective manual soap opera testing and challenges in automating the process. We then develop a multi-agent system leveraging LLMs and a Scenario Knowledge Graph (SKG) to automate soap opera testing. The system consists of three multi-modal agents, Planner, Player, and Detector that collaborate to execute tests and identify potential bugs. Experimental results demonstrate the potential of automated soap opera testing, but there remains a significant gap compared to manual execution, especially under-explored scenario boundaries and incorrectly identified bugs. Based on the observation, we envision road ahead for the future of automated soap opera testing, focusing on three key aspects: the synergy of neural and symbolic approaches, human-AI co-learning, and the integration of soap opera testing with broader software engineering practices. These insights aim to guide and inspire the future research.;High-Level Test Gen;LLM-Pure-Prompting;Custom;N/S;Bugs Rate, ML-metrics;-
P71;Make LLM a Testing Expert: Bringing Human-Like Interaction to Mobile GUI Testing via Functionality-Aware Decisions;2024;LiuGUI2024;C: ICSE;Conference;"@Inproceedings{LiuGUI2024 ,
  author =       ""Liu, Zhe and Chen, Chunyang and Wang, Junjie and Chen, Mengzhuo and Wu, Boyu and Che, Xing and Wang, Dandan and Wang, Qing"",
  title =        ""Make LLM a Testing Expert: Bringing Human-like Interaction to Mobile GUI Testing via Functionality-aware Decisions"",
  booktitle =    ""Proceedings of the IEEE/ACM 46th International Conference on Software Engineering"",
  series =       ""ICSE '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""IEEE/ACM 47th International Conference on Software Engineering (ICSE) "",
  pages =        ""1--13"",
  doi =          ""10.1145/3597503.3639180"",
  url =          ""https://doi.org/10.1145/3597503.3639180""
}";Research Contribution;Automated Graphical User Interface (GUI) testing plays a crucial role in ensuring app quality, especially as mobile applications have become an integral part of our daily lives. Despite the growing popularity of learning-based techniques in automated GUI testing due to their ability to generate human-like interactions, they still suffer from several limitations, such as low testing coverage, inadequate generalization capabilities, and heavy reliance on training data. Inspired by the success of Large Language Models (LLMs) like ChatGPT in natural language understanding and question answering, we formulate the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process. Within this framework, we have also introduced a functionality-aware memory prompting mechanism that equips the LLM with the ability to retain testing knowledge of the whole process and conduct long-term, functionality-based reasoning to guide exploration. We evaluate it on 93 apps from Google Play and demonstrate that it outperforms the best baseline by 32% in activity coverage, and detects 31% more bugs at a faster rate. Moreover, GPTDroid identifies 53 new bugs on Google Play, of which 35 have been confirmed and fixed.;High-Level Test Gen;LLM-Pure-Prompting;Themis, Custom;GPT-Family;Test Coverage, Bugs Rate, Other;GPTDroid
P72;Towards an understanding of large language models in software engineering tasks;2025;Zheng2025;J: Emp. Soft. Eng.;Journal;"@article{Zheng2025,
  title = ""Towards an understanding of large language models in software engineering tasks"",
  author = ""Zheng, Zibin and Ning, Kaiwen and Zhong, Qingyuan and Chen, Jiachi and Chen, Wenqing and Guo, Lianghong and Wang, Weicheng and Wang, Yanlin"",
  journal = ""Empirical Softw. Engg."",
  volume = ""30"",
  number = ""2"",
  pages = ""1--38"",
  year = ""2025"",
  publisher = ""Kluwer Academic Publishers"",
  doi = ""10.1007/s10664-024-10602-0"",
  url = ""https://doi.org/10.1007/s10664-024-10602-0""
}";Survey;"Large Language Models (LLMs) have drawn widespread attention
and research due to their astounding performance in text generation and
reasoning tasks. Derivative products, like ChatGPT, have been extensively
deployed and highly sought after. Meanwhile, the evaluation and optimization
of LLMs in software engineering tasks, such as code generation, have become
a research focus. However, there is still a lack of systematic research on
applying and evaluating LLMs in software engineering. Therefore, this paper
comprehensively investigate and collate the research and products combining
LLMs with software engineering, aiming to answer two questions: (1) What
are the current integrations of LLMs with software engineering? (2) Can
LLMs effectively handle software engineering tasks? To find the answers,
we have collected related literature as extensively as possible from seven
mainstream databases and selected 123 timely papers published starting from
2022 for analysis. We have categorized these papers in detail and reviewed
the current research status of LLMs from the perspective of seven major
software engineering tasks, hoping this will help researchers better grasp the
research trends and address the issues when applying LLMs. Meanwhile, we
have also organized and presented papers with evaluation content to reveal the
performance and effectiveness of LLMs in various software engineering tasks,
guiding researchers and developers to optimize.";Reflections;None;No Bmk-Ds;;;-
P74;Unit Test Case Generation with Transformers and Focal Context ;2021;Tufano2021;arXiv;arXiv;"@misc{Tufano2021,
  title = ""Unit Test Case Generation with Transformers and Focal Context"",
  author = ""Michele Tufano and Dawn Drain and Alexey Svyatkovskiy and Shao Kun Deng and Neel Sundaresan"",
  archivePrefix = ""arXiv Preprint"",
  year = ""2021"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2009.05617"",
  url = ""https://doi.org/10.48550/arXiv.2009.05617""
}";Research Contribution;Automated unit test case generation tools facilitate test-driven development and support developers by suggesting tests intended to identify flaws in their code. Existing approaches are usually guided by the test coverage criteria, generating synthetic test cases that are often difficult for developers to read or understand. In this paper we propose AthenaTest, an approach that aims to generate unit test cases by learning from real-world focal methods and developer-written testcases. We formulate unit test case generation as a sequence-to-sequence learning task, adopting a two-step training procedure consisting of denoising pretraining on a large unsupervised Java corpus, and supervised finetuning for a downstream translation task of generating unit tests. We investigate the impact of natural language and source code pretraining, as well as the focal context information surrounding the focal method. Both techniques provide improvements in terms of validation loss, with pretraining yielding 25% relative improvement and focal context providing additional 11.1% improvement. We also introduce Methods2Test, the largest publicly available supervised parallel corpus of unit test case methods and corresponding focal methods in Java, which comprises 780K test cases mined from 91K open-source repositories from GitHub. We evaluate AthenaTest on five defects4j projects, generating 25K passing test cases covering 43.7% of the focal methods with only 30 attempts. We execute the test cases, collect test coverage information, and compare them with test cases generated by EvoSuite and GPT-3, finding that our approach outperforms GPT-3 and has comparable coverage w.r.t. EvoSuite. Finally, we survey professional developers on their preference in terms of readability, understandability, and testing effectiveness of the generated tests, showing overwhelmingly preference towards AthenaTest;Unit Test Generation;LLM-Pure-FineTune;Methods2Test, CodeSearchNet, Defects4J;BERT, GPT-Family;% Build, % Pass, Test Coverage, % Test Fail, % Test Syntax Error, % Test Correct, Validation Loss;ATHENEATEST
P75;TOGA: A Neural Method for Test Oracle Generation;2022;Elizabeth2022;C: ICSE;Conference;"@Inproceedings{Elizabeth2022 ,
  author =       ""Dinella, Elizabeth and Ryan, Gabriel and Mytkowicz, Todd and Lahiri, Shuvendu K."",
  title =        ""TOGA: a neural method for test oracle generation"",
  booktitle =    ""Proceedings of the 44th International Conference on Software Engineering"",
  series =       ""ICSE '22"",
  editor =       """",
  volume =       """",
  year =         ""2022"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Pittsburgh, PA, USA"",
  pages =        ""2130–2141"",
  doi =          ""10.1145/3510003.3510141"",
  url =          ""https://doi.org/10.1145/3510003.3510141""
}";Research Contribution;Testing is widely recognized as an important stage of the software development lifecycle. Effective software testing can provide benef its such as bug finding, preventing regressions, and documentation. In terms of documentation, unit tests express a unit’s intended functionality, as conceived by the developer. A test oracle, typically expressed as an condition, documents the intended behavior of a unit under a given test prefix. Synthesizing a functional test oracle is a challenging problem, as it must capture the intended functionality rather than the implemented functionality. In this paper, we propose TOGA (a neural method for Test Oracle GenerAtion), a unified transformer-based neural approach to infer both exceptional and assertion test oracles based on the context of the focal method. Our approach can handle units with ambiguous or missing documentation, and evenunits withamissingimplementation. We evaluate our approach on both oracle inference accuracy and functional bug-finding. Our technique improves accuracy by 33% over existing oracle inference approaches, achieving 96% overall accuracy on a held out test dataset. Furthermore, we show that when integrated with a automated test generation tool (EvoSuite), our approach finds 57 real world bugs in large-scale Java programs, including 30 bugs that are not found by anyother automatedtesting method in our evaluation.;Oracle Generation;LLM-Pure-FineTune;Defects4J, Methods2Test, Other;CodeBert;Other;TOGA
P76;Learning Deep Semantics for Test Completion;2023;Nie2023;C: ICSE;Conference;"@Inproceedings{Nie2023 ,
  author =       ""Nie, Pengyu and Banerjee, Rahul and Li, Junyi Jessy and Mooney, Raymond J. and Gligoric, Milos"",
  title =        ""Learning Deep Semantics for Test Completion"",
  booktitle =    ""Proceedings of the 45th International Conference on Software Engineering"",
  series =       ""ICSE '23"",
  editor =       """",
  volume =       """",
  year =         ""2023"",
  publisher =    ""IEEE Press"",
  address =      ""Melbourne, Victoria, Australia"",
  pages =        ""2111–2123"",
  doi =          ""10.1109/ICSE48619.2023.00178"",
  url =          ""https://doi.org/10.1109/ICSE48619.2023.00178""
}";Research Contribution;Writing tests is a time-consuming yet essential task during software development. We propose to leverage recent advances in deep learning for text and code generation to assist developers in writing tests. We formalize the novel task of test completion to automatically complete the next statement in a test method based on the context of prior statements and the code under test. We develop TECO—a deep learning model using code semantics for test completion. The key insight underlying TECO is that predicting the next statement in a test method requires reasoning about code execution, which is hard to do with only syntax-level data that existing code completion models use. TECO extracts and uses six kinds of code semantics data, including the execution result of prior statements and the execution context of the test method. To provide a testbed for this new task, as well as to evaluate TECO, we collect a corpus of 130,934 test methods from 1,270 open-source Java projects. Our results show that TECO achieves an exact-match accuracy of 18, which is 29% higher than the best baseline using syntax-level data only. When measuring functional correctness of generated next statement, TECO can generate runnable code in 29% of the cases compared to 18% obtained by the best baseline. Moreover, TECO is significantly better than prior work on test oracle generation.;Unit Test Generation, Oracle Generation;LLM-Pure-FineTune;CodeSearchNet;TECO, CodeT5, ATLAS, TOGA, CodeGPT;% Build, %Run, BLEU, CodeBLEU, XMatch, ROUGE, EditSim, Acc@x;TECO
P77;Neural-Based Test Oracle Generation: A Large-Scale Evaluation and Lessons Learned ;2023;Soneya2023;C: FSE;Conference;"@Inproceedings{Soneya2023 ,
  author =       ""Hossain, Soneya Binta and Filieri, Antonio and Dwyer, Matthew B. and Elbaum, Sebastian and Visser, Willem"",
  title =        ""Neural-Based Test Oracle Generation: A Large-Scale Evaluation and Lessons Learned"",
  booktitle =    ""Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering"",
  series =       """",
  editor =       """",
  volume =       """",
  year =         ""2023"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""San Francisco, CA, USA"",
  pages =        ""120–132"",
  doi =          ""10.1145/3611643.3616265"",
  url =          ""https://doi.org/10.1145/3611643.361626"",
  number =       """",
  month =        """",
  organization = """",
  note =         """",
}";Survey;Defining test oracles is crucial and central to test development, but manual construction of oracles is expensive. While recent neuralbased automated test oracle generation techniques have shown promise, their real-world effectiveness remains a compelling question requiring further exploration and understanding. This paper investigates the effectiveness of TOGA, a recently developed neuralbased method for automatic test oracle generation by Dinella et al.[16]. TOGA utilizes EvoSuite-generated test inputs and generates both exception and assertion oracles. In a Defects4j study, TOGA outperformed specification, search, and neural-based techniques, detecting 57 bugs, including 30 unique bugs not detected by other methods. To gain a deeper understanding of its applicability in real-world settings, we conducted a series of external, extended, and conceptual replication studies of TOGA. In a large-scale study involving 25 real-world Java systems, 223.5K test cases, and 51K injected faults, we evaluate TOGA’s ability to improve fault-detection effectiveness relative to the stateof-the-practice and the state-of-the-art. We find that TOGA misclassifies the type of oracle needed 24% of the time and that when it classifies correctly around 62% of the time it is not confident enough to generate any assertion oracle. When it does generate an assertion oracle, more than 47% of them are false positives, and the true positive assertions only increase fault detection by 0.3% relative to prior work. These findings expose limitations of the state-of-the-art neural-based oracle generation technique, provide valuable insights for improvement, and offer lessons for evaluating future automated oracle generation methods.;Oracle Generation, Reflections;None;Defects4J, Other;;Bugs Rate, Other;-
P78;CAT-LM Training Language Models on Aligned Code And Tests;2023;Rao2024;C: ASE;Conference;"@Inproceedings{Rao2024 ,
  author =       ""Rao, Nikitha and Jain, Kush and Alon, Uri and Goues, Claire Le and Hellendoorn, Vincent J."",
  title =        ""CAT-LM Training Language Models on Aligned Code and Tests"",
  booktitle =    ""Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering"",
  series =       """",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Press"",
  address =      ""Echternach, Luxembourg"",
  pages =        ""409–420"",
  doi =          ""10.1109/ASE56229.2023.00193"",
  url =          ""https://doi.org/10.1109/ASE56229.2023.00193""
}";Research Contribution;Testing is an integral but often neglected part of the software development process. Classical test generation tools such as EvoSuite generate behavioral test suites by optimizing for coverage, but tend to produce tests that are hard to understand. Language models trained on code can generate code that is highly similar to that written by humans, but current models are trained to generate each file separately, as is standard practice in natural language processing, and thus fail to consider the codeunder-test context when producing a test file. In this work, we propose the Aligned Code And Tests Language Model (CATLM), a GPT-style language model with 2.7 Billion parameters, trained on a corpus of Python and Java projects. We utilize a novel pretraining signal that explicitly considers the mapping between code and test files when available. We also drastically increase the maximum sequence length of inputs to 8,192 tokens, 4x more than typical code generation models, to ensure that the code context is available to the model when generating test code. We analyze its usefulness for realistic applications, showing that sampling with filtering (e.g., by compilability, coverage) allows it to efficiently produce tests that achieve coverage similar to ones written by developers while resembling their writing style. By utilizing the code context, CAT-LM generates more valid tests than even much larger language models trained with more data (CodeGen 16B and StarCoder) and substantially outperforms a recent test-specific model (TeCo) at test completion. Overall, our work highlights the importance of incorporating software-specific insights when training language models for code and paves the way to more powerful automated test generation.;Unit Test Generation;LLM-Pure-FineTune;Other;StarCoder, Codegen, TECO, GPT-Family;ROUGE, CodeBLEU, % Build, % Pass, XMatch;CAT-LLM
P79;Assessing Evaluation Metrics for Neural Test Oracle Generation;2024;Jiho2024;J: TSE;Journal;"@article{Jiho2024,
  title = ""Assessing Evaluation Metrics for Neural Test Oracle Generation"",
  author = ""Shin, Jiho and Hemmati, Hadi and Wei, Moshi and Wang, Song"",
  journal = ""IEEE Trans. Softw. Eng."",
  volume = ""50"",
  number = ""9"",
  pages = ""2337--2349"",
  year = ""2024"",
  publisher = ""IEEE Press"",
  doi = ""10.1109/TSE.2024.3433463"",
  url = ""https://doi.org/10.1109/TSE.2024.3433463""
}";Survey;Recently, deep learning models have shown promising results in test oracles generation. Static evaluation metrics from Natural Language Generation (NLG) such as BLEU, CodeBLEU, ROUGE-L, METEOR, and Accuracy, which is mainly based on textual comparisons, have been widely adopted to measure the performance of Neural Oracle Generation (NOG) models. However, these NLG-based metrics may not reflect the testing effectiveness of the generated oracle within a test suite, which is often measured by dynamic (execution-based) test adequacy metrics such as code coverage and mutation score. In this work, we revisit existing oracle generation studies plus ChatGPT to empirically investigate the current standing of their performance in both NLG-based and test adequacy metrics. Specifically, we train and run four state-of-the-art test oracle generation models on five NLG-based and two test adequacy metrics for our analysis. We apply two different correlation analyses between these two different sets of metrics. Surprisingly, we found no significant correlation between the NLG-based metrics and test adequacy metrics. For instance, oracles generated from ChatGPT on the project activemq-artemis had the highest performance on all the NLG-based metrics among the studied NOGs, however, it had the most number of projects with a decrease in test adequacy metrics compared to all the studied NOGs. We further conduct a qualitative analysis to explore the reasons behind our observations, we found that oracles with high NLG-based metrics but low test adequacy metrics tend to have complex or multiple chained method invocations within the oracle’s parameters, making it hard for the model to generate completely, affecting the test adequacy metrics. On the other hand, oracles with low NLG-based metrics but high test adequacy metrics tend to have to call different assertion types or a different method that functions similarly to the ones in the ground truth. Overall, this work complements prior studies on test oracle generation with an extensive performance evaluation with both NLG andtest adequacy metrics and provides guidelines for better assessment of deep learning applications in software test generation in the future.;Oracle Generation, Reflections;None;Other;;BLEU, CodeBLEU, METEOR, ROUGE, EditSim, Other, Test Coverage, Mutation Rate;
P80;Towards More Realistic Evaluation for Neural Test Oracle Generation;2023;Zhongxin2023;C: ISSTA;Conference;"@Inproceedings{Zhongxin2023 ,
  author =       ""Liu, Zhongxin and Liu, Kui and Xia, Xin and Yang, Xiaohu"",
  title =        ""Towards More Realistic Evaluation for Neural Test Oracle Generation"",
  booktitle =    ""Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis"",
  series =       ""ISSTA 2023"",
  editor =       """",
  volume =       """",
  year =         ""2023"",
  publisher =    ""Association for Computing Machinery"",
  location =      ""Seattle, WA, USA"",
  pages =        ""589–-600"",
  doi =          ""10.1145/3597926.3598080"",
  url =          ""https://doi.org/10.1145/3597926.3598080"",
  number =       """",
  month =        """",
  organization = """",
  note =         """",
}";Evaluation;Unit testing has become an essential practice during software development and maintenance. Effective unit tests can help guard and improve software quality but require a substantial amount of time and effort to write and maintain. A unit test consists of a test prefix and a test oracle. Synthesizing test oracles, especially functional oracles, is a well-known challenging problem. Recent studies proposed to leverage neural models to generate test oracles, i.e., neural test oracle generation (NTOG), and obtained promising results. However, after a systematic inspection, we find there are some inappropriate settings in existing evaluation methods for NTOG. These settings could mislead the understanding of existing NTOG approaches’ performance. We summarize them as ① generating test prefixes from bug-fixed program versions, ② evaluating with an unrealistic metric, and ③ lacking a straightforward baseline. In this paper, we first investigate the impacts of these settings on evaluating and understanding the performance of NTOG approaches. We find that ❶ unrealistically generating test prefixes from bug-fixed program versions inflates the number of bugs found by the state-of-the-art NTOG approach TOGA by 61.8%, ❷ FPR (False Positive Rate) is not a realistic evaluation metric and the Precision of TOGA is only 0.38%, and ❸ a straightforward baseline NoException, which simply expects no exception should be raised, can find 61% of the bugs found by TOGA with twice the Precision. Furthermore, we introduce an additional ranking step to existing evaluation methods and propose an evaluation metric named Found@Ktobetter measure the cost-effectiveness of NTOG approaches in terms of bug-finding. We propose a novel unsupervised ranking method to instantiate this ranking step, significantly improving the cost-effectiveness of TOGA. Eventually, based on our experimental results and observations, we propose a more realistic;Oracle Generation, Reflections;None;Defects4J;;;
P81;A3Test: Assertion-Augmented Automated Test Case Generation;2024;Alagarsamy2024;J: IST;Journal;"@article{Alagarsamy2024,
  title = ""A3Test: Assertion-Augmented Automated Test case generation"",
  author = ""Saranya Alagarsamy and Chakkrit Tantithamthavorn and Aldeida Aleti"",
  journal = ""Information and Software Technology"",
  volume = ""176"",
  number = """",
  pages = ""107565"",
  year = ""2024"",
  publisher = ""Institute of Electrical and Electronics Engineers Inc."",
  doi = ""10.1016/j.infsof.2024.107565"",
  url = ""https://doi.org/10.1016/j.infsof.2024.107565""
}
";Research Contribution;"Test case generation is an important activity, yet a time-consuming and laborious task. Recently, AthenaTest -- a deep learning approach for generating unit test cases -- is proposed. However, AthenaTest can generate less than one-fifth of the test cases correctly, due to a lack of assertion knowledge and test signature verification. In this paper, we propose A3Test, a DL-based test case generation approach that is augmented by assertion knowledge with a mechanism to verify naming consistency and test signatures. A3Test leverages the domain adaptation principles where the goal is to adapt the existing knowledge from an assertion generation task to the test case generation task. We also introduce a verification approach to verify naming consistency and test signatures. Through an evaluation of 5,278 focal methods from the Defects4j dataset, we find that our A3Test (1) achieves 147% more correct test cases and 15% more method coverage, with a lower number of generated test cases than AthenaTest; (2) still outperforms the existing pre-trained models for the test case generation task; (3) contributes substantially to performance improvement via our own proposed assertion pre-training and the verification components; (4) is 97.2% much faster while being more accurate than AthenaTest.";Unit Test Generation, Oracle Generation;LLM-Pure-FineTune;Defects4J;;% Build, Test Coverage, Execution Time, Mutation Rate, Other;A3Test
P82;An Empirical Study on Focal Methods in Deep-Learning-Based Approaches for Assertion Generation;2024;Yibo2024;J: Proc. ACM Softw. Eng;Conference;"@article{Yibo2024,
  title = ""An Empirical Study on Focal Methods in Deep-Learning-Based Approaches for Assertion Generation"",
  author = ""He, Yibo and Huang, Jiaming and Yu, Hao and Xie, Tao"",
  journal = ""Proc. ACM Softw. Eng."",
  volume = ""1"",
  number = ""FSE"",
  numpages = ""22"",
  year = ""2024"",
  publisher = ""Association for Computing Machinery"",
  doi = ""10.1145/3660785"",
  url = ""https://doi.org/10.1145/3660785""
}";Evaluation;"Unit testing is widely recognized as an essential aspect of the software development process. Generating high-quality assertions automatically is one of the most important and challenging problems in automatic unit test generation. To generate high-quality assertions, deep-learning-based approaches have been proposed in recent years. For state-of-the-art deep-learning-based approaches for assertion generation (DLAGs), the focal method (i.e., the main method under test) for a unit test case plays an important role of being a required part of the input to these approaches. To use DLAGs in practice, there are two main ways to provide a focal method for these approaches: (1) manually providing a developer-intended focal method or (2) identifying a likely focal method from the given test prefix (i.e., complete unit test code excluding assertions) with test-to-code traceability techniques. However, the state-of-the-art DLAGs are all evaluated on the ATLAS dataset, where the focal method for a test case is assumed as the last non-JUnit-API method invoked in the complete unit test code (i.e., code from both the test prefix and assertion portion). There exist two issues of the existing empirical evaluations of DLAGs, causing inaccurate assessment of DLAGs toward adoption in practice. First, it is unclear whether the last method call before assertions (LCBA) technique can accurately reflect developer-intended focal methods. Second, when applying DLAGs in practice, the assertion portion of a unit test is not available as a part of the input to DLAGs (actually being the output of DLAGs); thus, the assumption made by the ATLAS dataset does not hold in practical scenarios of applying DLAGs. To address the first issue, we conduct a study of seven test-to-code traceability techniques in the scenario of assertion generation. We find that the LCBA technique is not the best among the seven techniques and can accurately identify focal methods with only 43.38% precision and 38.42% recall; thus, the LCBA technique cannot accurately reflect developer-intended focal methods, raising a concern on using the ATLAS dataset for evaluation. To address the second issue along with the concern raised by the preceding finding, we apply all seven test-to-code traceability techniques, respectively, to identify focal methods automatically from only test prefixes and construct a new dataset named ATLAS+ by replacing the existing focal methods in the ATLAS dataset with the focal methods identified by the seven traceability techniques, respectively. On a test set from new ATLAS+, we evaluate four state-of-the-art DLAGs trained on a training set from the ATLAS dataset. We find that all of the four DLAGs achieve lower accuracy on a test set in ATLAS+ than the corresponding test set in the ATLAS dataset, indicating that DLAGs should be (re)evaluated with a test set in ATLAS+, which better reflects practical scenarios of providing focal methods than the ATLAS dataset. In addition, we evaluate state-of-the-art DLAGs trained on training sets in ATLAS+. We find that using training sets in ATLAS+ helps effectively improve the accuracy of the ATLAS approach and T5 approach over these approaches trained using the corresponding training set from the ATLAS dataset.";Oracle Generation, Reflections;None;Other;;Other, BLEU, Execution Time;
P83;ASTER: Natural and Multi-language Unit Test Generation with LLMs;2025;Pan2025;C: ICSE;Conference;"@Inproceedings{Pan2025 ,
  author =       ""Rangeet Pan and Myeongsoo Kim and Rahul Krishna and Raju Pavuluri and Saurabh Sinha"",
  title =        ""ASTER: Natural and Multi-language Unit Test Generation with LLMs"",
  booktitle =    ""Proceedings of the 47th International Conference on Software Engineering"",
  series =       ""ICSE '25"",
  editor =       """",
  volume =       """",
  year =         ""2025"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""ASTER: Natural and Multi-language Unit Test Generation with LLMs"",
  pages =        ""-"",
  doi =          ""10.48550/arXiv.2409.03093"",
  url =          ""https://doi.org/10.48550/arXiv.2409.03093""
}";Research Contribution;Implementing automated unit tests is an important but time-consuming activity in software development. To assist developers in this task, many techniques for automating unit test generation have been developed. However, despite this effort, usable tools exist for very few programming languages. Moreover, studies have found that automatically generated tests suffer poor readability and do not resemble developer-written tests. In this work, we present a rigorous investigation of how large language models (LLMs) can help bridge the gap. We describe a generic pipeline that incorporates static analysis to guide LLMs in generating compilable and high-coverage test cases. We illustrate how the pipeline can be applied to different programming languages, specifically Java and Python, and to complex software requiring environment mocking. We conducted an empirical study to assess the quality of the generated tests in terms of code coverage and test naturalness -- evaluating them on standard as well as enterprise Java applications and a large Python benchmark. Our results demonstrate that LLM-based test generation, when guided by static analysis, can be competitive with, and even outperform, state-of-the-art test-generation techniques in coverage achieved while also producing considerably more natural test cases that developers find easy to understand. We also present the results of a user study, conducted with 161 professional developers, that highlights the naturalness characteristics of the tests generated by our approach.;Unit Test Generation;LLM-Pure-Prompting;Apache-Commons, BugsInPy, Pynguin;CodeLlama, Granite, GPT-Family, Llama-Family;Test Coverage, % Test Smells, Other;-
P84;Exploring Automated Assertion Generation via Large Language Models;2025;ZhangTSE2025;J: TOSEM;Journal;"@article{ZhangTSE2025,
  title = ""Exploring Automated Assertion Generation via Large Language Models"",
  author = ""Zhang, Quanjun and Sun, Weifeng and Fang, Chunrong and Yu, Bowen and Li, Hongyan and Yan, Meng and Zhou, Jianyi and Chen, Zhenyu"",
  journal = ""ACM Trans. Softw. Eng. Methodol."",
  volume = ""34"",
  number = ""3"",
  pages = ""1--25"",
  year = ""2025"",
  publisher = ""Association for Computing Machinery"",
  doi = ""10.1145/3699598"",
  url = ""https://doi.org/10.1145/3699598""
}";Evaluation;"Unit testing aims to validate the correctness of software system units and has become an essential practice in software development and maintenance. However, it is incredibly time-consuming and labor-intensive for testing experts to write unit test cases manually, including test inputs (i.e., prefixes) and test oracles (i.e., assertions). Very recently, some techniques have been proposed to apply Large Language Models (LLMs) to generate unit assertions and have proven the potential in reducing manual testing efforts. However, there has been no systematic comparison of the effectiveness of these LLMs, and their pros and cons remain unexplored.
To bridge this gap, we perform the first extensive study on applying various LLMs to automated assertion generation. The experimental results on two independent datasets show that studied LLMs outperform six state-of-the-art techniques with a prediction accuracy of 51.82%–58.71% and 38.72%–48.19%. The improvements achieve 29.60% and 12.47% on average. Besides, as a representative LLM, CodeT5 consistently outperforms all studied LLMs and all baselines on both datasets, with an average improvement of 13.85% and 26.64%, respectively. We also explore the performance of generated assertions in detecting real-world bugs, and find LLMs are able to detect 32 bugs from Defects4J on average, with an improvement of 52.38% against the most recent approach EditAS. Inspired by the findings, we construct a simplistic retrieval-and-repair-enhanced LLM-based approach by transforming the assertion generation problem into a program repair task for retrieved similar assertions. Surprisingly, such a simplistic approach can further improve the prediction accuracy of LLMs by 9.40% on average, leading to new records on both datasets. Besides, we provide additional discussions from different aspects (e.g., the impact of assertion types and test lengths) to illustrate the capacity and limitations of LLM-based approaches. Finally, we further pinpoint various practical guidelines (e.g., the improvement of multiple candidate assertions) for advanced LLM-based assertion generation in the near future. Overall, our work underscores the promising future of adopting off-the-shelf LLMs to generate accurate and meaningful assertions in real-world test cases and reduce the manual efforts of unit testing experts in practical scenarios.";Oracle Generation, Reflections;Hybrid-Prompting;Defects4J, No Bmk-Ds;CodeBert, GraphCodeBert, UniXcoder, CodeT5, CodeGPT, CodeLlama, Other;Bugs Rate, Accuracy;-
P85;Deep Multiple Assertions Generation;2024;Hailong2024;C: Other;Conference;"@Inproceedings{Hailong2024 ,
  author =       ""Wang, Hailong and Xu, Tongtong and Wang, Bei"",
  title =        ""Deep Multiple Assertions Generation"",
  booktitle =    ""Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering"",
  series =       ""FORGE '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Lisbon, Portugal"",
  pages =        ""1-–11"",
  doi =          ""10.1145/3650105.3652293"",
  url =          ""https://doi.org/10.1145/3650105.3652293""
}";Research Contribution;"Software testing is one of the most crucial parts of the software development life cycle. Developers spend substantial amount of time and efforts on software testing. Recently, there has been a growing scholarly interest in the automation of software testing. However, recent studies have revealed significant limitations in the quality and efficacy of the generated assert statements. These limitations primarily arise due to: (i) the inherent complexity involved in generating assert statements that are both meaningful and effective; (ii) the challenge of capturing the relationship between multiple assertions in a single test case. In recent research, deep learning techniques have been employed to generate meaningful assertions.
However, it is typical for a single assertion to be generated for each test case, which contradicts the current situation where over 40% of test cases contain multiple assertions.
To address these open challenges, we propose a novel approach, called DeepAssert that exploits the pre-trained model GraphCodeBERT to automatically generate multiple assertions for test methods.
It can recommend a sequence of assert statements effectively given a test method and a focal method (the method under test).
To evaluate the effectiveness of our approach, we conduct extensive experiments on the dataset built on the top of Methods2Test dataset. Experimental results show that DeepAssert achieves scores of 54.16%, 18.36%, and 15.38% in terms of CodeBLEU, accuracy and perfect prediction and substantially outperforms the state-of-the-art baselines by a large margin. Furthermore, we evaluate the effectiveness of DeepAssert on the task of bug detection and the result indicates that the assert sequences generated by DeepAssert can assist in exposing 42 real-world bugs extracting from Defects4J while only considering the first compiled assert sequence, outperforming the SOTA approaches by a large margin as well.";Oracle Generation;LLM-Pure-FineTune;Methods2Test, Defects4J;GraphCodeBert;Accuracy, CodeBLEU, Bugs Rate, Other;DeepAssert
P86;PyTester: Deep Reinforcement Learning for Text-to-Testcase Generation;2025;Takerngsaksiri2025;J: JSS;Journal;"@article{Takerngsaksiri2025,
  title = ""Pytester: Deep reinforcement learning for text-to-testcase generation"",
  author = ""Wannita Takerngsaksiri and Rujikorn Charakorn and Chakkrit Tantithamthavorn and Yuan-Fang Li"",
  journal = ""Journal of Systems and Software"",
  volume = ""224"",
  number = """",
  pages = ""112381"",
  year = ""2025"",
  publisher = ""Elsevier"",
  doi = ""10.1016/j.jss.2025.112381"",
  url = ""https://doi.org/10.1016/j.jss.2025.112381""
 }";Research Contribution;Test-driven development (TDD) is a widely-employed software development practice that mandates writing test cases based on requirements before writing the actual code. While writing test cases is the centerpiece of TDD, it is time-consuming, expensive, and often shunned by developers. To address these issues associated with TDD, automated test case generation approaches have recently been investigated. Such approaches take source code as input, but not the requirements. Therefore, existing work does not fully support true TDD, as actual code is required to generate test cases. In addition, current deep learning-based test case generation approaches are trained with one learning objective, i.e., to generate test cases that are exactly matched with the ground-truth test cases. However, such approaches may limit the model's ability to generate different yet correct test cases. In this paper, we introduce PyTester, a Text-to-Testcase generation approach that can automatically generate syntactically correct, executable, complete, and effective test cases while being aligned with a given natural language requirement. We evaluate PyTester on the public APPS benchmark dataset, and the results show that our Deep RL approach enables PyTester, a small language model, to outperform much larger language models like GPT3.5, StarCoder, and InCoder. Our findings suggest that future research could consider improving small over large LMs for better resource efficiency by integrating the SE domain knowledge into the design of reinforcement learning architecture.;;;;;;
P87;CasModaTest: A Cascaded and Model-agnostic Self-directed Framework for Unit Test Generation;2024;Ni2024;arXiv;arXiv;"@misc{Ni2024,
  title = ""CasModaTest: A Cascaded and Model-agnostic Self-directed Framework for Unit Test Generation"",
  author = ""Chao Ni and Xiaoya Wang and Liushan Chen and Dehai Zhao and Zhengong Cai and Shaohua Wang and Xiaohu Yang"",
  archivePrefix = ""arXiv Preprint"",
  year = ""2024"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2406.15743"",
  url = ""https://doi.org/10.48550/arXiv.2406.15743""
}";Research Contribution;"Though many machine learning (ML)-based unit testing generation approaches have been proposed and indeed achieved remarkable performance, they still have several limitations in effectiveness and practical usage. More precisely, existing ML-based approaches (1) generate partial content of a unit test, mainly focusing on test oracle generation; (2) mismatch the test prefix with the test oracle semantically; and (3) are highly bound with the close-sourced model, eventually damaging data security. We propose CasModaTest, a cascaded, model-agnostic, and end-to-end unit test generation framework, to alleviate the above limitations with two cascaded stages: test prefix generation and test oracle generation. Then, we manually build large-scale demo pools to provide CasModaTest with high-quality test prefixes and test oracles examples. Finally, CasModaTest automatically assembles the generated test prefixes and test oracles and compiles or executes them to check their effectiveness, optionally appending with several attempts to fix the errors occurring in compiling and executing phases. To evaluate the effectiveness of CasModaTest, we conduct large-scale experiments on a widely used dataset (Defects4J) and compare it with four state-of-the-art (SOTA) approaches by considering two performance measures. The experimental results indicate that CasModaTest outperforms all SOTAs with a substantial improvement (i.e., 60.62%-352.55% in terms of accuracy, 2.83%-87.27% in terms of focal method coverage). Besides, we also conduct experiments of CasModaTest on different open-source LLMs and find that CasModaTest can also achieve significant improvements over SOTAs (39.82%-293.96% and 9.25%-98.95% in terms of accuracy and focal method coverage, respectively) in end-to-end unit test generation";Unit Test Generation, Oracle Generation;LLM-Pure-Prompting;Defects4J;GPT-Family, DeepSeekCoder, CodeLlama;Test Coverage, Accuracy, Execution Time, Other;CasModaTest
P88;Advancing Bug Detection in Fastjson2 with Large Language Models Driven Unit Test Generation;2024;Zhong2024;arXiv;arXiv;"@misc{Zhong2024,
  title = ""Advancing Bug Detection in Fastjson2 with Large Language Models Driven Unit Test Generation"",
  author = ""Zhiyuan Zhong and Sinan Wang and Hailong Wang and Shaojin Wen and Hao Guan and Yida Tao and Yepang Liu"",
  archivePrefix = ""arXiv Preprint"",
  year = ""2024"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2410.09414"",
  url = ""https://doi.org/10.48550/arXiv.2410.09414""
}";Research Contribution;Data-serialization libraries are essential tools in software development, responsible for converting between programmable data structures and data persistence formats. Among them, JSON is the most popular choice for exchanging data between different systems and programming languages, while JSON libraries serve as the programming toolkit for this task. Despite their widespread use, bugs in JSON libraries can cause severe issues such as data inconsistencies and security vulnerabilities. Unit test generation techniques are widely adopted to identify bugs in various libraries. However, there is limited systematic testing effort specifically for exposing bugs within JSON libraries in industrial practice. In this paper, we propose JSONTestGen, an approach leveraging large language models (LLMs) to generate unit tests for fastjson2, a popular open source JSON library from Alibaba. Pre-trained on billions of open-source text and code corpora, LLMs have demonstrated remarkable abilities in programming tasks. Based on historical bug-triggering unit tests, we utilize LLMs to generate more diverse test cases by incorporating JSON domain-specific mutation rules. To systematically and efficiently identify potential bugs, we adopt differential testing on the results of the generated unit tests. Our evaluation shows that JSONTestGen outperforms existing test generation tools in unknown defect detection. With JSONTestGen, we found 34 real bugs in fastjson2, 30 of which have already been fixed, including 12 non-crashing bugs. While manual inspection reveals that LLM-generated tests can be erroneous, particularly with self-contradictory assertions, we demonstrate that LLMs have the potential for classifying false-positive test failures. This suggests a promising direction for improved test oracle automation in the future.;Unit Test Generation;Hybrid-Prompting;FastJson;GPT-Family, Llama-Family;Test Coverage, Bugs Rate, Accuracy;JSONTestGen
P89;Can ChatGPT advance software testing intelligence? An experience report on metamorphic testing;2023;Hung2023;arXiv;arXiv;"@misc{Hung2023,
  title = ""Can ChatGPT advance software testing intelligence? An experience report on metamorphic testing"",
  author = ""Quang-Hung Luu and Huai Liu and Tsong Yueh Chen"",
  archivePrefix = ""arXiv Preprint"",
  year = ""2023"",
  publisher = ""arXiv"",
  doi = ""10.48550/arXiv.2310.19204"",
  url = ""https://doi.org/10.48550/arXiv.2310.19204""
}";Evaluation;"This paper reports on a pilot study of using ChatGPT, a language model based on GPT-3.5 architecture, for
automatic generation of metamorphic relations (MRs), in the
context of testing of autonomous driving systems (ADSs). The
oracle problem is a major challenge in testing such systems,
where it is difficult to determine whether or not the output of
a system is correct. Metamorphic testing (MT) can alleviate this
problem by checking the consistency of the system’s outputs
under various transformations. However, manual generation
of MRs is often a time-consuming and error-prone process.
Automated MR generation can yield several benefits, including
enhanced efficiency, quality, coverage, scalability, and reusability
in software testing, thereby facilitating a more comprehensive
and effective testing process. In this paper, we investigate the
effectiveness of using ChatGPT for automatic generation of MRs
for ADSs. We provide a detailed methodology for generating
MRs using ChatGPT and evaluate the generated MRs using our
domain knowledge and existing MRs. The results of our study
indicate that our proposed approach is effective at generating
high-quality MRs, and can significantly reduce the manual
effort required for MR generation. Furthermore, we discuss the
practical implications and limitations of using ChatGPT for MR
generation and provide recommendations for future research.
Our study contributes to the advancement of automated testing
of ADSs, which is crucial for ensuring their safety and reliability
in real-world scenarios.";Oracle Generation;LLM-Pure-Prompting;Other;GPT-Family;Other;--
P90;Automated Metamorphic-Relation Generation with ChatGPT: An Experience Report;2023;Yifan2023;C: Other;Conference;"
@Inproceedings{Yifan2023 ,
  author =       ""Zhang, Yifan and Towey, Dave and Pike, Matthew"",
  title =        ""Automated Metamorphic-Relation Generation with ChatGPT: An Experience Report"",
  booktitle =    ""Proceedings of the IEEE 47th Annual Computers, Software, and Applications Conference (COMPSAC)"",
  series =       ""COMPSAC'23"",
  editor =       """",
  volume =       """",
  year =         ""2023"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Torino, Italy"",
  pages =        ""1780--1785"",
  doi =          ""https://doi.org/10.1109/COMPSAC57700.2023.00275"",
  url =          ""10.1109/COMPSAC57700.2023.00275""
}";Evaluation;"This paper reports on a pilot study of using ChatGPT, a language model based on GPT-3.5 architecture, for
automatic generation of metamorphic relations (MRs), in the
context of testing of autonomous driving systems (ADSs). The
oracle problem is a major challenge in testing such systems,
where it is difficult to determine whether or not the output of
a system is correct. Metamorphic testing (MT) can alleviate this
problem by checking the consistency of the system’s outputs
under various transformations. However, manual generation
of MRs is often a time-consuming and error-prone process.
Automated MR generation can yield several benefits, including
enhanced efficiency, quality, coverage, scalability, and reusability
in software testing, thereby facilitating a more comprehensive
and effective testing process. In this paper, we investigate the
effectiveness of using ChatGPT for automatic generation of MRs
for ADSs. We provide a detailed methodology for generating
MRs using ChatGPT and evaluate the generated MRs using our
domain knowledge and existing MRs. The results of our study
indicate that our proposed approach is effective at generating
high-quality MRs, and can significantly reduce the manual
effort required for MR generation. Furthermore, we discuss the
practical implications and limitations of using ChatGPT for MR
generation and provide recommendations for future research.
Our study contributes to the advancement of automated testing
of ADSs, which is crucial for ensuring their safety and reliability
in real-world scenarios.";Oracle Generation;LLM-Pure-Prompting;;GPT-Family;;
P91;Towards Generating Executable Metamorphic Relations Using Large Language Models;2024;Seung2024;C: QUATIC;Conference;"@Inproceedings{Seung2024 ,
  author =       ""Shin, Seung Yeob and Pastore, Fabrizio and Bianculli, Domenico and Baicoianu, Alexandra"",
  title =        ""Towards Generating Executable Metamorphic Relations Using Large Language Models"",
  booktitle =    ""Quality of Information and Communications Technology"",
  series =       ""QUATIC'24"",
  editor =       ""Bertolino, Antonia and Pascoal Faria, Jo{\~a}o and Lago, Patricia and Semini, Laura"",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Springer Nature Switzerland"",
  address =      ""Pisa, Italy"",
  pages =        ""126--141"",
  doi =          ""https://doi.org/10.1007/978-3-031-70245-7_9"",
  url =          ""10.1007/978-3-031-70245-7_9""
}";Research Contribution;"Metamorphic testing (MT) has proven to be a successful solution to automating testing and addressing the oracle problem. However, it entails manually deriving metamorphic relations (MRs) and converting them into an executable form; these steps are time-consuming and may prevent the adoption of MT. In this paper, we propose an approach for automatically deriving executable MRs (EMRs) from requirements using large language models (LLMs). Instead of merely asking the LLM to produce EMRs, our approach relies on a few-shot prompting strategy to instruct the LLM to perform activities in the MT process, by providing requirements and API specifications, as one would do with software engineers. To assess the feasibility of our approach, we conducted a questionnaire-based survey in collaboration with Siemens Industry Software, a worldwide leader in providing industry software and services, focusing on four of their software applications. Additionally, we evaluated the accuracy of the generated EMRs for a Web application. The outcomes of our study are highly promising, as they demonstrate the capability of our approach to generate MRs and EMRs that are both comprehensible and pertinent for testing purposes.";Oracle Generation;LLM-Pure-Prompting;Other;GPT-Family;Other;
P92;MR-Adopt: Automatic Deduction of Input Transformation Function for Metamorphic Testing;2024;Congying2024;C: ASE;Conference;"@Inproceedings{Congying2024 ,
  author =       ""Xu, Congying and Chen, Songqiang and Wu, Jiarong and Cheung, Shing-Chi and Terragni, Valerio and Zhu, Hengcheng and Cao, Jialun"",
  title =        ""MR-Adopt: Automatic Deduction of Input Transformation Function for Metamorphic Testing"",
  booktitle =    ""Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering"",
  series =       ""ASE'24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Sacramento, CA, USA"",
  pages =        ""557–-569"",
  doi =          ""https://doi.org/10.1145/3691620.3696020"",
  url =          ""10.1145/3691620.3696020""
}
";Research Contribution;While a recent study reveals that many developer-written test cases can encode a reusable Metamorphic Relation (MR), over 70% of them directly hard-code the source input and follow-up input in the encoded relation. Such encoded MRs, which do not contain an explicit input transformation to transform the source inputs to corresponding follow-up inputs, cannot be reused with new source inputs to enhance test adequacy. In this paper, we propose MR-Adopt (Automatic Deduction Of inPut Transformation) to automatically deduce the input transformation from the hard-coded source and follow-up inputs, aiming to enable the encoded MRs to be reused with new source inputs. With typically only one pair of source and follow-up inputs available in an MR-encoded test case as the example, we leveraged LLMs to understand the intention of the test case and generate additional examples of source-followup input pairs. This helps to guide the generation of input transformations generalizable to multiple source inputs. Besides, to mitigate the issue that LLMs generate erroneous code, we refine LLM-generated transformations by removing MRirrelevant code elements with data-flow analysis. Finally, we assess candidate transformations based on encoded output relations and select the best transformation as the result. Evaluation results show that MR-Adopt can generate input transformations applicable to all experimental source inputs for 72.00% of encoded MRs, which is 33.33% more than using vanilla GPT-3.5. By incorporating MRAdopt-generated input transformations, encoded MR-based test cases can effectively enhance the test adequacy, increasing the line coverage and mutation score by 10.62% and 18.91%, respectively;Oracle Generation;LLM-Pure-Prompting;Other;DeepSeekCoder, CodeQwen, GPT-Family, Llama-Family;Other, Test Coverage, Mutation Rate;MR-Adopt
P93;Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?;2024;Madeline2024;C: FSE;Journal;"@article{Madeline2024,
  title = ""Can Large Language Models Transform Natural Language Intent into Formal Method Postconditions?"",
  author = ""Endres, Madeline and Fakhoury, Sarah and Chakraborty, Saikat and Lahiri, Shuvendu K."",
  journal = ""Proc. ACM Softw. Eng."",
  volume = ""1"",
  number = ""FSE"",
  pages = ""24"",
  year = ""2024"",
  publisher = ""Association for Computing Machinery"",
  doi = ""10.1145/3660791"",
  url = ""https://doi.org/10.1145/3660791""
}
";Research Contribution;"nformal natural language that describes code functionality, such as code comments or function documentation, may contain substantial information about a program’s intent. However, there is typically no guarantee that a program’s implementation and natural language documentation are aligned. In the case of a conflict, leveraging information in code-adjacent natural language has the potential to enhance fault localization, debugging, and code trustworthiness. In practice, however, this information is often underutilized due to the inherent ambiguity of natural language which makes natural language intent challenging to check programmatically. The “emergent abilities” of Large Language Models (LLMs) have the potential to facilitate the translation of natural language intent to programmatically checkable assertions. However, it is unclear if LLMs can correctly translate informal natural language specifications into formal specifications that match programmer intent. Additionally, it is unclear if such translation could be useful in practice. In this paper, we describe nl2postcond, the problem of leveraging LLMs for transforming informal natural language to formal method postconditions, expressed as program assertions. We introduce and validate metrics to measure and compare different nl2postcond approaches, using the correctness and discriminative power of generated postconditions. We then use qualitative and quantitative methods to assess the quality of nl2postcond postconditions, finding that they are generally correct and able to discriminate incorrect code. Finally, we find that nl2postcond via LLMs has the potential to be helpful in practice; nl2postcond generated postconditions were able to catch 64 real-world historical bugs from Defects4J.";Oracle Generation;LLM-Pure-Prompting;HumanEval, Defects4J;StarChat, GPT-Family;Other;nl2postcond
P96;Adversarial generation method for smart contract fuzz testing seeds guided by chain-based LLM;2024;Sun2024;J: Other;Journal;"@article{Sun2024,
  title = ""Adversarial generation method for smart contract fuzz testing seeds guided by chain-based LLM"",
  author = ""Sun, Jiaze
and Yin, Zhiqiang
and Zhang, Hengshan
and Chen, Xiang
and Zheng, Wei"",
  journal = ""Automated Software Engineering"",
  volume = ""32"",
  number = ""1"",
  pages = ""12"",
  year = ""2024"",
  publisher = ""Springer Nature"",
  doi = ""10.1007/s10515-024-00483-4"",
  url = ""https://doi.org/10.1007/s10515-024-00483-4""
}";Research Contribution;"With the rapid development of smart contract technology and the continuous expansion
of blockchain application scenarios, the security issues of smart contracts have
garnered significant attention. However, traditional fuzz testing typically relies on
randomly generated initial seed sets. This random generation method fails to understand
the semantics of smart contracts, resulting in insufficient seed coverage. Additionally,
traditional fuzz testing often ignores the syntax and semantic constraints
within smart contracts, leading to the generation of seeds that may not conform to
the syntactic rules of the contracts and may even include logic that violates contract
semantics, thereby reducing the efficiency of fuzz testing. To address these
challenges, we propose a method for adversarial generation for smart contract fuzz
testing seeds guided by Chain-Based LLM, leveraging the deep semantic understanding
capabilities of LLM to assist in seed set generation. Firstly, we propose
a method that utilizes Chain-Based prompts to request LLM to generate fuzz testing
seeds, breaking down the LLM tasks into multiple steps to gradually guide the
LLM in generating high-coverage seed sets. Secondly, by establishing adversarial
roles for the LLM, we guide the LLM to autonomously generate and optimize seed
sets, producing high-coverage initial seed sets for the program under test. To evaluate
the effectiveness of the proposed method, 2308 smart contracts were crawled
from Etherscan for experimental purposes. Results indicate that using Chain-Based
prompts to request LLM to generate fuzz testing seed sets improved instruction coverage
by 2.94% compared to single-step requests. The method of generating seed
sets by establishing adversarial roles for the LLM reduced the time to reach maximum
instruction coverage from 60 s to approximately 30 s compared to single-role
methods. Additionally, the seed sets generated by the proposed method can directly
trigger simple types of vulnerabilities (e.g., timestamp dependency and block number
dependency vulnerabilities), with instruction coverage improvements of 3.8%
and 4.1%, respectively.";;Hybrid-Prompting;Other;GPT-Family;Test Coverage, F1, Time, Other;LLM-Chain
P98;AI-Powered Multi-Agent Framework for Automated Unit Test Case Generation: Enhancing Software Quality through LLM’s;2024;Garlapati2024;C: Other;Conference;"@Inproceedings{Garlapati2024 ,
  author =       ""Garlapati, Anusha and Satya Sai Muni Parmesh, M N V and Savitha and S, Jaisri"",
  title =        ""AI-Powered Multi-Agent Framework for Automated Unit Test Case Generation: Enhancing Software Quality through LLM’s"",
  booktitle =    ""2024 5th IEEE Global Conference for Advancement in Technology (GCAT)"",
  series =       ""GCAT '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Bangalore, India"",
  pages =        ""1--5"",
  doi =          ""10.1109/GCAT62922.2024.10923987"",
  url =          ""https://doi.org/10.1109/GCAT62922.2024.10923987""
}";Research Contribution;"Recent years have witnessed an enormous rise in
the design, repair and the enhancement of software automation
tests. The reliability of program’s unit testing has major impact
on its overall performance. The anticipated influence of
Artificial Intelligence advancements on test automation
methodologies are significant. Many studies on automated
testing implicitly assume that the test results are deterministic,
means that similar tests faults remain same. The precision of
software is largely ensured by unit testing. But writing unit tests
manually is a time-consuming process, which leads us to drive
into “Automation Analysis”. Recent years comprised the
application of Large Language Models (LLM’s) in numerous
fields related to software development, especially the automated
creation of unit testing.
However, these frameworks require more instructions, or
few shot learnings on sample tests that already exist. This
research provides a comprehensive empirical assessment of the
efficiency of LLM’s for automating unit testing production, with
no need for further manual analysis. The method we employ is
put into practice for test cases, an adaptable Agents and LLMbased
testing framework that evaluates test cases generated, by
reviewing and re-writing them in different phases. Evaluation of
this test cases was done by using mistral-large LLM Model. The
analysis results that developed acquired an overall coverage of
100% for code given. Finally, to enhance the typical evaluation,
this research suggests and concludes that LLMs, can be
successfully incorporated into present practices, through
adaptative instructions and improvements.";Unit Test Generation, Test Agents;LLM-Pure-Prompting;No Bmk-Ds;N/S;No Eval.;
P99;Automated Structural Test Case Generation for Human-Computer Interaction Software Based on Large Language Model;2024;Kang2024;C: Other;Conference;"@Inproceedings{Kang2024 ,
  author =       ""Kang, Long and Ai, Jun and Lu, Minyan"",
  title =        ""Automated Structural Test Case Generation for Human-Computer Interaction Software Based on Large Language Model"",
  booktitle =    ""2024 11th International Conference on Dependable Systems and Their Applications (DSA)"",
  series =       ""DSA '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Taicang, Suzhou, China"",
  pages =        ""132--140"",
  doi =          ""10.1109/DSA63982.2024.00027"",
  url =          ""https://doi.org/10.1109/DSA63982.2024.00027""
}";Research Contribution;"As software systems expand in complexity, managing
the vast and varied collection of test cases becomes
increasingly difficult with traditional manual testing methods.
This paper presents a new approach for automating the generation
of structured test cases, named Test Element Extraction
and Restructuring (TEER), which leverages the advanced natural
language processing capabilities of large language models
(LLMs). Specifically targeting human-computer interaction
(HCI) software, TEER employs prompt tuning techniques to
extract critical elements from natural language test cases and
systematically reassemble them into structured formats. The
study evaluates the effectiveness of TEER by applying it
to common test cases from desktop HCI applications. The
experimental results demonstrate that this method successfully
produces structured test cases that meet predefined requirements.";High-Level Test Gen;LLM-Pure-FineTune;;ChatGLM;BLEU, ROUGE;
P100;Automated Test Case Generation for Satellite FRD Using NLP and Large Language Model;2024;Shakthi2024;C: Other;Conference;"@Inproceedings{Shakthi2024 ,
  author =       ""S, Shakthi and Srivastava, Pratibha and L, Ravi Kumar and Prasad, SG"",
  title =        ""Automated Test Case Generation for Satellite FRD Using NLP and Large Language Model"",
  booktitle =    ""4th International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)"",
  series =       ""ICECCME '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Male, Maldives"",
  pages =        ""1--8"",
  doi =          ""10.1109/ICECCME62383.2024.10796866"",
  url =          ""https://doi.org/10.1109/ICECCME62383.2024.10796866""
}";Research Contribution;"In recent times, the research on the use of Large Language Models (LLMs) for developing software applications has grown exponentially. Generating test cases for satellite Functional Requirement Documents (FRDs) pose a significant challenge due to their complex nature, requiring intricate analysis.
Manual methods are time-consuming and error-prone, prompting the need for automated solutions or semi-automated solutions. This work proposes a novel approach to automate test case generation from FRDs using LLMs and Natural Language Processing (NLP). By harnessing the capabilities of LLMs, our system extracts and interprets complex variables and equations, facilitating the automated creation of comprehensive test cases.
This approach aims to streamline the satellite testing process, improving efficiency and accuracy while reducing the burden on human analysts. We generate a custom dataset of 10 samples and then benchmark 4 LLMs on the dataset. We open-source the complete codebase for implementation and for further research.";High-Level Test Gen;LLM-Pure-Prompting;Other;Gemini-Pro, Mistral, GPT-Family, Llama-Family;% Test Correct;data-set of the study
P102;Comparing the Adaptability of a Genetic Algorithm and an LLM-Based Framework for Automated Software Test Data Generation: In the Context of Web Applications;2024;Wanigasekara2024;C: ICDDS;Conference;"@Inproceedings{Wanigasekara2024 ,
  author =       ""Wanigasekara, Sashini and Asanka, Dinesh and Rajapakse, Chathura and Wickramaarachchi, Dilani and Wijesinghe, Abhiru"",
  title =        ""Comparing the Adaptability of a Genetic Algorithm and an LLM-Based Framework for Automated Software Test Data Generation: In the Context of Web Applications"",
  booktitle =    ""IEEE 3rd International Conference on Data, Decision and Systems (ICDDS)"",
  series =       ""ICDDS '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Bangalore, India"",
  pages =        ""1--6"",
  doi =          ""10.1109/ICDDS62937.2024.10910287"",
  url =          ""https://doi.org/10.1109/ICDDS62937.2024.10910287""
}";Evaluation;"In the fast-paced world of software development,
ensuring software quality is paramount. Software Quality
Assurance (SQA) plays a vital role, primarily through testing,
which can be carried out manually or automatically. Yet,
creating comprehensive test data (TD) for web applications can
be a formidable task. Manual test data generation (TDG) is
time-consuming and error-prone. Automation of TDG has
become increasingly important in software quality assurance as
it enables efficient and effective testing of software systems. An
appropriate framework for automated TDG is critical to
achieve comprehensive and reliable test coverage. Automated
TDG offers significant advantages, including time and resource
savings, improved test coverage, and seamless integration into
the software development process. The core aim of this research
is to bridge the gap between manual and existing automated
methods, resulting in time and cost savings, heightened testing
efficiency, and elevated software quality. The study conducts
experiments on the triangle classification program with two
automated TDG models, an Adaptive Genetic Algorithm (AGA)
and a Large Language Model (LLM). The LLM model
demonstrated superior accuracy and performance over the
AGA, testing the triangle classification program. Further, it
compares the adaptability of these two models for web
applications. Additionally, this paper examines the challenges
associated with implementing the AGA-based framework and
how the LLM-based approach effectively overcomes these
hurdles, providing valuable insights into its practical benefits
for improving SQA practices.";Unit Test Generation;LLM-Pure-Prompting;No Bmk-Ds;N/S;% Pass;
P103;Comprehensive Evaluation and Insights Into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation;2024;Karpurapu2024;J: Other;Journal;"@article{Karpurapu2024,
  title = ""Comprehensive Evaluation and Insights Into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation"",
  author = ""Karpurapu, Shanthi and Myneni, Sravanthy and Nettur, Unnati and Gajja, Likhit Sagar and Burke, Dave and Stiehm, Tom and Payne, Jeffery"",
  journal = ""IEEE Access"",
  volume = ""12"",
  number = """",
  pages = ""58715--58721"",
  year = ""2024"",
  publisher = ""IEEE Computer Society"",
  doi = ""10.1109/ACCESS.2024.3391815"",
  url = ""https://doi.org/10.1109/ACCESS.2024.3391815""
}";Evaluation;"Behavior-driven development (BDD) is an Agile testing methodology fostering collaboration
among developers, QA analysts, and stakeholders. In this manuscript, we propose a novel approach to
enhance BDD practices using large language models (LLMs) to automate acceptance test generation. Our study uses zero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B, and PaLM-2. The paper presents a detailed methodology that includes the dataset, prompt techniques, LLMs, and the evaluation process. The results demonstrate that GPT-3.5 and GPT-4 generate error-free BDD acceptance tests with better performance. The few-shot prompt technique highlights its ability to provide higher accuracy by incorporating examples for in-context learning. Furthermore, the study examines syntax errors, validation accuracy, and comparative analysis of LLMs, revealing their effectiveness in enhancing BDD practices.
However, our study acknowledges that there are limitations to the proposed approach. We emphasize that this approach can support collaborative BDD processes and create opportunities for future research into automated BDD acceptance test generation using LLMs.";High-Level Test Gen;LLM-Pure-Prompting;Dalpiaz User Stories Dataset;PaLM-2, GPT-Family, Llama-Family;% Test Syntax Error;
P105;DrWASI: LLM-assisted Differential Testing for WebAssembly System Interface Implementations;2025;Zhang2025;J: TOSEM;Journal;"@article{Zhang2025,
  title = ""DrWASI: LLM-assisted Differential Testing for WebAssembly System Interface Implementations"",
  author = ""Zhang, Yixuan and He, Ningyu and Gao, Jianting and Cao, Shangtong and Liu, Kaibo and Wang, Haoyu and Ma, Yun and Huang, Gang and Liu, Xuanzhe"",
  journal = ""ACM Trans. Softw. Eng. Methodol."",
  volume = """",
  number = """",
  pages = """",
  year = ""2025"",
  publisher = ""Association for Computing Machinery"",
  doi = ""10.1145/3716379"",
  url = ""https://doi.org/10.1145/3716379""
}";Research Contribution;"WebAssembly (Wasm) is an emerging binary format that serves as a compilation target for over 40 programming languages. Wasm runtimes provide execution environments that enhance portability by abstracting away operating systems and hardware
details. A key component in these runtimes is the WebAssembly System Interface (WASI), which manages interactions with
operating systems, like file operations. Considering the critical role of Wasm runtimes, the community has aimed to detect
their implementation bugs. However, no work has focused on WASI-specific bugs that can affect the original functionalities
of running Wasm binaries and cause unexpected results. To fill the void, we present DrWASI, the first general-purpose
differential testing framework for WASI implementations. Our approach uses a large language model to generate seeds and applies variant and environment mutation strategies to expand and enrich the test case corpus. We then perform differential
testing across major Wasm runtimes. By leveraging dynamic and static information collected during and after the execution,
DrWASI can identify bugs. Our evaluation shows that DrWASI uncovered 33 unique bugs, with all confirmed and 7 fixed by developers. This research represents a pioneering step in exploring a promising yet under-explored area of the Wasm
ecosystem, providing valuable insights for stakeholders.";High-Level Test Gen;LLM-Pure-Prompting;;GPT-Family;Bugs Rate, Bugs Rate;DrWASI
P106;Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat;2024;Feng2024;C: ASE;Conference;"@Inproceedings{Feng2024 ,
  author =       ""Feng, Sidong and Lu, Haochuan and Jiang, Jianqin and Xiong, Ting and Huang, Likun and Liang, Yinglin and Li, Xiaoqin and Deng, Yuetang and Aleti, Aldeida"",
  title =        ""Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs: A Case Study in WeChat"",
  booktitle =    ""Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering"",
  series =       ""ASE '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Sacramento, CA, USA"",
  pages =        ""1973–-1978"",
  doi =          ""10.1145/3691620.3695260"",
  url =          ""https://doi.org/10.1145/3691620.3695260""
}";Research Contribution;"UI automation tests play a crucial role in ensuring the quality of mobile
applications. Despite the growing popularity of machine learning
techniques to generate these tests, they still face several challenges,
such as the mismatch of UI elements. The recent advances
in Large Language Models (LLMs) have addressed these issues
by leveraging their semantic understanding capabilities. However,
a significant gap remains in applying these models to industriallevel
app testing, particularly in terms of cost optimization and
knowledge limitation. To address this, we introduce CAT to create
cost-effective UI automation tests for industry apps by combining
machine learning and LLMs with best practices. Given the task
description, CAT employs Retrieval Augmented Generation (RAG)
to source examples of industrial app usage as the few-shot learning
context, assisting LLMs in generating the specific sequence
of actions. CAT then employs machine learning techniques, with
LLMs serving as a complementary optimizer, to map the target
element on the UI screen. Our evaluations on the WeChat testing
dataset demonstrate the CAT’s performance and cost-effectiveness,
achieving 90% UI automation with $0.34 cost, outperforming the
state-of-the-art.We have also integrated our approach into the realworld
WeChat testing platform, demonstrating its usefulness in
detecting 141 bugs and enhancing the developers’ testing process.";High-Level Test Gen;LLM-Pure-Prompting;Custom;GPT-Family, Llama-Family;test cost, Execution Time;
P107;Enhancing Exploratory Testing by Large Language Model and Knowledge Graph;2024;Su2024;C: ICSE;Conference;"@Inproceedings{Su2024 ,
  author =       ""Su, Yanqi and Liao, Dianshu and Xing, Zhenchang and Huang, Qing and Xie, Mulong and Lu, Qinghua and Xu, Xiwei"",
  title =        ""Enhancing Exploratory Testing by Large Language Model and Knowledge Graph"",
  booktitle =    ""Proceedings of the IEEE/ACM 46th International Conference on Software Engineering"",
  series =       ""ICSE '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Lisbon, Portugal"",
  pages =        ""12"",
  doi =          ""10.1145/3597503.3639157"",
  url =          ""https://doi.org/10.1145/3597503.3639157""
}";Research Contribution;"Exploratory testing leverages the tester’s knowledge and creativity
to design test cases for effectively uncovering system-level bugs
from the end user’s perspective. Researchers have worked on test
scenario generation to support exploratory testing based on a system
knowledge graph, enriched with scenario and oracle knowledge
from bug reports. Nevertheless, the adoption of this approach is
hindered by difficulties in handling bug reports of inconsistent quality
and varied expression styles, along with the infeasibility of the
generated test scenarios. To overcome these limitations, we utilize
the superior natural language understanding (NLU) capabilities of
Large Language Models (LLMs) to construct a System KG of User
Tasks and Failures (SysKG-UTF). Leveraging the system and bug
knowledge from the KG, along with the logical reasoning capabilities
of LLMs, we generate test scenarios with high feasibility and
coherence. Particularly, we design chain-of-thought (CoT) reasoning
to extract human-like knowledge and logical reasoning from
LLMs, simulating a developer’s process of validating test scenario
feasibility. Our evaluation shows that our approach significantly
enhances the KG construction, particularly for bug reports with
low quality. Furthermore, our approach generates test scenarios
with high feasibility and coherence. The user study further proves
the effectiveness of our generated test scenarios in supporting exploratory
testing. Specifically, 8 participants find 36 bugs from 8 seed bugs in two hours using our test scenarios, a significant improvement
over the 21 bugs found by the state-of-the-art baseline.";High-Level Test Gen;LLM-Pure-Prompting;Bugzilla;N/S;Bugs Rate;
P108;Evaluation of Large Language Models for Unit Test Generation;2024;Konuk2024;C: Other;Conference;"@Inproceedings{Konuk2024 ,
  author =       ""Konuk, Metin and Baglum, Cem and Yayan, Ugur"",
  title =        ""Evaluation of Large Language Models for Unit Test Generation"",
  booktitle =    ""2024 Innovations in Intelligent Systems and Applications Conference (ASYU)"",
  series =       ""ASYU '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Ankara, Turkiye"",
  pages =        ""1--6"",
  doi =          ""10.1109/ASYU62119.2024.10756954"",
  url =          ""https://doi.org/10.1109/ASYU62119.2024.10756954""
}";Research Contribution;"In recent years, Artificial Intelligence (AI) has
significantly transformed various industries, especially software
development, through automation and enhanced decisionmaking
processes. Traditional software testing, often manual
and error-prone, cannot keep up with rapid development cycles
and complex systems, leading to extended development times,
higher costs, and undetected bugs. This study develops an AIbased
platform using OpenAI models to generate and execute
unit tests across multiple programming languages. By
leveraging Large Language Models (LLMs) like GPT, we
automate unit test creation, demonstrating proficiency in
understanding and generating natural language to interpret
code. Our web-based system architecture ensures efficient test
generation and execution, significantly reducing manual effort
and mitigating human error, thus revolutionizing software
testing. Furthermore, we introduce unique evaluation metrics
such as ""Is Executable"" and ""Assertion Count"" to assess the
performance and effectiveness of the generated unit tests,
providing a comprehensive measure of the models' capabilities.";Unit Test Generation;LLM-Pure-Prompting;No Bmk-Ds;GPT-Family;Other, % Build, Execution Time;
P110;Fix the Tests: Augmenting LLMs to Repair Test Cases with Static Collector and Neural Reranker;2024;LiuISSRE2024;C: ISSRE;Conference;"@Inproceedings{LiuISSRE2024 ,
  author =       ""Liu, Jun and Yan, Jiwei and Xie, Yuanyuan and Yan, Jun and Zhang, Jian"",
  title =        ""Fix the Tests: Augmenting LLMs to Repair Test Cases with Static Collector and Neural Reranker"",
  booktitle =    ""2024 IEEE 35th International Symposium on Software Reliability Engineering (ISSRE) "",
  series =       ""ISSRE 2024"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Tsukuba, Japan"",
  pages =        ""367--378"",
  doi =          ""10.1109/ISSRE62328.2024.00043"",
  url =          ""https://doi.org/10.1109/ISSRE62328.2024.00043""
}";Research Contribution;"During software evolution, it is advocated that test
code should co-evolve with production code. In real development
scenarios, test updating may lag behind production code
changing, which may cause the project to fail to compile or
bring other troubles. Existing techniques based on pre-trained
language models can be adopted to repair obsolete tests caused
by such unsynchronized code changes, especially syntactic-related
ones. However, the lack of target-oriented contextual information
affects repair accuracy on large-scale projects. Starting from an
obsoleted test, the key challenging task is precisely identifying
and constructing Test-Repair-Oriented Contexts (TROCtx) from
the whole repository within a limited token size.
In this paper, we propose SYNBCIATR (Syntactic-Breaking-
Change-Induced Automated Test Repair), a novel approach to
automatically repair obsolete test cases via precise and concise
TROCtx construction. Inspired by developers’ programming
practices of the task, we design three types of TROCtx: class contexts,
usage contexts, and environment contexts. For every type of
TROCtx, SYNBCIATR automatically collects the changed-tokenrelated
code information through static analysis techniques. Then
it generates reranking queries to identify the most relevant
TROCtxs, which will be taken as the repair-required key context
and be input to Large Language Model for the final test repair.
To evaluate the effectiveness of SYNBCIATR, we construct
a benchmark dataset that contains diverse syntactic breaking
changes. The experimental results show that SYNBCIATR outperforms
baseline approaches both on textual- and intent-matching
metrics. With the augmentation of TROCtx constructed by
SYNBCIATR, hallucinations are reduced by 57.1%.";Test Augmentation or Improvement;Hybrid-Prompting;Other;GPT-Family;CodeBLEU, diffBleu, Accuracy, % Test Syntax Error;
P111;Getting pwn’d by AI: Penetration Testing with Large Language Models;2023;Happe2023;C: FSE;Conference;"@Inproceedings{Happe2023 ,
  author =       ""Happe, Andreas and Cito, Jurgen"",
  title =        ""Getting pwn’d by AI: Penetration Testing with Large Language Models"",
  booktitle =    ""Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering"",
  series =       ""ESEC/FSE 2023"",
  editor =       """",
  volume =       """",
  year =         ""2023"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""San Francisco, CA, USA"",
  pages =        ""2082-–2086"",
  doi =          ""10.1145/3611643.3613083"",
  url =          ""https://doi.org/10.1145/3611643.3613083""
}
";Research Contribution;"The feld of software security testing, more specifically penetration
testing, requires high levels of expertise and involves many manual
testing and analysis steps. This paper explores the potential use of
large-language models, such as GPT3.5, to augment penetration
testers with AI sparring partners.We explore two distinct use cases:
high-level task planning for security testing assignments and lowlevel
vulnerability hunting within a vulnerable virtual machine.
For the latter, we implemented a closed-feedback loop between
LLM-generated low-level actions with a vulnerable virtual machine
(connected through SSH) and allowed the LLM to analyze the machine
state for vulnerabilities and suggest concrete attack vectors
which were automatically executed within the virtual machine. We
discuss promising initial results, detail avenues for improvement,
and close deliberating on the ethics of AI sparring partners.";Non Functional Testing;LLM-Pure-Prompting;No Bmk-Ds;GPT-Family;No Eval.;
P112;HITS: High-coverage LLM-based Unit Test Generation via Method Slicing;2024;Wang2024;C: ASE;Conference;"@Inproceedings{Wang2024 ,
  author =       ""Wang, Zejun and Liu, Kaibo and Li, Ge and Jin, Zhi"",
  title =        ""HITS: High-coverage LLM-based Unit Test Generation via Method Slicing"",
  booktitle =    ""Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering"",
  series =       ""ASE '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Sacramento, CA, USA"",
  pages =        ""1258–-1268"",
  doi =          ""10.1145/3691620.3695501"",
  url =          ""https://doi.org/10.1145/3691620.3695501""
}";Research Contribution;"Large language models (LLMs) have behaved well in generating
unit tests for Java projects. However, the performance for covering
the complex focal methods within the projects is poor. Complex
methods comprise many conditions and loops, requiring the test
cases to be various enough to cover all lines and branches. However,
existing test generation methods with LLMs provide the whole
method-to-test to the LLM without assistance on input analysis. The
LLM has difficulty inferring the test inputs to cover all conditions,
resulting in missing lines and branches. To tackle the problem, we
propose decomposing the focal methods into slices and asking the
LLM to generate test cases slice by slice. Our method simplifies the
analysis scope, making it easier for the LLM to cover more lines
and branches in each slice. We build a dataset comprising complex
focal methods collected from the projects used by existing state-ofthe-
art approaches. Our experiment results show that our method
significantly outperforms current test case generation methods
with LLMs and the typical SBST method Evosuite regarding both
line and branch coverage scores.";Unit Test Generation;LLM-Pure-Prompting;ChatUniTest, EvoSuite;GPT-Family;Test Coverage;
P113;Increasing Test Coverage by Automating BDD Tests in Proofs of Concepts (POCs) using LLM;2024;SantosSBQS2024;C: Other;Conference;"@Inproceedings{SantosSBQS2024 ,
  author =       ""Santos, Shexmo Richarlison Ribeiro dos and Fernandes, Raiane Eunice S. and Santos, Marcos Cesar Barbosa dos and Soares, Michel S. and Rocha, Fabio Gomes and Marczak, Sabrina"",
  title =        ""Increasing Test Coverage by Automating BDD Tests in Proofs of Concepts (POCs) using LLM"",
  booktitle =    ""Proceedings of the XXIII Brazilian Symposium on Software Quality"",
  series =       ""SBQS '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Salvador Bahia,Brazil "",
  pages =        ""519-–525"",
  doi =          ""10.1145/3701625.3701637"",
  url =          ""https://doi.org/10.1145/3701625.3701637""
}";Research Contribution;"In today’s landscape, software manufacturers must deliver quickly
and with high quality to remain competitive, especially in the cybersecurity
sector. Recognizing this need, we have recently implemented
several strategies to accelerate our time to market without
compromising quality. We introduced the Proof of Concept (POC)
and Proof of Value (POV) stages in the Enterprise Architecture team
before initiating inception processes for Minimum Viable Products
(MVP) and new features. Initially, these proofs focused only on
concept validation. However, since 2023, due to the growing need
for rapid and high-quality innovation, POCs/POVs have begun to
include robust implementations. We adopted the Behaviour-Driven
Development (BDD) approach to define user stories and acceptance
criteria, which provided a solid evaluation of POC/POV quality
and involved the implementation team from the outset. To prevent
products from incurring technical debt, we implemented AutoDevSuite,
which uses LLM to generate tests based on user stories
and acceptance criteria automatically. We used AutoDevSuite in
a POC/POV of a cybersecurity product, and the results showed a
significant expansion in test coverage, aligned with the acceptance
criteria, demonstrating the tool’s effectiveness in automating and
improving test quality.";High-Level Test Gen;Hybrid-Prompting;Custom;Gemini;Test Coverage;AutoDevSuite
P114;Intent-Driven Mobile GUI Testing with Autonomous Large Language Model Agents;2024;Yoon2024;C: ICST;Conference;"@Inproceedings{Yoon2024 ,
  author =       ""Yoon, Juyeon and Feldt, Robert and Yoo, Shin"",
  title =        ""Intent-Driven Mobile GUI Testing with Autonomous Large Language Model Agents"",
  booktitle =    ""2024 IEEE Conference on Software Testing, Verification and Validation (ICST)"",
  series =       ""ICTSS '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Toronto, ON, Canada"",
  pages =        ""129--139"",
  doi =          ""10.1109/ICST60714.2024.00020"",
  url =          ""https://doi.org/10.1109/ICST60714.2024.00020""
}";Research Contribution;"GUI testing checks if a software system behaves
as expected when users interact with its graphical interface,
e.g., testing specific functionality or validating relevant use case
scenarios. Currently, deciding what to test at this high level is a
manual task since automated GUI testing tools target lower level
adequacy metrics such as structural code coverage or activity
coverage. We propose DROIDAGENT, an autonomous GUI testing
agent for Android, for semantic, intent-driven automation of
GUI testing. It is based on Large Language Models and support
mechanisms such as long- and short-term memory. Given an
Android app, DROIDAGENT sets relevant task goals and subsequently
tries to achieve them by interacting with the app. Our
empirical evaluation of DROIDAGENT using 15 apps from the
Themis benchmark shows that it can set up and perform realistic
tasks, with a higher level of autonomy. For example, when testing
a messaging app, DROIDAGENT created a second account and
added a first account as a friend, testing a realistic use case,
without human intervention. On average, DROIDAGENT achieved
61% activity coverage, compared to 51% for current state-ofthe-
art GUI testing techniques. Further, manual analysis shows
that 317 out of the 547 autonomously created tasks are realistic
and relevant to app functionalities, and also that DROIDAGENT
interacts deeply with the apps and covers more features.";High-Level Test Gen, Test Agents;LLM-Pure-Prompting;Themis, Other;GPT-Family;Activity Coverage, BS Coverage;DroidAgent
P115;KAT: Dependency-Aware Automated API Testing with Large Language Models;2024;Le2024;C: ICST;Conference;"@Inproceedings{Le2024 ,
  author =       ""Le, Tri and Tran, Thien and Cao, Duy and Le, Vy and Nguyen, Tien N. and Nguyen, Vu"",
  title =        ""KAT: Dependency-Aware Automated API Testing with Large Language Models"",
  booktitle =    ""2024 IEEE Conference on Software Testing, Verification and Validation (ICST)"",
  series =       ""ICTSS '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Toronto, ON, Canada"",
  pages =        ""82--92"",
  doi =          ""10.1109/ICST60714.2024.00017"",
  url =          ""https://doi.org/10.1109/ICST60714.2024.00017""
}";Research Contribution;"API testing has increasing demands for software
companies. Prior API testing tools were aware of certain types of
dependencies that needed to be concise between operations and
parameters. However, their approaches, which are mostly done
manually or using heuristic-based algorithms, have limitations
due to the complexity of these dependencies. In this paper, we
present KAT (Katalon API Testing), a novel AI-driven approach
that leverages the large language model GPT in conjunction
with advanced prompting techniques to autonomously generate
test cases to validate RESTful APIs. Our comprehensive strategy
encompasses various processes to construct an operation dependency
graph from an OpenAPI specification and to generate test
scripts, constraint validation scripts, test cases, and test data. Our
evaluation of KAT using 12 real-world RESTful services shows
that it can improve test coverage, detect more undocumented
status codes, and reduce false positives in these services in
comparison with a state-of-the-art automated test generation tool.
These results indicate the effectiveness of using the large language
model for generating test scripts and data for API testing.";High-Level Test Gen;LLM-Pure-Prompting;REST services;GPT-Family;Test Coverage, failure detection, Bugs Rate;
P116;Large Language Model-based Test Case Generation for GP Agents;2024;Jorgensen2024;C: Other;Conference;"@Inproceedings{Jorgensen2024 ,
  author =       ""Jorgensen, Steven and Nadizar, Giorgia and Pietropolli, Gloria and Manzoni, Luca and Medvet, Eric and O'Reilly, Una-May and Hemberg, Erik"",
  title =        ""Large Language Model-based Test Case Generation for GP Agents"",
  booktitle =    ""Proceedings of the Genetic and Evolutionary Computation Conference"",
  series =       ""GECCO '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Melbourne, VIC, Australia"",
  pages =        ""914–-923"",
  doi =          ""10.1145/3638529.3654056"",
  url =          ""https://doi.org/10.1145/3638529.3654056""
}

";Research Contribution;"Genetic programming (GP) is a popular problem-solving and optimization
technique. However, generating effective test cases for
training and evaluating GP programs requires strong domain knowledge.
Furthermore, GP programs often prematurely converge on
local optima when given excessively difficult problems early in
their training. Curriculum learning (CL) has been effective in addressing
similar issues across different reinforcement learning (RL)
domains, but it requires the manual generation of progressively difficult
test cases as well as their careful scheduling. In this work, we
leverage the domain knowledge and the strong generative abilities
of large language models (LLMs) to generate effective test cases
of increasing difficulties and schedule them according to various
curricula.We show that by integrating a curriculum scheduler with
LLM-generated test cases we can effectively train a GP agent player
with environments-based curricula for a single-player game and
opponent-based curricula for a multi-player game. Finally, we discuss
the benefits and challenges of implementing this method for
other problem domains.";Test Agents;Hybrid-Prompting;;GPT-Family;fitness function;
P117;Large Language Models are Few-Shot Testers: Exploring LLM-Based General Bug Reproduction;2023;Kang2023;C: ICSE;Conference;"@Inproceedings{Kang2023 ,
  author =       ""Kang, Sungmin and Yoon, Juyeon and Yoo, Shin"",
  title =        ""Large Language Models are Few-Shot Testers: Exploring LLM-Based General Bug Reproduction"",
  booktitle =    ""Proceedings of the 45th International Conference on Software Engineering"",
  series =       ""ICSE '23"",
  editor =       """",
  volume =       """",
  year =         ""2023"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Melbourne, VIC, Australia"",
  pages =        ""2312-–2323"",
  doi =          ""10.1109/ICSE48619.2023.00194"",
  url =          ""https://doi.org/10.1109/ICSE48619.2023.00194""
}";Research Contribution;"Many automated test generation techniques have
been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose LIBRO, a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks. Since LLMs themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when LLMs are effective, and rank the produced tests according to their validity. Our evaluation of LIBRO shows that, on the widely studied Defects4J benchmark, LIBRO can generate failure reproducing test cases for 33% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination (i.e., the possibility of the LLM simply remembering the test code either partially or in whole), we also evaluate LIBRO against 31 bug reports submitted after the collection of the LLM training data terminated: LIBRO produces bug reproducing tests for 32% of the studied bug reports. Overall, our results show LIBRO has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports";Unit Test Generation;LLM-Pure-Prompting;Defects4J, Custom;OpenAI Codex;Acc@x, wef;-
P119;Leveraging Large Language Models for Python Unit Test;2024;Jiri2024;C: AITEST;Conference;"@Inproceedings{Jiri2024 ,
  author =       ""Jiri, Medlen and Emese, Bari and Medlen, Patrick"",
  title =        ""Leveraging Large Language Models for Python Unit Test"",
  booktitle =    ""2024 IEEE International Conference on Artificial Intelligence Testing (AITest)"",
  series =       ""AITEST '24"",
  editor =       """",
  volume =       """",
  year =         ""2023"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Shanghai, China"",
  pages =        ""95--100"",
  doi =          ""10.1109/AITest62860.2024.00020"",
  url =          ""https://doi.org/10.1109/AITest62860.2024.00020""
}";Evaluation;"Generative Artificial Intelligence is becoming an
integral and enduring part of our lives, growing more powerful with each passing day. This paper explores Large Language Models and their application in text generation, specifically examining their potential to assist software quality assurance engineers in their daily tasks. Our focus is on the generation of unit tests as a critical component of software development. The research question is simple: Can Generative AI generate comprehensive unit tests? We started with Python and a very simple use case, and if Gen AI is successful, we will continue with complex tasks. Current literature focuses on success, but we are interested in failures as well. How many test cases are missing?";Unit Test Generation;LLM-Pure-Prompting;No Bmk-Ds;Claude 3, Gemini, GPT-Family;ML-metrics;-
P120;Leveraging Large Language Models for Usability Testing: a Preliminary Study;2025;Calvano2025;C: Other;Conference;"@Inproceedings{Calvano2025 ,
  author =       ""Calvano, Miriana and Curci, Antonio and Lanzilotti, Rosa and Piccinno, Antonio and Ragone, Azzurra"",
  title =        ""Leveraging Large Language Models for Usability Testing: a Preliminary Study"",
  booktitle =    ""Companion Proceedings of the 30th International Conference on Intelligent User Interfaces"",
  series =       ""IUI '25 Companion"",
  editor =       """",
  volume =       """",
  year =         ""2025"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Cagliari Italy"",
  pages =        ""78–-81"",
  doi =          ""10.1145/3708557.3716341"",
  url =          ""https://doi.org/10.1145/3708557.3716341""
}";Evaluation;"Despite growing efforts to prioritize user experience in product development, software organizations often perform little or no us- ability engineering activities. Therefore, it is crucial to develop strategies to integrate them effectively into software development processes. The rapid advances in Artificial Intelligence have signif- icantly influenced various aspects of daily life, particularly with the emergence of Large Language Models (LLMs), which can serve as promising tools to support activities to enhance the usability of software products. This paper presents a study investigating the potential of LLMs to assist practitioners in conducting usabil- ity tests. Specifically, we conducted an experiment where LLMs generate usability test tasks. Our goal is to assess whether AI can effectively support evaluators by comparing tasks generated by LLMs to those defined by usability experts. The findings indicate that while LLMs can provide valuable support, effective usability testing still requires human oversight and expert intervention.
CCS";Non Functional Testing;LLM-Pure-Prompting;Custom;Mistral, Gemini, GPT-Family;;-
P121;Leveraging Large Vision-Language Model for Better Automatic Web GUI Testing;2024;WangICSME2024;C: Other;Conference;"@Inproceedings{WangICSME2024 ,
  author =       ""Wang, Siyi and Wang, Sinan and Fan, Yujia and Li, Xiaolei and Liu, Yepang"",
  title =        ""Leveraging Large Vision-Language Model for Better Automatic Web GUI Testing"",
  booktitle =    ""2024 IEEE International Conference on Software Maintenance and Evolution (ICSME)"",
  series =       ""ICSME '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Flagstaff, AZ, USA"",
  pages =        ""125--137"",
  doi =          ""10.1109/ICSME58944.2024.00022"",
  url =          ""https://doi.org/10.1109/ICSME58944.2024.00022""
}";Research Contribution;"With the rapid development of web technology, more
and more software applications have become web-based in the past decades. To ensure software quality and user experience, various techniques have been proposed to automatically test web applications by interacting with their GUIs. To achieve high functional coverage, web GUI testing tools often need to generate high-quality text inputs and interact with the associated GUI elements (e.g., click submit buttons). However, developing a holistic approach that solves both subtasks is challenging because the web GUI context can be complicated and highly dynamic, which is hard to process programmatically. The recent development of large vision-language models (LVLM) provides new opportunities to handle these longstanding problems. We in this paper propose VETL, the first LVLM-driven end-to-end web testing technique. With LVLM’s scene understanding capabilities, VETL can generate valid and meaningful text inputs focusing on the local context, while avoiding the need to extract precise textual attributes. The selection of associated GUI elements is formulated as a visual question answering problem, allowing LVLM to capture the logical connection between the input box and the relevant element based on visual instructions. Further, the GUI exploration is guided by a multi-armed bandit module employing a curiosity-oriented strategy. Experiments show that VETL is effective in exploring web state/action spaces and detecting bugs. Compared with WebExplor, the state-of-the-art web testing technique, VETL can discover 25% more unique web actions on benchmark websites. Moreover, it can expose functional bugs in top-ranking commercial websites, which have been confirmed by the website maintainers. Our work makes the first attempt of leveraging LVLM in end-to-end GUI testing, demonstrating promising results of this research direction.";High-Level Test Gen;LLM-Pure-Prompting;Custom;LLaVA-1.5;NºWebStates/Actions;-
P122;Leveraging Pre-Trained Large Language Models (LLMs) for On-Premises Comprehensive Automated Test Case Generation: An Empirical Study;2024;Yin2024;C: Other;Conference;"@Inproceedings{Yin2024 ,
  author =       ""Yin, Hang and Mohammed, Hamza and Boyapati, Sai"",
  title =        ""Leveraging Pre-Trained Large Language Models (LLMs) for On-Premises Comprehensive Automated Test Case Generation: An Empirical Study"",
  booktitle =    ""2024 9th International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS)"",
  series =       ""ICIIBMS '24"",
  editor =       """",
  volume =       ""9"",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Okinawa, Japan"",
  pages =        ""597--607"",
  doi =          ""10.1109/ICIIBMS62405.2024.10792720"",
  url =          ""https://doi.org/10.1109/ICIIBMS62405.2024.10792720""
}";Evaluation;"The rapidly evolving field of Artificial Intelligence
(AI)-assisted software testing has predominantly focused on automated test code generation, with limited research exploring the realm of automated test case generation from user stories requirements. This paper presents a comprehensive empirical study on harnessing pre-trained Large Language Models (LLMs) for generating concrete test cases from natural language requirements given in user stories. We investigate the efficacy of various prompting and alignment techniques, including prompt chaining, few-shot instructions, and agency-based approaches, to facilitate secure on-premises deployment. By integrating our learnings with an on-premises model setup, wherein we deploy a RoPE scaled 4-bit quantized LLaMA 3 70B Instruct model, optionally augmented with LoRA adapters trained on QA datasets, we demonstrate that this approach yields more accurate and consistent test cases despite video random-access memory (VRAM) constraints, thereby maintaining the security benefits of an on-premises deployme";High-Level Test Gen;LLM-Pure-FineTune;Custom;Llama-Family;Test Coverage;
P124;LLM-based methods for the creation of unit tests in game development;2024;PaduraruJournal2024;J: Other;Journal;"@article{PaduraruJournal2024,
  title = ""LLM-based methods for the creation of unit tests in game development"",
  author = ""Ciprian Paduraru and Adelina Staicu and Alin Stefanescu"",
  journal = ""Procedia Computer Science"",
  volume = ""246"",
  number = """",
  pages = ""2459--2468"",
  year = ""2024"",
  publisher = ""Association for Computing Machinery"",
  doi = ""10.1016/j.procs.2024.09.473"",
  url = ""https://doi.org/10.1016/j.procs.2024.09.473""
}
";Research Contribution;Problems related to the quality of games, whether on the initial release or after updates, can lead to player dissatisfaction, media attention, and potential financial setbacks. These issues can stem from software bugs, performance bottlenecks, or security vulnerabilities. Despite these challenges, game developers often rely on manual playtesting, highlighting the need for more robust and automated processes in game development. This research explores the application of Large Language Models (LLMs) to automate the creation of unit tests in game development, focusing on strongly typed programming languages such as C++ and C#, which are widely used in the industry. The study focuses on fine-tuning Code Llama, an advanced code generation model, to address common scenarios in game development, including game engines and specific APIs or backends. Although the prototyping and evaluations primarily took place within the Unity game engine, the proposed methods can be adapted to other internal or publicly available solutions. The evaluation results demonstrate these methods’ effectiveness in improving existing unit test suites or automatically generating new tests based on natural language descriptions of class contexts and targeted methods.;Unit Test Generation;LLM-Pure-FineTune;No Bmk-Ds;CodeLlama, GPT-Family;% Build, % Pass, Test Coverage;GameUnitGen
P125;LLM-Driven Testing for Autonomous Driving Scenarios;2024;Petrovic2024;C: Other;Conference;"@Inproceedings{Petrovic2024 ,
  author =       ""Petrovic, Nenad and Lebioda, Krzysztof and Zolfaghari, Vahid and Schamschurko, André and Kirchner, Sven and Purschke, Nils and Pan, Fengjunjie and Knoll, Alois"",
  title =        ""LLM-Driven Testing for Autonomous Driving Scenarios"",
  booktitle =    ""2024 2nd International Conference on Foundation and Large Language Models (FLLM)"",
  series =       ""FLLM '23"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Dubai, United Arab Emirates"",
  pages =        ""173--178"",
  doi =          ""10.1109/FLLM63129.2024.10852505"",
  url =          ""https://doi.org/10.1109/FLLM63129.2024.10852505""
}";Research Contribution;"In this paper, we explore the potential of
leveraging Large Language Models (LLMs) for automated test generation based on free-form textual descriptions in area of automotive. As outcome, we implement a prototype and evaluate the proposed approach on autonomous driving feature scenarios in CARLA open-source simulation environment. Two pre-trained LLMs are taken into account for comparative evaluation: GPT-4 and Llama3. According to the achieved results, GPT-4 outperforms Llama3, while the presented approach speeds-up the process of testing (more than 10 times) and reduces cognitive load thanks to automated code generation and adoption of flexible simulation environment for quick evaluation.";High-Level Test Gen;LLM-Pure-Prompting;No Bmk-Ds;Llama-Family, GPT-Family;% Error Rate, Nº Tokens, Execution Time;-
P126;LLM-Driven, Self-Improving Framework for Security Test Automation: Leveraging Karate DSL for Augmented API Resilience;2025;Pasca2025;J: IEEE Access;Journal;"@article{Pasca2025,
  title = ""LLM-Driven, Self-Improving Framework for Security Test Automation: Leveraging Karate DSL for Augmented API Resilience"",
  author = ""Marian Pasca, Emil and Delinschi, Daniela and Erdei, Rudolf and Matei, Oliviu"",
  journal = ""IEEE Access"",
  volume = ""13"",
  number = """",
  pages = ""56861--56886"",
  year = ""2025"",
  publisher = ""IEEE Computer Society"",
  doi = ""10.1109/ACCESS.2025.3554960"",
  url = ""https://doi.org/10.1109/ACCESS.2025.3554960""
}";Research Contribution;Modern software architectures heavily rely on APIs, yet face significant security challenges, particularly with Broken Object Level Authorization (BOLA) vulnerabilities, which remain the most critical API security risk according to OWASP. This paper introduces Karate-BOLA-Guard, an innovative framework leveraging Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) techniques to automate security-focused test case generation for APIs. Our approach integrates vector databases for context retrieval, multiple LLM models for test generation, and observability tools for process monitoring. Initial experiments were carried out on three deliberately vulnerable APIs (VAmPI, Crapi, and OWASP Juice Shop), with subsequent validation on fifteen additional production APIs spanning diverse domains including social media, version control systems, financial services, and transportation services. Our evaluation metrics show Llama 3 8B achieving consistent performance (Accuracy: 3.1-3.4, Interoperability: 3.7-4.3) with an average processing time of 143.76 seconds on GPU. Performance analysis revealed significant GPU acceleration benefits, with 20-25x improvement over CPU processing times. Smaller models demonstrated efficient processing, with Phi-3 Mini averaging 69.58 seconds and Mistral 72.14 seconds, while maintaining acceptable accuracy scores. Token utilization patterns showed Llama 3 8B using an average of 36,591 tokens per session, compared to Mistral’s 25,225 and Phi-3 Mini’s 31,007. Our framework’s effectiveness varied across APIs, with notably strong performance in complex platforms (Instagram: A = 4.3, I = 4.4) while maintaining consistent functionality in simpler implementations (VAmPI: A = 3.6, I = 4.3). The iterative refinement process, evaluated through comprehensive metrics including Accuracy (A), Complexity (C), and Interoperability (I), represents a significant advancement in automated API security testing, offering an efficient, accurate, and adaptable approach to detecting BOLA vulnerabilities across diverse API architectures.;Non Functional Testing;LLM-Pure-Prompting;Other;Mistral, Other, Llama-Family;Accuracy, Other;Karate-BOLA-Guard
P127;LLM-enhanced evolutionary test generation for untyped languages;2025;Yang2025;J: Aut. Soft. Eng;Journal;"@article{Yang2025,
  title = ""LLM-enhanced evolutionary test generation for untyped languages"",
  author = ""Yang, Ruofan and Xu, Xianghua and Wang, Ran"",
  journal = ""Automated Software Engg."",
  volume = ""32"",
  number = ""1"",
  pages = ""37"",
  year = ""2025"",
  publisher = ""Kluwer Academic Publishers"",
  doi = ""10.1007/s10515-025-00496-7"",
  url = ""https://doi.org/10.1007/s10515-025-00496-7""
}";Research Contribution;Dynamic programming languages, such as Python, are widely used for their flexibility and support for rapid development. However, the absence of explicit parameter type declarations poses significant challenges in generating automated test cases. This often leads to random assignment of parameter types, increasing the search space and reducing testing efficiency. Current evolutionary algorithms, which rely heavily on random mutations, struggle to handle specific data types and frequently fall into local optima, making it difficult to generate high-quality test cases. Moreover, the resulting test suites often contain errors, preventing immediate usage in real-world applications. To address these challenges, this paper proposes the use of large language models to enhance test case generation for dynamic programming languages. Our method involves three key steps: analyzing parameter types to narrow the search space, introducing meaningful data during mutations to increase test case relevance, and using large language models to automatically repair errors in the generated test suites. Experimental results demonstrate a 16% improvement in test coverage, faster evolutionary cycles, and an increase in the number of executable test suites. These findings highlight the potential of large language models in improving both the efficiency and reliability of test case generation for dynamic programming languages;Unit Test Generation;Hybrid-FineTune;Pynguin, HumanEval;DeepSeekCoder, GPT-Family;Test Coverage, Execution Time;pytLMtester
P128;LLM4VV: Developing LLM-driven testsuite for compiler validation;2024;Munley2024;J: Other;Journal;"@article{Munley2024,
  title = ""LLM4VV: Developing LLM-driven testsuite for compiler validation"",
  author = ""Christian Munley and Aaron Jarmusch and Sunita Chandrasekaran"",
  journal = ""Future Generation Computer Systems"",
  volume = ""160"",
  number = """",
  pages = ""1--13"",
  year = ""2024"",
  publisher = ""Elsevier"",
  doi = ""10.1016/j.future.2024.05.034"",
  url = ""https://doi.org/10.1016/j.future.2024.05.034""
}";Research Contribution;Large language models (LLMs) are a new and powerful tool for a wide span of applications involving natural language and demonstrate impressive code generation abilities. The goal of this work is to automatically generate tests and use these tests to validate and verify compiler implementations of a directive-based parallel programming paradigm, OpenACC. To do so, in this paper, we explore the capabilities of state-of-the-art LLMs, including open-source LLMs - Meta’s Codellama, Phind’s fine-tuned version of Codellama, Deepseek’s Deepseek Coder and closed-source LLMs - OpenAI’s GPT-3.5-Turbo and GPT-4-Turbo. We further fine-tuned the open-source LLMs and GPT-3.5-Turbo using our own testsuite dataset along with using the OpenACC specification. We also explored these LLMs using various prompt engineering techniques that include code template, template with retrieval augmented generation (RAG), one-shot example, one-shot with RAG, expressive prompt with code template and RAG. This paper highlights our findings from over 5000 tests generated via all the above mentioned methods. Our contributions include: (a) exploring the capabilities of the latest and relevant LLMs for code generation, (b) investigating fine-tuning and prompt methods, and (c) analyzing the outcome of LLMs generated tests including manually analysis of representative set of tests. We found the LLM Deepseek-Coder-33b-Instruct produced the most passing tests followed by GPT-4-Turbo;Unit Test Generation;LLM-Pure-FineTune;Other;CodeLlama, DeepSeekCoder, GPT-Family;% Pass, Other;LLM4VV
P129;Optimizing Search-Based Unit Test Generation with Large Language Models: An Empirical Study;2024;XiaoInt2024;C: Other;Conference;"@Inproceedings{XiaoInt2024 ,
  author =       ""Xiao, Danni and Guo, Yimeng and Li, Yanhui and Chen, Lin"",
  title =        ""Optimizing Search-Based Unit Test Generation with Large Language Models: An Empirical Study"",
  booktitle =    ""Proceedings of the 15th Asia-Pacific Symposium on Internetware"",
  series =       ""Internetware '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Macau, China"",
  pages =        ""71–-80"",
  doi =          ""10.1145/3671016.3674813"",
  url =          ""https://doi.org/10.1145/3671016.3674813""
}";Survey;"Search-based unit test generation methods have been considered effective and widely applied, and Large Language Models (LLMs) have also demonstrated their powerful generation ability. Therefore, some scholars have proposed using LLMs to enhance search-based unit test generation methods and have preliminarily confirmed that LLMs can help alleviate the problem of test coverage plateaus. However, it is still unclear when and how LLMs should intervene in the time-consuming test generation process. This paper explores the application of LLMs at various stages of search-based test generation (SBTG) (including the initial stage, the test generation period, and the test coverage plateaus), as well as strategies for controlling the frequency of LLM intervention. A comprehensive empirical study was conducted on 486 Python benchmark modules from 27 projects. The experimental results show that 1) LLM intervention has a positive effect at any stage, whether to improve coverage over a fixed period or to reduce the time to reach a specific coverage; 2) a reasonable intervention frequency is crucial for LLMs to have a positive effect on SBTG. This work can better help understand when and how LLMs should be applied in SBTG and provide valuable suggestions for developers in practice.";Unit Test Generation;LLM-Pure-Prompting;Pynguin, BugsInPy;CodeLlama;Test Coverage, Execution Time;-
P130;Pay Attention! Human-Centric Improvements of LLM-based Interfaces for Assisting Software Test Case Development;2024;Shi2024;C: Other;Conference;"@Inproceedings{Shi2024 ,
  author =       ""Shi, Billy and Kristensson, Per Ola"",
  title =        ""Pay Attention! Human-Centric Improvements of LLM-based Interfaces for Assisting Software Test Case Development"",
  booktitle =    ""Adjunct Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology"",
  series =       ""UIST Adjunct '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Pittsburgh, PA, USA"",
  pages =        ""71–-80"",
  doi =          ""10.1145/3672539.3686341"",
  url =          ""https://doi.org/10.1145/3672539.3686341""
}";Survey;"Implementing automation testing is difcult and as a consequence there is a growing desire for semi-automated software testing systems with humans in the loop. Leveraging the growth of LLMs, recent research has demonstrated LLMs’ potential to improve performance on test generation, reporting, and bug triaging. However, relatively little work has explored the interactivity issues that emerge in semi-automated LLM-assisted software test case development. To fll this gap, we present two user studies (N1 = 16, N2 = 24) that investigate productivity, creativity, and user attention in three semiautomated LLM-assisted interaction strategies: (1) pre-emptive prompting; (2) bufered response; and (3) guided input.We fnd that pre-emptively prompting the user signifcantly enhances branch coverage and task creativity by more than 30% while reducing user’s of-task idle time by up to 48.7%. We conclude by suggesting concrete research directions applying mixed-initiative principles for LLM-based interactive systems for semi-automated software testing";Reflections;LLM-Pure-Prompting;No Bmk-Ds;GPT-Family;Test Coverage, Other;-
P131;PENTEST-AI, an LLM-Powered Multi-Agents Framework for Penetration Testing Automation Leveraging Mitre Attack;2024;Bianou2024;C: Other;Conference;"@Inproceedings{Bianou2024 ,
  author =       ""Bianou, Stanislas G. and Batogna, Rodrigue G."",
  title =        ""PENTEST-AI, an LLM-Powered Multi-Agents Framework for Penetration Testing Automation Leveraging Mitre Attack"",
  booktitle =    ""2024 IEEE International Conference on Cyber Security and Resilience (CSR)"",
  series =       ""UIST Adjunct '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""London, United Kingdom"",
  pages =        ""763--770"",
  doi =          ""10.1109/CSR61664.2024.10679480"",
  url =          ""https://doi.org/10.1109/CSR61664.2024.10679480""
}
";Research Contribution;In the digital transformation era, the surge of better development technologies and citizen developers disrupted the space of innovation by increasing the number and complexity of applications used in production. This context prompts advanced cybersecurity measures and more frequent and thorough penetration testing to protect an organization's security posture. The scarcity of skilled expertise in cybersecurity today makes it challenging to cope with the evolving challenge and the growing demand. This paper introduces PENTESTAI, a novel framework for penetration testing automation using Large Language Model (LLM)-powered agents leveraging the MITRE ATTACK knowledge base. The paper provides an overview of the current state of research on cybersecurity and LLM-powered agents, followed by a detailed description of PENTESTAI building blocks. A proof-of-concept implementation is discussed to validate the framework's core constructs. The paper concludes with suggestions for future research directions to achieve the highest level of penetration testing automation with average skilled human-agent collaboration and to create citizen penetration testers.;Test Agents, Non Functional Testing;LLM-Pure-Prompting;No Bmk-Ds;GPT-Family;No Eval.;PENTESTAI
P132;PENTESTGPT: evaluating and harnessing large language models for automated penetration testing;2024;Deng2024;C: Other;Conference;"@Inproceedings{Deng2024 ,
  author =       ""Gelei Deng and Yi Liu and Víctor Mayoral-Vilches and Peng Liu and Yuekang Li and Yuan Xu and Tianwei Zhang and Yang Liu and Martin Pinzger and Stefan Rass"",
  title =        ""PentestGPT: Evaluating and Harnessing Large Language Models for Automated Penetration Testing"",
  booktitle =    ""33rd USENIX Security Symposium (USENIX Security 24)"",
  series =       ""USENIX Security '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""USENIX Association"",
  address =      """",
  pages =        ""847--864"",
  ISBN = ""978-1-939133-44-1"",
  url =          ""https://www.usenix.org/conference/usenixsecurity24/presentation/deng""
}";Research Contribution;"Penetration testing, a crucial industrial practice for ensuring system security, has traditionally resisted automation due to the extensive expertise required by human professionals. Large Language Models (LLMs) have shown significant advancements in various domains, and their emergent abilities suggest their potential to revolutionize industries. In this work, we establish a comprehensive benchmark using real-world penetration testing targets and further use it to explore the capabilities of LLMs in this domain. Our findings reveal that while LLMs demonstrate proficiency in specific sub-tasks within the penetration testing process, such as using testing tools, interpreting outputs, and proposing subsequent actions, they also encounter difficulties maintaining a whole context of the overall testing scenario.

Based on these insights, we introduce PENTESTGPT, an LLM-empowered automated penetration testing framework that leverages the abundant domain knowledge inherent in LLMs. PENTESTGPT is meticulously designed with three self-interacting modules, each addressing individual sub-tasks of penetration testing, to mitigate the challenges related to context loss. Our evaluation shows that PENTESTGPT not only outperforms LLMs with a task-completion increase of 228.6% compared to the GPT-3.5 model among the benchmark targets, but also proves effective in tackling real-world penetration testing targets and CTF challenges. Having been open-sourced on GitHub, PENTESTGPT has garnered over 6,500 stars in 12 months and fostered active community engagement, attesting to its value and impact in both the academic and industrial spheres.
";Non Functional Testing;LLM-Pure-Prompting;Other;Other, GPT-Family;test cost, Other;PentestGPT
P133;PTGroup: An Automated Penetration Testing Framework Using LLMs and Multiple Prompt Chains;2024;Wu2024;C: Other;Conference;"@Inproceedings{Wu2024 ,
  author =       ""Wu, Lei and Zhong, Xiaofeng and Liu, Jingju and Wang, Xiang"",
  title =        ""PTGroup: An Automated Penetration Testing Framework Using LLMs and Multiple Prompt Chains"",
  booktitle =    ""Advanced Intelligent Computing Technology and Applications: 20th International Conference, ICIC 2024, Tianjin, China, August 5–8, 2024, Proceedings, Part IX"",
  series =       ""ICIC '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Springer-Verlag"",
  address =      ""Tianjin, China"",
  pages =        ""220-–232"",
  doi =          ""10.1007/978-981-97-5606-3_19"",
  url =          ""https://doi.org/10.1007/978-981-97-5606-3_19""
}";Research Contribution;Abstract. Penetration testing is an effective means of maintaining network security. To address the challenge of high labor costs in traditional penetration testing, researchers have been investigating the potential of automated solutions. In this paper, we introduce an automated penetration testing framework with multi-agent characteristics, named PTGroup, which utilizes the prior knowledge and textual comprehension capabilities of Large Language Models (LLMs). The framework follows the operational mode of ReAct, executing a multi-round decision-making process through successive Thought-Act-Observe cycles, thereby sequentially accomplishing automated penetration testing tasks. We conducted experiments using different LLMs and demonstrated that our framework is capable of adapting to various LLMs. In addition, we proposed an approach to improve PTGroup’s adaptability to different vulnerability environments by designing multiple prompt chains and we demonstrated the efficacy of this approach through a series of experiments. Finally, we propose the directions for improvement of the PTGroup, encompassing the use of more specialized LLMs and the adoption of automated prompt chain generation.;Non Functional Testing, Test Agents;LLM-Pure-Prompting;Other;GPT-Family;Successful exploits;PTGroup
P135;Quantizing Large-Language Models for Predicting Flaky Tests;2024;Rahman2024;C: ICST;Conference;"@Inproceedings{Rahman2024 ,
  author =       ""{Rahman, Shanto and Baz, Abdelrahman and Misailovic, Sasa and Shi, August"",
  title =        ""Quantizing Large-Language Models for Predicting Flaky Tests"",
  booktitle =    ""2024 IEEE Conference on Software Testing, Verification and Validation (ICST)"",
  series =       ""ICST '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Toronto, ON, Canada"",
  pages =        ""93--104"",
  doi =          ""10.1109/ICST60714.2024.00018"",
  url =          ""https://doi.org/10.1109/ICST60714.2024.00018""
}
";Research Contribution;A major challenge in regression testing practice is the presence of flaky tests, which non-deterministically pass or fail when run on the same code. Previous research identified multiple categories of flaky tests. Prior research has also developed techniques for automatically detecting which tests are flaky or categorizing flaky tests, but these techniques generally involve repeatedly rerunning tests in various ways, making them costly to use. Although several recent approaches have utilized large-language models (LLMs) to predict which tests are flaky or predict flaky-test categories without needing to rerun tests, they are costly to use due to relying on a large neural network to perform feature extraction and prediction. We propose FlakyQ to improve the effectiveness of LLM-based flaky-test prediction by quantizing LLM’s weights. The quantized LLM can extract features from test code more efficiently. To make up for loss in prediction performance due to quantization, we further train a traditional ML classifier (e.g., a random forest) to learn from the quantized LLM-extracted features and do the same prediction. The final model has similar prediction performance while running faster than the non-quantized LLM. Our evaluation finds that FlakyQ classifiers consistently improves prediction time over the non-quantized LLM classifier, saving 25.4% in prediction time over all tests, along with a 48.4% reduction in memory usage. Furthermore, prediction performance is equal or better than the non-quantized LLM classifier.;Test Augmentation or Improvement;Hybrid-FineTune;Other;CodeBert;Other, Time, Memory;FlakyQ
P136;State Diagram Extension and Test Case Generation Based on Large Language Models for Improving Test Engineers’ Efficiency in Safety Testing;2024;SuQingram2024;C: ISSRE;Conference;"@Inproceedings{SuQingram2024 ,
  author =       ""Su, Qingran and Li, Xingze and Ren, Yuming and Ouyang, Xulang and Hu, Chunming and Yin, Yongfeng"",
  title =        ""State Diagram Extension and Test Case Generation Based on Large Language Models for Improving Test Engineers’ Efficiency in Safety Testing"",
  booktitle =    ""2024 IEEE 35th International Symposium on Software Reliability Engineering Workshops (ISSREW)"",
  series =       ""ISSREW '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Tsukuba, Japan"",
  pages =        ""287--294"",
  doi =          ""10.1109/ISSREW63542.2024.00092"",
  url =          ""https://doi.org/10.1109/ISSREW63542.2024.00092""
}";Research Contribution;Safety testing is a key method to ensure software quality. But the quality of the test depends on the level of the test engineers. The method of generating safety test cases based on state diagrams often perform poorly due to lack of personnel experience. Meanwhile, the manual construction of test cases constrains the improvement of safety testing efficiency. This article utilizes LLMs to achieve automated extension and testing of safety state diagrams. The main contributions include extracting safety properties from aviation standards and extending the basic state diagram with LLMs to enable safety testing capabilities. In addition, a genetic algorithm combining edge first depth traversal and LLMs optimization was proposed to improve the efficiency of test case generation. Compared to classical genetic algorithms, the average execution round achieved a 90% reduction. Our LLMs optimization strategy can also enhance other improved genetic algorithms, further improving the efficiency of safety testing.;High-Level Test Gen, Non Functional Testing;Hybrid-FineTune;Other;Qwen 2;Other;
P137;Symbolic Execution with Test Cases Generated by Large Language Models;2024;XuQRS2024;C: Other;Conference;"@Inproceedings{XuQRS2024 ,
  author =       ""Xu, Jiahe and Xu, Jingwei and Chen, Taolue and Ma, Xiaoxing"",
  title =        ""Symbolic Execution with Test Cases Generated by Large Language Models"",
  booktitle =    ""2024 IEEE 24th International Conference on Software Quality, Reliability and Security (QRS)"",
  series =       ""QRS '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Cambridge, United Kingdom"",
  pages =        ""228--237"",
  doi =          ""10.1109/QRS62785.2024.00031"",
  url =          ""https://doi.org/10.1109/QRS62785.2024.00031""
}";Research Contribution;Symbolic execution is a powerful program analysis technique. External environment construction and internal path explosion are two long-standing problems which may affect the effectiveness and performance of symbolic execution on complex programs. The intrinsic challenge is to achieve a sufficient understanding of the program context to construct a set of execution environments which can guide the selection of symbolic states. In this paper, we propose a novel program-context-guided symbolic execution framework LangSym based on program’s instruction/user manual. Leveraging the capabilities of natural language understanding and code generation in large language models (LLMs), LangSym can automatically extract the knowledge related to the functionality of the program, and generate adequate test cases and the corresponding environments as the prior knowledge for symbolic execution. We instantiate LangSym in KLEE, a widely adopted symbolic execution engine, to build a pipeline that could automatically leverage LLMs to boost the symbolic execution. We evaluate LangSym on almost all GNU Coreutils programs and considerable large-scale programs, showing that LangSym outperforms the existing strategies in KLEE with at least a 10% increase for line coverage.;High-Level Test Gen;LLM-Pure-Prompting;GNU Core- utils;CodeLlama, Gemini, GPT-Family;Test Coverage;LangSym
P138;Test Case Migration from Monolith to Microservices Using Large Language Models;2024;Yeh2024;C: Other;Conference;"@Inproceedings{Yeh2024 ,
  author =       ""Yeh, Hang-Wei and Ma, Shang-Pin and Chen, Yi"",
  title =        ""Test Case Migration from Monolith to Microservices Using Large Language Models"",
  booktitle =    ""2024 IEEE International Conference on e-Business Engineering (ICEBE)"",
  series =       ""ICEBE '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Shanghai, China"",
  pages =        ""29--35"",
  doi =          ""10.1109/ICEBE62490.2024.00014"",
  url =          ""https://doi.org/10.1109/ICEBE62490.2024.00014""
}";Research Contribution;Due to microservices' modularity, high scalability, and good fault tolerance, more and more software systems are transitioning from monolithic architectures to microservice architectures. In this migration process, the migration of test cases is a crucial step to ensure functional consistency and completeness before and after the migration, thereby maintaining the stability and reliability of the microservice system post-migration. However, despite numerous studies on architecture migration, there is still a lack of methods to efficiently convert test cases of monolithic systems into ones for the migrated microservices. Therefore, this study proposes a test migration approach based on Large Language Models (LLM), called LTM^3 (LLM-based Test Migration from Monolith to Microservices). During migration, LTM^3 identifies the correspondence between existing test cases and monolithic functions, the connections between monolithic functions and migrated microservices, and the dependencies between microservices. Subsequently, by utilizing LLM and appropriate prompting, the integration test cases of the monolithic system are transformed into contract test cases for each microservice. The experimental results showed that LTM^3 can effectively migrate most test cases, with only a tiny portion requiring manual adjustment.;Unit Test Generation, High-Level Test Gen;LLM-Pure-Prompting;Other;GPT-Family;Other, Test Coverage;LTM^3
P139;Test-Agent: A Multimodal App Automation Testing Framework Based on the Large Language Model;2024;LiDTPI2024;C: Other;Conference;"@Inproceedings{LiDTPI2024 ,
  author =       ""Li, Youwei and Li, Yangyang and Yang, Yangzhao"",
  title =        ""Test-Agent: A Multimodal App Automation Testing Framework Based on the Large Language Model"",
  booktitle =    ""2024 IEEE 4th International Conference on Digital Twins and Parallel Intelligence (DTPI)"",
  series =       ""DTPI '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Wuhan, China"",
  pages =        ""609--614"",
  doi =          ""10.1109/DTPI61353.2024.10778901"",
  url =          ""https://doi.org/10.1109/DTPI61353.2024.10778901""
}";Research Contribution;This paper introduces a multi-modal agent-based app automation testing framework, named Test-Agent, built on the Large Language Model (LLM), designed to address the growing challenges in mobile application automation testing.As mobile applications become more prevalent and emerging systems like Harmony OS Next and mini-programs emerge,traditional automated testing methods, which depend on manually crafting test cases and scripts, are no longer sufficient for cross-platform compatibility and complex interaction logic. The Test-Agent framework employs artificial intelligence technologies to analyze application interface screenshots and user natural language instructions. Combined with deep learning models,it automatically generates and executes test actions on mobile devices. This innovative approach eliminates the need for pre-written test scripts or backend system access, relying solely on screenshots and UI structure information. It achieves cross-platform and cross-application universality, significantly reducing the workload of test case writing, enhancing test execution efficiency, and strengthening cross-platform adaptability. Test Agent offers an innovative and efficient solution for automated testing of mobile applications.;High-Level Test Gen;Hybrid-Prompting;Other;Other;Time, Other;Test-Agent
P140;Towards LLM-Assisted System Testing for Microservices;2024;Almutawa2024;C: Other;Conference;"@Inproceedings{Almutawa2024 ,
  author =       ""Almutawa, Mustafa and Ghabrah, Qusai and Canini, Marco"",
  title =        ""Towards LLM-Assisted System Testing for Microservices"",
  booktitle =    ""2024 IEEE 44th International Conference on Distributed Computing Systems Workshops (ICDCSW)"",
  series =       ""ICDCSW '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Jersey City, NJ, USA"",
  pages =        ""29--34"",
  doi =          ""10.1109/ICDCSW63686.2024.00011"",
  url =          ""https://doi.org/10.1109/ICDCSW63686.2024.00011""
}";Research Contribution;As modern applications are being designed in a distributed, Microservices Architecture (MSA), it becomes increasingly difficult to debug and test those systems. Typically, it is the role of software testing engineers or Quality Assurance(QA) engineers to write software tests to ensure the reliability of applications, but such a task can be labor-intensive and time-consuming. In this paper, we explore the potential of Large Language Models (LLMs) in assisting software engineers in generating test cases for software systems, with a particular focus on performing end-to-end (black-box) system testing on web-based MSA applications. We present our experience building Kashef,a software testing tool that utilizes the advanced capabilities of current LLMs in code generation and reasoning, and builds on top of the concept of communicative agents.;High-Level Test Gen, Test Agents;LLM-Pure-Prompting;Other;CodeLlama, GPT-Family, Llama-Family;Other, No Eval.;Kashef
P142;Unit Test Generation using Large Language Models for Unity Game Development;2024;Paduraru2024;C: Other;Conference;"@Inproceedings{Paduraru2024 ,
  author =       ""Paduraru, Ciprian and Stefanescu, Alin and Jianu, Augustin"",
  title =        ""Unit Test Generation using Large Language Models for Unity Game Development"",
  booktitle =    ""Proceedings of the 1st ACM International Workshop on Foundations of Applied Software Engineering for Games"",
  series =       ""FaSE4Games 2024"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Porto de Galinhas, Brazil"",
  pages =        ""7–-13"",
  doi =          ""10.1145/3663532.3664466"",
  url =          ""https://doi.org/10.1145/3663532.3664466""
}";Research Contribution;Challenges related to game quality, whether occurring during initial release or after updates, can result in player dissatisfaction, media scrutiny, and potential financial setbacks. These issues may stem from factors like software bugs, performance bottlenecks, or security vulnerabilities. Despite these challenges, game developers often rely on manual playtesting, highlighting the need for more robust and automated processes in game development. This research explores the application of Large Language Models (LLMs) for automating unit test creation in game development, with a specific focus on strongly typed programming languages like C++ and C#, widely used in the industry. The study centers around fine-tuning Code Llama, an advanced code generation model, to address common scenarios encountered in game development, including game engines and specific APIs or backends. Although the prototyping and evaluations primarily occurred within the Unity game engine, the proposed methods can be adapted to other internal or publicly available solutions. The evaluation outcomes demonstrate the effectiveness of these methods in enhancing existing unit test suites or automatically generating new tests based on natural language descriptions of class contexts and targeted methods.;Unit Test Generation, Test Augmentation or Improvement;LLM-Pure-FineTune;Other;CodeLlama;% Build, Test Coverage, % Pass;
P143;UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing;2024;He2024;C: ISSTA;Conference;"@Inproceedings{He2024 ,
  author =       ""He, Yifeng and Huang, Jiabo and Rong, Yuyang and Guo, Yiwen and Wang, Ethan and Chen, Hao"",
  title =        ""UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing"",
  booktitle =    ""Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis"",
  series =       ""ISSTA 2024"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""Association for Computing Machinery"",
  address =      ""Vienna, Austria"",
  pages =        ""1061–-1072"",
  doi =          ""10.1145/3650212.3680342"",
  url =          ""https://doi.org/10.1145/3650212.3680342""
}";Evaluation;The remarkable capability of large language models (LLMs) in generating high-quality code has drawn increasing attention in the software testing community. However, existing code LLMs often demonstrate unsatisfactory capabilities in generating accurate and complete tests since they were trained on code snippets collected without differentiating between code for testing purposes and other code. In this paper, we present a large-scale dataset UniTSyn, which is capable of enhancing the prowess of LLMs for Unit Test Synthesis. Associating tests with the tested functions is crucial for LLMs to infer the expected behavior and the logic paths to be verified. By leveraging Language Server Protocol, UniTSyn achieves the challenging goal of collecting focal-test pairs without perproject execution setups or per-language heuristics that tend to be fragile and difficult to scale. It contains 2.7 million focal-test pairs across five mainstream programming languages, making it possible to be utilized for enhancing the test generation ability of LLMs. The details of UniTSyn can be found in Table 1. Our experiments demonstrate that, by building an autoregressive model based on UniTSyn, we can achieve significant benefits in learning and understanding unit test representations, resulting in improved generation accuracy and code coverage across all evaluated programming languages. Code and data will be publicly available.;Reflections;Hybrid-FineTune;;CodeGen 2, CodeT5, Other;% Pass, Test Coverage;UnitSyn
P144;Using LLM for Mining and Testing Constraints in API Testing;2024;Huynh2024;C: ASE;Conference;"@Inproceedings{Huynh2024 ,
  author =       ""Huynh, Minh-Hieu and Le, Quoc-Tri and Nguyen, Tien N. and Nguyen, Vu"",
  title =        ""Using LLM for Mining and Testing Constraints in API Testing"",
  booktitle =    ""2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE)"",
  series =       ""ASE '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Sacramento, CA, USA"",
  pages =        ""2486--2487"",
  doi =          ""DOI not available"",
  url =          ""https://ieeexplore.ieee.org/document/10765048""
}";Research Contribution;Testing Representational State Transfer (REST) APIs is crucial for ensuring the reliability and performance of APIs, which are essential to modern web services. This testing process helps identify and resolve issues related to data exchange and integration with other systems. Among the various API testing techniques, black-box testing relies on the OpenAPI Specification (OAS) to generate test cases and data. However, current API test automation methods are primarily focused on status code [10] and schema validation [1]. Status code validation involves ensuring that each HTTP request returns a response with a status code, a three-digit integer that indicates the outcome of the request. Schema validation verifies the correctness of the response data by comparing it to the schema. This includes checking that all required properties are present and that data types of these properties align with the schema specified.;Oracle Generation;LLM-Pure-Prompting;;;;
P145;Variable Discovery with Large Language Models for Metamorphic Testing of Scientific Software;2024;TsigkanosICCS2024;C: Other;Conference;"@Inproceedings{TsigkanosICCS2024 ,
  author =       ""Tsigkanos, Christos and Rani, Pooja and Müller, Sebastian and Kehrer, Timo"",
  title =        ""Variable Discovery with Large Language Models for Metamorphic Testing of Scientific Software"",
  booktitle =    ""Computational Science – ICCS 2023: 23rd International Conference, Prague, Czech Republic, July 3–5, 2023, Proceedings, Part I"",
  series =       ""ICCS 2023"",
  editor =       """",
  volume =       """",
  year =         ""2023"",
  publisher =    ""Springer-Verlag"",
  address =      ""Prague, Czech Republic}"",
  pages =        ""321-–335"",
  doi =          ""10.1007/978-3-031-35995-8_23"",
  url =          ""https://doi.org/10.1007/978-3-031-35995-8_23""
}";Research Contribution;When testing scientific software, it is often challenging or even impossible to craft a test oracle for checking whether the program under test produces the expected output when being executed on a given input – also known as the oracle problem in software engineering. Metamorphic testing mitigates the oracle problem by reasoning on necessary properties that a program under test should exhibit regarding multiple input and output variables. A general approach consists of extracting metamorphic relations from auxiliary artifacts such as user manuals or documentation, a strategy particularly fitting to testing scientific software. However, such software typically has large input-output spaces, and the fundamental prerequisite – extracting variables of interest – is an arduous and non-scalable process when performed manually. To this end, we devise a workflow around an autoregressive transformer-based Large Language Model (LLM) towards the extraction of variables from user manuals of scientific software. Our end-to-end approach, besides a prompt specification consisting of few examples by a human user, is fully automated, in contrast to current practice requiring human intervention. We showcase our LLM workflow over three case studies of scientific software documentation, and compare variables extracted to ground truth manually labelled by experts.;Oracle Generation;LLM-Pure-Prompting;Other;GTP-J6B;Accuracy, Other;
P146;WebFuzzAuto: An Automated Fuzz Testing Tool Integrating Reinforcement Learning and Large Language Models for Web Security;2024;Chen2024;C: Other;Conference;"@Inproceedings{Chen2024 ,
  author =       ""Chen, Xiaoquan and Liu, Jian and Zhang, Yingkai and Hu, Qinsong and Han, Yupeng and Zhang, Ruqi and Ran, Jingqi and Yan, Lei and Huang, Baiqi and Ma, Shengting and Wang, Jiale"",
  title =        ""WebFuzzAuto: An Automated Fuzz Testing Tool Integrating Reinforcement Learning and Large Language Models for Web Security"",
  booktitle =    ""2024 12th International Conference on Information Systems and Computing Technology (ISCTech)"",
  series =       ""ISCTech '24"",
  editor =       """",
  volume =       """",
  year =         ""2024"",
  publisher =    ""IEEE Computer Society"",
  address =      ""Xi'an, China"",
  pages =        ""1-–10"",
  doi =          ""10.1109/ISCTech63666.2024.10845318"",
  url =          ""https://doi.org/10.1109/ISCTech63666.2024.10845318""
}";Research Contribution;This paper proposes a novel, fully automated fuzz testing tool for web applications that integrates reinforcement learning and large model techniques to enhance the intelligence of vulnerability detection and code coverage. By designing a reinforcement learning environment, the testing agent can intelligently explore complex web applications and continuously adjust strategies to discover more vulnerabilities. The tool employs dynamic adjusters and curriculum learning strategies to gradually optimize key parameters, enhancing the stability and convergence speed of the model. Additionally, it integrates a vulnerability detection module and a feedback manager. After each testing step, it calculates rewards and adjusts expert feedback based on the detection results, further optimizing the model’s learning path. Furthermore, it leverages natural language processing techniques from large language models for deep analysis, formulating testing strategies and guiding the behavior of the testing agent. Experimental results demonstrate that this tool outperforms the traditional OWASP ZAP tool in terms of code coverage and vulnerability detection rates on three major web applications: OWASP Juice Shop, phpMyAdmin, and WordPress, verifying its effectiveness and advantages in complex web applications.;Non Functional Testing;Hybrid-FineTune;Other;Other;Test Coverage, Successful exploits, Time, Memory, Other;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
;;;;;;;;;;;;;;
